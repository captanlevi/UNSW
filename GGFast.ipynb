{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import loadFlows\n",
    "from flowprintOptimal.sekigo.core.flowRepresentation import PacketFlowRepressentation\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from GGFast.grouper import Grouper\n",
    "from flowprintOptimal.sekigo.utils.documentor import Documenter\n",
    "from flowprintOptimal.sekigo.flowUtils.dataGetter import getTrainTestOOD\n",
    "from GGFast.utils import getGrouperData, getLVectorFromFlowRep\n",
    "from GGFast.core import LVector, Snippet, SnippetScorer\n",
    "from GGFast.gathering import Gathering\n",
    "from GGFast.commons import getLabelMapping, loadLVectors\n",
    "import heapq\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from flowprintOptimal.sekigo.flowUtils.dataGetterV2 import readTrainTestOODFlows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    data_config = dict(\n",
    "        dataset_name = \"vnat\",\n",
    "        subsampleConfig = None,#dict(max_gap = 20, min_gap = 5),                             \n",
    "        max_flow_length = 100, # in seconds  ( each flow sample cannot excede this length)\n",
    "        test_size = .2,\n",
    "        ood_classes = [],\n",
    "        do_balance = False,\n",
    "        data_type = \"packet_representation\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if len(configs[\"data_config\"][\"ood_classes\"]) == 0:\n",
    "    base_dir_path = \"data/ClassificationOnlyFlows\"\n",
    "else:\n",
    "    base_dir_path = \"data/ClassificationOODFlows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_packet_flows,test_packet_flows,ood_packet_flows, train_timeslot_flows,test_timeslot_flows,ood_timeslot_flows = readTrainTestOODFlows(base_path= base_dir_path, dataset_name= configs[\"data_config\"][\"dataset_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruncatePacketRep(flows : List[PacketFlowRepressentation], limit = 30):\n",
    "    truncated_flows = []\n",
    "    for flow in flows:\n",
    "        if len(flow) <= limit:\n",
    "            truncated_flows.append(flow)\n",
    "        else:\n",
    "            truncated_flows.append(flow.getSubFlow(0,limit))\n",
    "    return truncated_flows\n",
    "\n",
    "\n",
    "\n",
    "train_packet_flows = getTruncatePacketRep(flows= train_packet_flows)\n",
    "test_packet_flows = getTruncatePacketRep(flows= test_packet_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessFlows(flows):\n",
    "    for flow in flows:\n",
    "        flow.lengths = list(map(lambda x : np.round(x*1500), flow.lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessFlows(train_packet_flows)\n",
    "preProcessFlows(test_packet_flows)\n",
    "preProcessFlows(ood_packet_flows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_lengths,inbound_classes,outbound_lengths,outbound_classes = getGrouperData(flows= train_packet_flows)\n",
    "forward_grouper = Grouper(features= inbound_lengths, labels = inbound_classes, threshold= .001)\n",
    "backward_grouper = Grouper(features= outbound_lengths, labels = outbound_classes, threshold= .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l_vectors = []\n",
    "test_l_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2734/2734 [00:01<00:00, 1698.57it/s]\n",
      "100%|██████████| 684/684 [00:00<00:00, 1767.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for flow_rep in tqdm(train_packet_flows):\n",
    "    train_l_vectors.append(getLVectorFromFlowRep(flow_rep= flow_rep, forward_grouper = forward_grouper,backward_grouper= backward_grouper))\n",
    "\n",
    "for flow_rep in tqdm(test_packet_flows):\n",
    "    test_l_vectors.append(getLVectorFromFlowRep(flow_rep= flow_rep, forward_grouper = forward_grouper,backward_grouper= backward_grouper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLvectors(path,l_vectors : List[LVector]):\n",
    "    data = list(map(lambda x : x.__dict__,l_vectors))\n",
    "\n",
    "    for d in data:\n",
    "        for key in [\"lv1\", \"lv2\", \"lv3\", \"lv4\", \"lv5\"]:\n",
    "            d[key] = list(map(lambda x : str(int(x[0])) + str(x[1]), d[key]))\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = saveLvectors(path= \"GGFast/storage/vnat_train_l_vectors.json\", l_vectors= train_l_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = saveLvectors(path= \"GGFast/storage/vnat_test_l_vectors.json\", l_vectors= test_l_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lv1': ['64->',\n",
       "  '60<-',\n",
       "  '52->',\n",
       "  '154->',\n",
       "  '52<-',\n",
       "  '1470<-',\n",
       "  '294<-',\n",
       "  '52->',\n",
       "  '191->',\n",
       "  '52<-',\n",
       "  '95->',\n",
       "  '52<-',\n",
       "  '95<-',\n",
       "  '137<-',\n",
       "  '52->'],\n",
       " 'lv2': ['22->',\n",
       "  '19<-',\n",
       "  '10->',\n",
       "  '112->',\n",
       "  '11<-',\n",
       "  '1151<-',\n",
       "  '251<-',\n",
       "  '10->',\n",
       "  '149->',\n",
       "  '11<-',\n",
       "  '53->',\n",
       "  '11<-',\n",
       "  '54<-',\n",
       "  '96<-',\n",
       "  '10->'],\n",
       " 'lv3': ['22->',\n",
       "  '0<-',\n",
       "  '10->',\n",
       "  '112->',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '10->',\n",
       "  '149->',\n",
       "  '0<-',\n",
       "  '53->',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '10->'],\n",
       " 'lv4': ['0->',\n",
       "  '19<-',\n",
       "  '0->',\n",
       "  '0->',\n",
       "  '11<-',\n",
       "  '1151<-',\n",
       "  '251<-',\n",
       "  '0->',\n",
       "  '0->',\n",
       "  '11<-',\n",
       "  '0->',\n",
       "  '11<-',\n",
       "  '54<-',\n",
       "  '96<-',\n",
       "  '0->'],\n",
       " 'lv5': ['0->',\n",
       "  '0<-',\n",
       "  '0->',\n",
       "  '0->',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0->',\n",
       "  '0->',\n",
       "  '0<-',\n",
       "  '0->',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0<-',\n",
       "  '0->'],\n",
       " 'class_type': 'MAIL'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gathering:\n",
    "    def __init__(self,n_gram_range = [1,4], top_k = 25000):\n",
    "        self.n_gram_range = n_gram_range\n",
    "        self.top_k = top_k\n",
    "\n",
    "        \n",
    "    def __getCandidatesFromEncoding(self,encoding : list, tp : int):\n",
    "\n",
    "        def getSnippetsFromNGram(n_gram):\n",
    "            snippets = []\n",
    "            for i in range(0,len(encoding) - n_gram + 1):\n",
    "                s1 = Snippet(sequence= encoding[i:i+n_gram], position= i+1, negation= False, tp= tp)\n",
    "                s2 = Snippet(sequence= encoding[i:i+n_gram], position= -(len(encoding) - i), negation= False, tp= tp)\n",
    "                s3 = Snippet(sequence= encoding[i:i+n_gram], position= \"*\", negation= False, tp= tp)\n",
    "\n",
    "                snippets.append(s1)\n",
    "                snippets.append(s2)\n",
    "                snippets.append(s3)\n",
    "            return snippets\n",
    "\n",
    "        snippets = []\n",
    "        for n_gram in range(self.n_gram_range[0], self.n_gram_range[1] + 1):\n",
    "            if len(encoding) < n_gram:\n",
    "                break\n",
    "            snippets.extend(getSnippetsFromNGram(n_gram= n_gram))\n",
    "        return snippets\n",
    "\n",
    "\n",
    "    def getSnippetsFromSingleLvector(self,lvector : LVector):\n",
    "        snippets = []\n",
    "        snippets.extend(self.__getCandidatesFromEncoding(encoding= lvector.lv1, tp= 1))\n",
    "        snippets.extend(self.__getCandidatesFromEncoding(encoding= lvector.lv2, tp= 2))\n",
    "        snippets.extend(self.__getCandidatesFromEncoding(encoding= lvector.lv3, tp= 3))\n",
    "        snippets.extend(self.__getCandidatesFromEncoding(encoding= lvector.lv4, tp= 4))\n",
    "        snippets.extend(self.__getCandidatesFromEncoding(encoding= lvector.lv5, tp= 5))\n",
    "        \n",
    "        return snippets\n",
    "\n",
    "\n",
    "    def getCandidates(self,l_vectors : List[LVector], snippet_scorer : SnippetScorer):\n",
    "        \n",
    "        top_snippets = []  # this is min heap\n",
    "        for l_vector in l_vectors:\n",
    "            snippets = self.getSnippetsFromSingleLvector(lvector= l_vector)\n",
    "            \n",
    "            scores = []\n",
    "            for snippet in tqdm(snippets):\n",
    "                scores.append(snippet_scorer.score(snippet))\n",
    "            #scores = [snippet_scorer.score(s) for s in snippets]\n",
    "            #scores = Parallel(n_jobs=min(len(snippets), 2))(delayed(snippet_scorer.score)(snippet) for snippet in snippets)\n",
    "            candidates = [(score,snippet) for score,snippet in zip(scores,snippets)]\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                heapq.heappush(top_snippets,candidate)\n",
    "\n",
    "            while len(top_snippets) > self.top_k:\n",
    "                heapq.heappop(top_snippets)\n",
    "\n",
    "        return top_snippets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_scorer = SnippetScorer(l_vectors= l_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathering = Gathering(n_gram_range= [1,4], top_k=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n",
      "one process ended\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "n_scoring_processes = 16\n",
    "snippets_publish_batch = 50\n",
    "\n",
    "def putLVectorIntoQueue(l_vector_queue : Queue,l_vectors : List[LVector]):\n",
    "    for l_vector in l_vectors:\n",
    "        while l_vector_queue.qsize() > 10000:\n",
    "            time.sleep(1)\n",
    "        l_vector_queue.put(l_vector)\n",
    "    l_vector_queue.put(None)\n",
    "\n",
    "def putSnippetsIntoQueue(l_vector_queue : Queue,snippet_queue : Queue,gathering : Gathering):\n",
    "    snippets_to_publish = []\n",
    "    while True:\n",
    "        l_vector = l_vector_queue.get()\n",
    "        if l_vector == None:\n",
    "            if len(snippets_to_publish) > 0:\n",
    "                snippet_queue.put(snippets_to_publish)\n",
    "            l_vector_queue.put(None) # to signal siblings\n",
    "            snippet_queue.put(None) # to signal the scoring process\n",
    "            break\n",
    "        snippets = gathering.getSnippetsFromSingleLvector(lvector= l_vector)\n",
    "\n",
    "        for snippet in snippets:\n",
    "            while snippet_queue.qsize() > 1000:\n",
    "                time.sleep(1)\n",
    "            snippets_to_publish.append(snippet)\n",
    "\n",
    "            if len(snippets_to_publish) == snippets_publish_batch:\n",
    "                snippet_queue.put(snippets_to_publish)\n",
    "                snippets_to_publish = []\n",
    "            \n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "def scoreSnippetsFromQueue(snippet_queue : Queue,candidate_queue : Queue,snippet_scorer : SnippetScorer):\n",
    "    candidates_to_publish = []\n",
    "    while True:\n",
    "        snippets = snippet_queue.get()\n",
    "        if snippets == None:\n",
    "            snippet_queue.put(None) # to signal sibling processes\n",
    "            break\n",
    "\n",
    "        for snippet in snippets:\n",
    "            score = snippet_scorer.score(snippet= snippet)\n",
    "            candidate = (score,snippet)\n",
    "            candidates_to_publish.append(candidate)\n",
    "        candidate_queue.put(candidates_to_publish)\n",
    "    candidate_queue.put(None)\n",
    "\n",
    "\n",
    "l_vector_queue = Queue()\n",
    "snippet_queue = Queue()\n",
    "candidate_queue = Queue()\n",
    "\n",
    "l_vector_process = Process(target= putLVectorIntoQueue, args=(l_vector_queue,l_vectors[:1]))\n",
    "snippet_process = Process(target= putSnippetsIntoQueue,  args= (l_vector_queue, snippet_queue, gathering))\n",
    "scoring_processes : List[Process] = []\n",
    "\n",
    "for _ in range(n_scoring_processes):\n",
    "    scoring_processes.append(Process(target= scoreSnippetsFromQueue, args= (snippet_queue, candidate_queue,snippet_scorer)))\n",
    "\n",
    "l_vector_process.start()\n",
    "snippet_process.start()\n",
    "\n",
    "for scoring_process in scoring_processes:\n",
    "    scoring_process.start()\n",
    "\n",
    "\n",
    "top_candidates = []\n",
    "sentinel_count = 0\n",
    "while sentinel_count < n_scoring_processes:\n",
    "    candidates = candidate_queue.get()\n",
    "\n",
    "    if candidates == None:\n",
    "        print(\"one process ended\")\n",
    "        sentinel_count += 1\n",
    "        continue\n",
    "\n",
    "    for candidate in candidates:\n",
    "        heapq.heappush(top_candidates,candidate)\n",
    "    \n",
    "    while len(top_candidates) > 25000:\n",
    "        heapq.heappop(top_candidates)\n",
    "\n",
    "\n",
    "l_vector_process.join()\n",
    "snippet_process.join()\n",
    "for scoring_process in scoring_processes:\n",
    "    scoring_process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2 = heapq.nlargest(2, top_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<-'), (0, '<-'), (0, '<-'), (12, '->'), (151, '->')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2[0][1].sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2[0][1].tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.702333692464288"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
