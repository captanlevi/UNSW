{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flowprintOptimal.sekigo.modeling.neuralNetworks import BaseLSTMNetwork\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from flowprintOptimal.sekigo.core.flowRepresentation import FlowRepresentation,PacketFlowRepressentation\n",
    "from flowprintOptimal.sekigo.dataAnalysis.vNATDataFrameProcessor import VNATDataFrameProcessor\n",
    "from flowprintOptimal.sekigo.core.flowConfig import FlowConfig\n",
    "import random\n",
    "from flowprintOptimal.sekigo.flowUtils.flowDatasets import PacketFlowDataset, BaseFlowDataset\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import normalizePacketRep\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import saveFlows,loadFlows\n",
    "from flowprintOptimal.sekigo.dataAnalysis.dataFrameProcessor import UTMobileNetProcessor\n",
    "from flowprintOptimal.sekigo.flowUtils.dataGetter import getTrainTestOOD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "from flowprintOptimal.sekigo.modeling.trainers import NNClassificationTrainer\n",
    "from flowprintOptimal.sekigo.modeling.neuralNetworks import LSTMNetwork,TransformerGenerator,CNNNetwork1D, LSTMDuelingNetwork\n",
    "from flowprintOptimal.sekigo.modeling.loggers import Logger\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.core import MemoryElement,Rewarder,State\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.memoryFiller import MemoryFiller\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.datasets import MemoryDataset\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.trainers import EarlyClassificationtrainer\n",
    "from flowprintOptimal.sekigo.utils.documentor import Documenter\n",
    "from flowprintOptimal.sekigo.utils.evaluations import Evaluator,EarlyEvaluation\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import dropPacketFromPacketRep\n",
    "from typing import List\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    name = \"VNAT_no_sample_ood_dueling\",\n",
    "    description = \"Sample UNIBS dueling with ood detection and ood samples generated, reward function gives out 0 for ood\",\n",
    "    \n",
    "    common_config = dict(\n",
    "        max_length = 15\n",
    "    ),\n",
    "    \n",
    "    full_model_kwargs = dict(\n",
    "        lstm_hidden_size = 256,\n",
    "        layers= 2, lstm_input_size = 3\n",
    "    ),\n",
    "\n",
    "    early_model_kwargs = dict(\n",
    "        lstm_input_size= 3,lstm_hidden_size= 256,layers = 2        \n",
    "    ),\n",
    "    \n",
    "    data_config = dict(\n",
    "        dataset_name = \"VNAT\",\n",
    "        subsampleConfig = None,#dict(max_gap = 30, min_gap = 10),\n",
    "        max_flow_length = 80, # in seconds  ( each flow sample cannot excede this length)\n",
    "        test_size = .2,\n",
    "        ood_classes = [],\n",
    "        do_balance = False\n",
    "\n",
    "    ),\n",
    "\n",
    "    rewarder_config = dict(\n",
    "        l = .5\n",
    "    ),\n",
    "\n",
    "    dataset_config = dict(\n",
    "        aug = [0,.2]\n",
    "    ),\n",
    "\n",
    "    memory_fillter_config = dict(\n",
    "        ood_config = dict(ood_aug = [.6,.9], ood_prob = .2),\n",
    "        min_length = 5,\n",
    "        use_balancer = False\n",
    "    ),\n",
    "    full_trainer_config = dict(\n",
    "        use_sampler = False\n",
    "    ),\n",
    "    early_trainer_config = dict(\n",
    "        use_sampler = False  # this is for giving more weight to wait samples\n",
    "    ),\n",
    "\n",
    "    ppo_configs = dict(\n",
    "        num_envs = 512,\n",
    "        num_steps = 32,\n",
    "        observation_dim = [3],\n",
    "        action_space_dim = None,\n",
    "        gamma = .99,\n",
    "        num_mini_batches = 8,\n",
    "        update_epochs = 2,\n",
    "        clip_coef = .2,\n",
    "        entropy_coef = .05,\n",
    "        value_coef = .5,\n",
    "        max_grad_norm = 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full class distrubation\n",
      "FT           16420\n",
      "control      13591\n",
      "streaming     1759\n",
      "chat          1244\n",
      "Name: count, dtype: int64\n",
      "using no sampling\n",
      "filtering max_flow_length = 80\n",
      "post num packet filter class distrubation\n",
      "streaming    1541\n",
      "chat          639\n",
      "control       609\n",
      "FT            456\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "train class distrubation\n",
      "streaming    1218\n",
      "chat          522\n",
      "control       492\n",
      "FT            364\n",
      "Name: count, dtype: int64\n",
      "test class distrubation\n",
      "streaming    323\n",
      "chat         117\n",
      "control      117\n",
      "FT            92\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_flows,test_flows,ood_flows = getTrainTestOOD(**configs[\"data_config\"], packet_limit= configs[\"common_config\"][\"max_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PacketFlowDataset(flows= train_flows,label_to_index= None,aug= configs[\"dataset_config\"][\"aug\"])\n",
    "test_dataset = PacketFlowDataset(flows= test_flows,label_to_index= train_dataset.label_to_index)\n",
    "ood_dataset = PacketFlowDataset(flows= ood_flows, label_to_index= None) if (ood_flows != None and len(ood_flows) != 0) else None\n",
    "num_labels = len(train_dataset.label_to_index)\n",
    "configs[\"full_model_kwargs\"][\"output_dim\"] = num_labels \n",
    "configs[\"early_model_kwargs\"][\"output_dim\"] = num_labels + 1\n",
    "configs[\"rewarder_config\"][\"num_labels\"] = num_labels\n",
    "configs[\"rewarder_config\"][\"max_length\"] = configs[\"common_config\"][\"max_length\"]\n",
    "configs[\"ppo_configs\"][\"action_space_dim\"] = num_labels + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyEnvironment:\n",
    "    def __init__(self,dataset,num_classes, reward_lam = .5):\n",
    "        \"\"\"\n",
    "        timeseries\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "        self.reward_lam = reward_lam\n",
    "        self.curr_iterator = None \n",
    "        self.num_episodes = 0\n",
    "        self.info = dict(\n",
    "            reward = 0,\n",
    "            length = 0\n",
    "        )\n",
    "        \n",
    "    def returnObs(self):\n",
    "        return self.dataset[self.curr_iterator[\"index\"]][\"data\"][self.curr_iterator[\"ts\"]]\n",
    "\n",
    "    def reset(self):\n",
    "        index = np.random.randint(0, len(self.dataset))\n",
    "        self.curr_iterator = dict(index = index, ts = 0, reward = 0, length = 0)\n",
    "        return self.returnObs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action >= 0 and action <= self.num_classes\n",
    "        label = self.dataset[self.curr_iterator[\"index\"]][\"label\"]\n",
    "        total_ts_length = len(self.dataset[self.curr_iterator[\"index\"]][\"data\"])\n",
    "        done = self.curr_iterator[\"ts\"] == total_ts_length - 1\n",
    "        reward = 0\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif action == self.num_classes:\n",
    "            if label == -1:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -self.reward_lam/total_ts_length\n",
    "        else:\n",
    "            reward  = -1\n",
    "            done = True\n",
    "        \n",
    "        result  = dict(next_obs = self.returnObs(), done = int(done), reward  = reward,label = label, ts = self.curr_iterator[\"ts\"])\n",
    "\n",
    "        self.curr_iterator[\"ts\"] += 1\n",
    "        self.curr_iterator[\"reward\"] += reward\n",
    "        self.curr_iterator[\"length\"] += 1\n",
    "        if done == True:\n",
    "            self.num_episodes += 1\n",
    "\n",
    "            self.info[\"reward\"] = (self.info[\"reward\"]*(self.num_episodes - 1) +  self.curr_iterator[\"reward\"])/self.num_episodes\n",
    "            self.info[\"length\"] = (self.info[\"length\"]*(self.num_episodes - 1) +  self.curr_iterator[\"length\"])/self.num_episodes\n",
    "            self.reset()\n",
    "        \n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class EarlyEnvs:\n",
    "    def __init__(self, early_environments : List[EarlyEnvironment]):\n",
    "        self.early_environments = early_environments\n",
    "    \n",
    "    def returnObs(self):\n",
    "        obs = list(map(lambda x : x.returnObs(), self.early_environments))\n",
    "        return np.array(obs)\n",
    "\n",
    "    def reset(self):\n",
    "        for early_environment in self.early_environments:\n",
    "            early_environment.reset()\n",
    "        return self.returnObs()\n",
    "    \n",
    "    def step(self,actions):\n",
    "        assert len(actions) == len(self.early_environments)\n",
    "\n",
    "        agg_results = []\n",
    "\n",
    "        for i in range(len(actions)):\n",
    "            agg_results.append(self.early_environments[i].step(action=actions[i]))\n",
    "        \n",
    "        keys = list(agg_results[0].keys())\n",
    "        master_agg = dict()\n",
    "        for key in keys:\n",
    "            master_agg[key] = []\n",
    "            for agg_result in agg_results:\n",
    "                master_agg[key].append(agg_result[key])\n",
    "            master_agg[key] = np.array(master_agg[key])\n",
    "        \n",
    "        return master_agg\n",
    "    \n",
    "    def getInfo(self):\n",
    "        reward, length = 0,0\n",
    "        for env in self.early_environments:\n",
    "            reward += env.info[\"reward\"]\n",
    "            length += env.info[\"length\"]\n",
    "\n",
    "        return dict(\n",
    "            reward = reward/len(self.early_environments),\n",
    "            length = length/len(self.early_environments)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(BaseLSTMNetwork):\n",
    "    def __init__(self, lstm_input_size, lstm_hidden_size, output_dim, layers=1) -> None:\n",
    "        super().__init__(lstm_input_size, lstm_hidden_size,layers)\n",
    "\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, 1.0)\n",
    "\n",
    "        self.action_linear = nn.Sequential(nn.Linear(lstm_hidden_size, lstm_hidden_size//2), nn.Tanh(),\n",
    "                                           nn.Linear(lstm_hidden_size//2, lstm_hidden_size//2), nn.Tanh(),\n",
    "                                           nn.Linear(lstm_hidden_size//2, output_dim)\n",
    "                                            )\n",
    "        self.value_linear = nn.Sequential(nn.Linear(lstm_hidden_size, lstm_hidden_size//2), nn.Tanh(),\n",
    "                                           nn.Linear(lstm_hidden_size//2, lstm_hidden_size//2), nn.Tanh(),\n",
    "                                           nn.Linear(lstm_hidden_size//2, 1)\n",
    "                                            )\n",
    "\n",
    "    def getStates(self,x,lstm_state,done):\n",
    "        \"\"\"\n",
    "        x is (seq,BS,3)\n",
    "        done is (seq,BS)\n",
    "        Seq_len is 1 when I am rolling out\n",
    "        where BS is the number of environments\n",
    "        \"\"\"\n",
    "        batch_size = lstm_state[0].shape[1]\n",
    "\n",
    "        x = x.reshape(-1,batch_size,self.lstm.input_size).permute((1,0,2))\n",
    "        done = done.reshape(-1,batch_size).permute((1,0))   # for the case where \n",
    "        output = []\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        for i in range(seq_len):\n",
    "\n",
    "            lstm_hidden,lstm_state = self.lstm(x[:,i:i+1,:],\n",
    "                (\n",
    "                    (1.0 - done[:,i]).view(1, -1, 1) * lstm_state[0],\n",
    "                    (1.0 - done[:,i]).view(1, -1, 1) * lstm_state[1],\n",
    "                )\n",
    "\n",
    "            # lstm_hidden is (BS,1,lstm_output_size)\n",
    "            )\n",
    "\n",
    "            output.append(lstm_hidden.permute((1,0,2)))\n",
    "        \n",
    "        output_hidden = torch.flatten(torch.cat(output),0,1)\n",
    "        return output_hidden,lstm_state\n",
    "\n",
    "\n",
    "    def getValue(self, x, lstm_state, done):\n",
    "        hidden, _ = self.getStates(x, lstm_state, done)\n",
    "        return self.value_linear(hidden)\n",
    "\n",
    "    def getActionAndValue(self, x, lstm_state, done, action=None):\n",
    "        \"\"\"\n",
    "        x is (BS,obs_dim which is 3 in this case)\n",
    "        \"\"\"\n",
    "        hidden, lstm_state = self.getStates(x, lstm_state, done)\n",
    "        logits = self.action_linear(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        \n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        return action, probs.log_prob(action), probs.entropy(), self.value_linear(hidden), lstm_state\n",
    "    \n",
    "    def earlyClassificationForward(self,X):\n",
    "        \"\"\"\n",
    "        X is the timeseries input of shape \n",
    "        (BS,Seq len, lstm_input_size)\n",
    "        outputs (BS,seq_len,feature_len)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            lstm_out, _ = self.lstm(X)\n",
    "            return self.action_linear(lstm_out),lstm_out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self,envs : EarlyEnvs,agent : PPOAgent,ppo_config : dict, device,logger : Logger, train_dataset,test_dataset, ood_dataset = None):\n",
    "        self.envs = envs\n",
    "        self.ppo_config = ppo_config\n",
    "        self.agent = agent.to(device)\n",
    "        self.device = device\n",
    "        assert len(self.envs.early_environments) == ppo_config[\"num_envs\"]\n",
    "        self.storage = self._initializeStorage()\n",
    "        self.logger = logger\n",
    "        self.evaluator = EarlyEvaluation(min_steps= 0, device= device,model= self.agent)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.ood_dataset = ood_dataset\n",
    "\n",
    "        self.best = dict(\n",
    "            score = 0,\n",
    "            model = None\n",
    "        )\n",
    "\n",
    "        self.logger.setMetricReportSteps(metric_name= \"test_eval_f1\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"train_eval_f1\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"train_eval_time\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"test_eval_time\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"ood_eval\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"ood_eval_time\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"incorrect_ood_test\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"incorrect_ood_train\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"avg_reward\", step_size= 1)\n",
    "        self.logger.setMetricReportSteps(metric_name= \"avg_length\", step_size= 1)\n",
    "        \n",
    "    def _initializeStorage(self):\n",
    "        config = self.ppo_config\n",
    "        num_steps,num_envs,observation_dim, action_space_dim = config[\"num_steps\"], config[\"num_envs\"], config[\"observation_dim\"], config[\"action_space_dim\"]\n",
    "\n",
    "        obs = torch.zeros([num_steps, num_envs] + observation_dim).to(self.device)\n",
    "        actions  = torch.ones([num_steps,num_envs]).to(self.device)*-1\n",
    "        log_probs = torch.zeros([num_steps,num_envs]).to(self.device)\n",
    "        rewards = torch.zeros([num_steps,num_envs]).to(self.device)\n",
    "        dones = torch.zeros([num_steps,num_envs]).to(self.device)\n",
    "        values = torch.zeros([num_steps,num_envs]).to(self.device)\n",
    "\n",
    "        return  dict(\n",
    "            obs = obs, actions = actions, log_probs = log_probs,\n",
    "            rewards = rewards, dones = dones,values = values\n",
    "        )\n",
    "\n",
    "\n",
    "    def _calcAdvantage(self,next_obs,next_lstm_state,next_done):\n",
    "        num_steps = self.ppo_config[\"num_steps\"]\n",
    "        rewards = self.storage[\"rewards\"]\n",
    "        with torch.no_grad():\n",
    "            # originally next_value is (num_envs,1) dimentional\n",
    "            next_value = self.agent.getValue(\n",
    "                    next_obs,\n",
    "                    next_lstm_state,\n",
    "                    next_done,\n",
    "                )[:,0]\n",
    "        \n",
    "        returns = torch.zeros_like(rewards).to(self.device)\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_non_terminal = 1 - next_done\n",
    "                next_return = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1 - self.storage[\"dones\"][t+1] # t+1 as the does are shifted ( hence next done)\n",
    "                next_return = returns[t+1]\n",
    "            returns[t] = rewards[t] + self.ppo_config[\"gamma\"]*next_non_terminal*next_return\n",
    "        \n",
    "        return returns , returns - self.storage[\"values\"]\n",
    "            \n",
    "            \n",
    "    def _updateModel(self, agent_optimizer):\n",
    "        num_envs, num_mini_batches = self.ppo_config[\"num_envs\"], self.ppo_config[\"num_mini_batches\"]\n",
    "        num_steps = self.ppo_config[\"num_steps\"]\n",
    "        assert num_envs%num_mini_batches == 0\n",
    "\n",
    "        # by reshaping -1 we get [t1,t1,t1,t1... num_envs times, t2,t2,t2....num_envs times]\n",
    "        b_obs = self.storage[\"obs\"].reshape(-1, *self.ppo_config[\"observation_dim\"])\n",
    "        b_log_probs = self.storage[\"log_probs\"].reshape(-1)\n",
    "        b_actions = self.storage[\"actions\"].reshape(-1)\n",
    "        b_dones = self.storage[\"dones\"].reshape(-1)\n",
    "        b_advantages = self.storage[\"advantages\"].reshape(-1)\n",
    "        b_returns = self.storage[\"returns\"].reshape(-1)\n",
    "        b_values = self.storage[\"values\"].reshape(-1)\n",
    "\n",
    "        envs_per_batch = num_envs//num_mini_batches\n",
    "        env_indices = np.arange(num_envs) # (0,1,2,3) for nenvs = 4\n",
    "        flat_indices = np.arange(num_envs*num_steps).reshape(num_steps,num_envs)  # [(0,1,2,3),(4,5,6,7), .....] for nenvs = 4\n",
    "\n",
    "        for epoch in range(self.ppo_config[\"update_epochs\"]):\n",
    "            np.random.shuffle(env_indices)\n",
    "\n",
    "            for start in range(0,num_envs,envs_per_batch):\n",
    "                end = start + envs_per_batch\n",
    "                mb_env_indices = env_indices[start : end]\n",
    "                mb_indices = flat_indices[:,mb_env_indices].ravel()\n",
    "                _, new_log_prob, entropy, newvalue, _ = self.agent.getActionAndValue(\n",
    "                    x = b_obs[mb_indices],\n",
    "                    lstm_state= (\n",
    "                        torch.zeros(self.agent.lstm.num_layers,envs_per_batch, self.agent.lstm.hidden_size).to(self.device),\n",
    "                        torch.zeros(self.agent.lstm.num_layers,envs_per_batch, self.agent.lstm.hidden_size).to(self.device)\n",
    "                    ),\n",
    "                    done= b_dones[mb_indices],\n",
    "                    action= b_actions.long()[mb_indices]\n",
    "                )\n",
    "\n",
    "                \n",
    "\n",
    "                logratio = new_log_prob - b_log_probs[mb_indices]\n",
    "                ratio = logratio.exp()\n",
    "                mb_advantages = b_advantages[mb_indices]\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.ppo_config[\"clip_coef\"], 1 + self.ppo_config[\"clip_coef\"])\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                newvalue = newvalue.view(-1)\n",
    "                \n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_indices]) ** 2).mean()\n",
    "\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss - self.ppo_config[\"entropy_coef\"] * entropy_loss + v_loss * self.ppo_config[\"value_coef\"]\n",
    "                \n",
    "                self.logger.addMetric(metric_name= \"pg_loss\" , value= pg_loss.item())\n",
    "                self.logger.addMetric(metric_name= \"value_loss\", value= v_loss.item())\n",
    "                self.logger.addMetric(metric_name= \"entropy_loss\", value= entropy_loss.item())\n",
    "                agent_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.agent.parameters(), self.ppo_config[\"max_grad_norm\"])\n",
    "                agent_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def runUpdate(self, agent_optimizer):\n",
    "        num_steps,num_envs = self.ppo_config[\"num_steps\"], self.ppo_config[\"num_envs\"]\n",
    "        next_obs = torch.tensor(self.envs.reset()).to(device).float()\n",
    "        next_done = torch.zeros(self.ppo_config[\"num_envs\"]).to(device)\n",
    "\n",
    "        next_lstm_state = (\n",
    "        torch.zeros(self.agent.lstm.num_layers, num_envs, self.agent.lstm.hidden_size).to(device),\n",
    "        torch.zeros(self.agent.lstm.num_layers, num_envs, self.agent.lstm.hidden_size).to(device),\n",
    "        )\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            self.storage[\"obs\"][step] = next_obs\n",
    "            self.storage[\"dones\"][step] = next_done\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action,log_prob,_,value,next_lstm_state = self.agent.getActionAndValue(x = next_obs,lstm_state= next_lstm_state,done= next_done)\n",
    "\n",
    "                # put episode logging code here\n",
    "                self.storage[\"actions\"][step] = action\n",
    "                self.storage[\"log_probs\"][step] = log_prob\n",
    "                self.storage[\"values\"][step] = value.flatten()\n",
    "\n",
    "                env_step = self.envs.step(action.cpu().numpy())\n",
    "                next_obs, reward, done = env_step[\"next_obs\"], env_step[\"reward\"], env_step[\"done\"]\n",
    "                self.storage[\"rewards\"][step] = torch.tensor(reward).to(self.device).float()\n",
    "\n",
    "                next_obs, next_done = torch.tensor(next_obs).to(self.device).float(), torch.tensor(done).to(self.device)\n",
    "\n",
    "        returns, advantages = self._calcAdvantage(next_obs= next_obs, next_done= next_done, next_lstm_state= next_lstm_state)\n",
    "        self.storage[\"advantages\"] = advantages\n",
    "        self.storage[\"returns\"] = returns\n",
    "\n",
    "        self._updateModel(agent_optimizer= agent_optimizer)\n",
    "\n",
    "    def eval(self,dataset : BaseFlowDataset):\n",
    "        metrices = self.evaluator.getMetrices(dataset= dataset,ood_dataset= None)\n",
    "        return metrices[\"macro_f1\"],metrices[\"time\"],metrices[\"incorrect_ood\"]\n",
    "    \n",
    "    def evalOOD(self):\n",
    "        metrices = self.evaluator.getMetrices(ood_dataset= self.ood_dataset, dataset= None)\n",
    "        self.logger.addMetric(metric_name= \"ood_eval\", value= metrices[\"ood_accuracy\"])\n",
    "        self.logger.addMetric(metric_name= \"ood_eval_time\", value= metrices[\"ood_time\"])\n",
    "\n",
    "    def evalTrain(self):\n",
    "        f1,average_time,incorrect_ood = self.eval(dataset= self.train_dataset)\n",
    "        self.logger.addMetric(metric_name= \"train_eval_f1\", value= f1)\n",
    "        self.logger.addMetric(metric_name= \"train_eval_time\", value= average_time)\n",
    "        self.logger.addMetric(metric_name= \"incorrect_ood_train\", value = incorrect_ood)\n",
    "\n",
    "    def evalTest(self):\n",
    "        f1,average_time,incorrect_ood = self.eval(dataset= self.test_dataset)\n",
    "\n",
    "        if f1 >= self.best[\"score\"]:\n",
    "            self.best[\"score\"] = f1\n",
    "            self.best[\"model\"] = deepcopy(self.agent)\n",
    "        \n",
    "        self.logger.addMetric(metric_name= \"test_eval_f1\", value= f1)\n",
    "        self.logger.addMetric(metric_name= \"test_eval_time\", value= average_time)\n",
    "        self.logger.addMetric(metric_name= \"incorrect_ood_test\", value= incorrect_ood)\n",
    "\n",
    "    def train(self, num_updates = 1, lr = 3e-4):\n",
    "        \n",
    "        agent_optimizer = torch.optim.Adam(params= self.agent.parameters(), lr= lr)\n",
    "        for update in range(num_updates):\n",
    "            self.storage = self._initializeStorage()\n",
    "            self.runUpdate(agent_optimizer = agent_optimizer)\n",
    "            if update%100 == 0:\n",
    "                self.evalTest()\n",
    "                info = self.envs.getInfo()\n",
    "\n",
    "                self.logger.addMetric(metric_name= \"avg_reward\", value= info[\"reward\"])\n",
    "                self.logger.addMetric(metric_name= \"avg_length\", value= info[\"length\"])\n",
    "                if self.ood_dataset != None:\n",
    "                    self.evalOOD()\n",
    "            if update%2000 == 0:\n",
    "                self.evalTrain()\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_envs = []\n",
    "\n",
    "for _ in range(configs[\"ppo_configs\"][\"num_envs\"]):\n",
    "    early_envs.append(EarlyEnvironment(dataset= train_dataset, num_classes= len(train_dataset.label_to_index)))\n",
    "\n",
    "envs = EarlyEnvs(early_environments= early_envs)\n",
    "agent = PPOAgent(**configs[\"early_model_kwargs\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(name= \"PPO\")\n",
    "logger.default_step_size = 1500\n",
    "\n",
    "trainer = PPOTrainer(envs= envs, agent= agent, ppo_config= configs[\"ppo_configs\"], device= device, train_dataset= train_dataset,\n",
    "                     logger= logger, test_dataset= test_dataset, ood_dataset= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO ---- 1 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 1 metric test_eval_time = 0.0\n",
      "PPO ---- 1 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 1 metric avg_reward = -0.49894022414793104\n",
      "PPO ---- 1 metric avg_length = 1.2339967131060683\n",
      "PPO ---- 1 metric train_eval_f1 = 0.15967488201363397\n",
      "PPO ---- 1 metric train_eval_time = 0.0\n",
      "PPO ---- 1 metric incorrect_ood_train = 0.0\n",
      "PPO ---- 1500 metric pg_loss = -0.0016347655194501082\n",
      "PPO ---- 1500 metric value_loss = 0.4859256957570712\n",
      "PPO ---- 1500 metric entropy_loss = 0.6610436387260755\n",
      "PPO ---- 2 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 2 metric test_eval_time = 0.0\n",
      "PPO ---- 2 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 2 metric avg_reward = -0.09694408895678905\n",
      "PPO ---- 2 metric avg_length = 1.3585769194285215\n",
      "PPO ---- 3000 metric pg_loss = -0.00017780265165492893\n",
      "PPO ---- 3000 metric value_loss = 0.4889269459644953\n",
      "PPO ---- 3000 metric entropy_loss = 0.5835638108452161\n",
      "PPO ---- 3 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 3 metric test_eval_time = 0.0\n",
      "PPO ---- 3 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 3 metric avg_reward = -0.0862993241438239\n",
      "PPO ---- 3 metric avg_length = 1.3610063141489985\n",
      "PPO ---- 4500 metric pg_loss = -0.00015468297867725293\n",
      "PPO ---- 4500 metric value_loss = 0.48853390328089397\n",
      "PPO ---- 4500 metric entropy_loss = 0.5969759087165196\n",
      "PPO ---- 4 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 4 metric test_eval_time = 0.0\n",
      "PPO ---- 4 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 4 metric avg_reward = -0.08179323055651305\n",
      "PPO ---- 4 metric avg_length = 1.3673223680363211\n",
      "PPO ---- 6000 metric pg_loss = -4.8087071937819324e-05\n",
      "PPO ---- 6000 metric value_loss = 0.4886914668281873\n",
      "PPO ---- 6000 metric entropy_loss = 0.5895545927286148\n",
      "PPO ---- 5 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 5 metric test_eval_time = 0.0\n",
      "PPO ---- 5 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 5 metric avg_reward = -0.08016407686437754\n",
      "PPO ---- 5 metric avg_length = 1.3674614247867627\n",
      "PPO ---- 7500 metric pg_loss = -0.00013403856242075562\n",
      "PPO ---- 7500 metric value_loss = 0.4887490413983663\n",
      "PPO ---- 7500 metric entropy_loss = 0.5888877726594607\n",
      "PPO ---- 6 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 6 metric test_eval_time = 0.0\n",
      "PPO ---- 6 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 6 metric avg_reward = -0.07888642050633088\n",
      "PPO ---- 6 metric avg_length = 1.3675156163232185\n",
      "PPO ---- 9000 metric pg_loss = -0.0001393742433283478\n",
      "PPO ---- 9000 metric value_loss = 0.4886272145708402\n",
      "PPO ---- 9000 metric entropy_loss = 0.5907654201189677\n",
      "PPO ---- 7 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 7 metric test_eval_time = 0.0\n",
      "PPO ---- 7 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 7 metric avg_reward = -0.07828145771034674\n",
      "PPO ---- 7 metric avg_length = 1.3704322817331545\n",
      "PPO ---- 10500 metric pg_loss = -0.00013785624015145003\n",
      "PPO ---- 10500 metric value_loss = 0.4882046367923419\n",
      "PPO ---- 10500 metric entropy_loss = 0.5975356499751409\n",
      "PPO ---- 8 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 8 metric test_eval_time = 0.0\n",
      "PPO ---- 8 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 8 metric avg_reward = -0.07757214233118644\n",
      "PPO ---- 8 metric avg_length = 1.3708852950642514\n",
      "PPO ---- 12000 metric pg_loss = -9.793144751650592e-05\n",
      "PPO ---- 12000 metric value_loss = 0.4888655121326447\n",
      "PPO ---- 12000 metric entropy_loss = 0.5896817864378293\n",
      "PPO ---- 9 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 9 metric test_eval_time = 0.0\n",
      "PPO ---- 9 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 9 metric avg_reward = -0.07693725095476178\n",
      "PPO ---- 9 metric avg_length = 1.3703473344286092\n",
      "PPO ---- 13500 metric pg_loss = -0.00014194845932070165\n",
      "PPO ---- 13500 metric value_loss = 0.48897524740298587\n",
      "PPO ---- 13500 metric entropy_loss = 0.5846992533008257\n",
      "PPO ---- 10 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 10 metric test_eval_time = 0.0\n",
      "PPO ---- 10 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 10 metric avg_reward = -0.07638082311981223\n",
      "PPO ---- 10 metric avg_length = 1.3679005242499327\n",
      "PPO ---- 15000 metric pg_loss = -0.00011214766406919808\n",
      "PPO ---- 15000 metric value_loss = 0.4888766415119171\n",
      "PPO ---- 15000 metric entropy_loss = 0.584741152703762\n",
      "PPO ---- 11 metric test_eval_f1 = 0.1661522633744856\n",
      "PPO ---- 11 metric test_eval_time = 0.0\n",
      "PPO ---- 11 metric incorrect_ood_test = 0.0\n",
      "PPO ---- 11 metric avg_reward = -0.07619975743144114\n",
      "PPO ---- 11 metric avg_length = 1.3690124122954956\n",
      "PPO ---- 16500 metric pg_loss = -0.0001396276984984676\n",
      "PPO ---- 16500 metric value_loss = 0.4885591892798742\n",
      "PPO ---- 16500 metric entropy_loss = 0.5938125121196111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 204\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self, num_updates, lr)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_updates):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initializeStorage()\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_optimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magent_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalTest()\n",
      "Cell \u001b[0;32mIn[67], line 171\u001b[0m, in \u001b[0;36mPPOTrainer.runUpdate\u001b[0;34m(self, agent_optimizer)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m advantages\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m returns\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_updateModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magent_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 133\u001b[0m, in \u001b[0;36mPPOTrainer._updateModel\u001b[0;34m(self, agent_optimizer)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maddMetric(metric_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39m entropy_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    132\u001b[0m agent_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 133\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    135\u001b[0m agent_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(num_updates= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO ---- 12 metric test_eval_f1 = 0.35914683940788333\n",
      "PPO ---- 12 metric test_eval_time = 4.888388148432256\n",
      "PPO ---- 12 metric incorrect_ood_test = 0.0\n"
     ]
    }
   ],
   "source": [
    "trainer.evalTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
