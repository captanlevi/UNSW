{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from flowprintOptimal.sekigo.modeling.neuralNetworks import LSTMFeatureExtractor,LinearPredictor,TransformerGenerator,CNNDiscriminator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import loadFlows, saveFlows\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from flowprintOptimal.sekigo.dataAnalysis.vNATDataFrameProcessor import VNATDataFrameProcessor\n",
    "from flowprintOptimal.sekigo.flowUtils.sampler import FixedLengthSampler\n",
    "from flowprintOptimal.sekigo.flowUtils.flowDatasets import MaxNormalizedDataset,ActivityDataset,BaseFlowDataset,DDQNActivityDataset\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.core import MemoryElement,Rewarder\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.datasets import MemoryDataset\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.trainers import EarlyClassificationtrainer\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.memoryFiller import MemoryFiller\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.core import State\n",
    "from flowprintOptimal.sekigo.core.flowConfig import FlowConfig\n",
    "from flowprintOptimal.sekigo.modeling.trainers import NNClassificationTrainer\n",
    "from flowprintOptimal.sekigo.modeling.loggers import Logger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from flowprintOptimal.sekigo.dataAnalysis.dataFrameProcessor import SoftwareUpdateDataProcessor,GamingDownloadDataFrameProcessor\n",
    "from flowprintOptimal.sekigo.dataAnalysis.dataFrameExtractor import DataFrameExtractor\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config = FlowConfig(grain= 1, band_thresholds= [1250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial software update length = 72826\n",
      "final software update length = 36790\n",
      "after adding uploads size = 73580\n"
     ]
    }
   ],
   "source": [
    "processor1 = SoftwareUpdateDataProcessor(parquet_path= \"data/software-update-1w.parquet\")\n",
    "processor2 = GamingDownloadDataFrameProcessor(parquet_path= \"data/2023-12-01--2023-12-31--flowprint.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = DataFrameExtractor.getData(data_frame_processors= [processor2,processor1],needed_flow_config= flow_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type_filtered_flows = flows\n",
    "FT_filtered_flows = list(filter(lambda x : x.class_type != \"Download\",class_type_filtered_flows))\n",
    "FT_flows = list(filter(lambda x : x.class_type == \"Download\",class_type_filtered_flows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Video': 0.0008191989266558945, 'Conferencing': 0.002246698061471074, 'Upload': 0.0009995096893413704}\n",
      "overlapping points = 0\n",
      "{'Download': 0.004999436005562642}\n",
      "overlapping points = 0\n",
      "{'Video': 0.0009095043201455207, 'Conferencing': 0.0024943655643104054, 'Download': 0.0011122128065388648, 'Upload': 0.0011096918598199467}\n",
      "overlapping points = 0\n"
     ]
    }
   ],
   "source": [
    "sampler = FixedLengthSampler(flow_config= FT_filtered_flows[0].flow_config,required_length_in_seconds = 30,ratio_of_median_to_sample= .001,min_activity_for_start_point= 25,sample_wise_train_ratio= .8, temporal_train_ratio= 1)\n",
    "FT_filtered_split_flows = sampler.sampleAndCutToLength(data= FT_filtered_flows)\n",
    "ft_sampler = FixedLengthSampler(flow_config= FT_filtered_flows[0].flow_config,required_length_in_seconds = 30,ratio_of_median_to_sample= .005,min_activity_for_start_point= 25,sample_wise_train_ratio= 1, temporal_train_ratio= 1)\n",
    "FT_split_flows = ft_sampler.sampleAndCutToLength(data= FT_flows)\n",
    "all_sampler = FixedLengthSampler(flow_config= class_type_filtered_flows[0].flow_config,required_length_in_seconds = 30,ratio_of_median_to_sample= .001,min_activity_for_start_point= 25,sample_wise_train_ratio= .8, temporal_train_ratio= 1)\n",
    "all_flows = all_sampler.sampleAndCutToLength(data= class_type_filtered_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_dataset = DDQNActivityDataset(flows= all_flows[\"train_flows\"],label_to_index= None)\n",
    "all_test_dataset = DDQNActivityDataset(flows = all_flows[\"test_flows\"], label_to_index= all_train_dataset.label_to_index)\n",
    "train_dataset = DDQNActivityDataset(flows= FT_filtered_split_flows[\"train_flows\"],label_to_index= None)\n",
    "test_dataset = DDQNActivityDataset(flows= FT_filtered_split_flows[\"test_flows\"],label_to_index= train_dataset.label_to_index)\n",
    "ood_dataset = DDQNActivityDataset(flows= FT_split_flows[\"train_flows\"],label_to_index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self,feature_dim,num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(feature_dim,num_classes)\n",
    "    def forward(self,X):\n",
    "        return self.linear(X)\n",
    "feature_extractor = LSTMFeatureExtractor(lstm_hidden_size= 64,lstm_input_size=4,feature_dim= 64)\n",
    "classifier = Predictor(feature_dim=64,num_classes= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification ---- 1 metric test_f1 = 0.11055450535601764\n",
      "classification ---- 1 metric train_f1 = 0.11101232904610468\n",
      "classification ---- 10 metric train_loss = 1.3878017783164978\n",
      "classification ---- 20 metric train_loss = 1.3829196214675903\n",
      "classification ---- 30 metric train_loss = 1.3803474664688111\n",
      "classification ---- 40 metric train_loss = 1.3790977835655212\n",
      "classification ---- 50 metric train_loss = 1.3648073554039002\n",
      "classification ---- 60 metric train_loss = 1.3654117345809937\n",
      "classification ---- 70 metric train_loss = 1.3509877920150757\n",
      "classification ---- 80 metric train_loss = 1.3502636075019836\n",
      "classification ---- 90 metric train_loss = 1.3400566458702088\n",
      "classification ---- 100 metric train_loss = 1.3143349409103393\n",
      "classification ---- 110 metric train_loss = 1.3038896679878236\n",
      "classification ---- 120 metric train_loss = 1.2998436570167542\n",
      "classification ---- 130 metric train_loss = 1.2679919242858886\n",
      "classification ---- 140 metric train_loss = 1.2475510239601135\n",
      "classification ---- 150 metric train_loss = 1.183767831325531\n",
      "classification ---- 160 metric train_loss = 1.0848152399063111\n",
      "classification ---- 170 metric train_loss = 0.9379727005958557\n",
      "classification ---- 180 metric train_loss = 0.8237975239753723\n",
      "classification ---- 190 metric train_loss = 0.7284395813941955\n",
      "classification ---- 200 metric train_loss = 0.6317700564861297\n",
      "classification ---- 210 metric train_loss = 0.5968459963798523\n",
      "classification ---- 220 metric train_loss = 0.5820002079010009\n",
      "classification ---- 230 metric train_loss = 0.5251965224742889\n",
      "classification ---- 240 metric train_loss = 0.5130817145109177\n",
      "classification ---- 250 metric train_loss = 0.4811920791864395\n",
      "classification ---- 260 metric train_loss = 0.4499021828174591\n",
      "classification ---- 270 metric train_loss = 0.4518087536096573\n",
      "classification ---- 280 metric train_loss = 0.4306602358818054\n",
      "classification ---- 290 metric train_loss = 0.41770526468753816\n",
      "classification ---- 300 metric train_loss = 0.44341094195842745\n",
      "classification ---- 310 metric train_loss = 0.45699722468853\n",
      "classification ---- 320 metric train_loss = 0.41462434232234957\n",
      "classification ---- 330 metric train_loss = 0.41593878865242007\n",
      "classification ---- 340 metric train_loss = 0.4468966335058212\n",
      "classification ---- 350 metric train_loss = 0.40577872693538664\n",
      "classification ---- 360 metric train_loss = 0.40626190006732943\n",
      "classification ---- 370 metric train_loss = 0.3546911910176277\n",
      "classification ---- 380 metric train_loss = 0.39286760687828065\n",
      "classification ---- 390 metric train_loss = 0.37682513892650604\n",
      "classification ---- 400 metric train_loss = 0.3688902258872986\n",
      "classification ---- 410 metric train_loss = 0.37205690890550613\n",
      "classification ---- 420 metric train_loss = 0.29979349076747897\n",
      "classification ---- 430 metric train_loss = 0.3855568289756775\n",
      "classification ---- 440 metric train_loss = 0.3460630938410759\n",
      "classification ---- 450 metric train_loss = 0.3184566617012024\n",
      "classification ---- 460 metric train_loss = 0.3030711427330971\n",
      "classification ---- 470 metric train_loss = 0.3410548374056816\n",
      "classification ---- 480 metric train_loss = 0.3661384418606758\n",
      "classification ---- 490 metric train_loss = 0.31774930506944654\n",
      "classification ---- 500 metric train_loss = 0.29043611139059067\n",
      "classification ---- 2 metric test_f1 = 0.8517380083996745\n",
      "classification ---- 510 metric train_loss = 0.270347498357296\n",
      "classification ---- 520 metric train_loss = 0.27544914186000824\n",
      "classification ---- 530 metric train_loss = 0.24391032606363297\n",
      "classification ---- 540 metric train_loss = 0.18767937198281287\n",
      "classification ---- 550 metric train_loss = 0.23790534660220147\n",
      "classification ---- 560 metric train_loss = 0.224646607786417\n",
      "classification ---- 570 metric train_loss = 0.1903875693678856\n",
      "classification ---- 580 metric train_loss = 0.17242174744606018\n",
      "classification ---- 590 metric train_loss = 0.1789902500808239\n",
      "classification ---- 600 metric train_loss = 0.1523037426173687\n",
      "classification ---- 610 metric train_loss = 0.18359469920396804\n",
      "classification ---- 620 metric train_loss = 0.16624401211738588\n",
      "classification ---- 630 metric train_loss = 0.1654158391058445\n",
      "classification ---- 640 metric train_loss = 0.20121657028794288\n",
      "classification ---- 650 metric train_loss = 0.1803826004266739\n",
      "classification ---- 660 metric train_loss = 0.17152185887098312\n",
      "classification ---- 670 metric train_loss = 0.1677131772041321\n",
      "classification ---- 680 metric train_loss = 0.1408419206738472\n",
      "classification ---- 690 metric train_loss = 0.13899268247187138\n",
      "classification ---- 700 metric train_loss = 0.17425878271460532\n",
      "classification ---- 710 metric train_loss = 0.14642354883253575\n",
      "classification ---- 720 metric train_loss = 0.1513975728303194\n",
      "classification ---- 730 metric train_loss = 0.1731232065707445\n",
      "classification ---- 740 metric train_loss = 0.13291514441370963\n",
      "classification ---- 750 metric train_loss = 0.13232695832848548\n",
      "classification ---- 760 metric train_loss = 0.13988733477890491\n",
      "classification ---- 770 metric train_loss = 0.13901168592274188\n",
      "classification ---- 780 metric train_loss = 0.13476027362048626\n",
      "classification ---- 790 metric train_loss = 0.164196303114295\n",
      "classification ---- 800 metric train_loss = 0.13250782787799836\n",
      "classification ---- 810 metric train_loss = 0.1298161093145609\n",
      "classification ---- 820 metric train_loss = 0.12766030430793762\n",
      "classification ---- 830 metric train_loss = 0.09294603317975998\n",
      "classification ---- 840 metric train_loss = 0.11761379726231098\n",
      "classification ---- 850 metric train_loss = 0.09054958596825599\n",
      "classification ---- 860 metric train_loss = 0.15782550759613515\n",
      "classification ---- 870 metric train_loss = 0.16015881001949311\n",
      "classification ---- 880 metric train_loss = 0.12125430908054113\n",
      "classification ---- 890 metric train_loss = 0.1768275760114193\n",
      "classification ---- 900 metric train_loss = 0.10163808949291706\n",
      "classification ---- 910 metric train_loss = 0.09339938536286355\n",
      "classification ---- 920 metric train_loss = 0.09205236416310073\n",
      "classification ---- 930 metric train_loss = 0.12217066921293736\n",
      "classification ---- 940 metric train_loss = 0.1025200804695487\n",
      "classification ---- 950 metric train_loss = 0.07590759862214327\n",
      "classification ---- 960 metric train_loss = 0.10005112811923027\n",
      "classification ---- 970 metric train_loss = 0.129006452485919\n",
      "classification ---- 980 metric train_loss = 0.08598680477589368\n",
      "classification ---- 990 metric train_loss = 0.10209294110536575\n",
      "classification ---- 1000 metric train_loss = 0.10107685681432485\n",
      "classification ---- 3 metric test_f1 = 0.9764063257480715\n",
      "classification ---- 2 metric train_f1 = 0.9758032986920879\n",
      "classification ---- 1010 metric train_loss = 0.10195128582417964\n",
      "classification ---- 1020 metric train_loss = 0.09798471517860889\n",
      "classification ---- 1030 metric train_loss = 0.15131983421742917\n",
      "classification ---- 1040 metric train_loss = 0.10544806160032749\n",
      "classification ---- 1050 metric train_loss = 0.10628559403121471\n",
      "classification ---- 1060 metric train_loss = 0.07688268572092057\n",
      "classification ---- 1070 metric train_loss = 0.07796902060508729\n",
      "classification ---- 1080 metric train_loss = 0.159782306663692\n",
      "classification ---- 1090 metric train_loss = 0.09877818804234266\n",
      "classification ---- 1100 metric train_loss = 0.12456637583673\n",
      "classification ---- 1110 metric train_loss = 0.10535751301795244\n",
      "classification ---- 1120 metric train_loss = 0.0827113650739193\n",
      "classification ---- 1130 metric train_loss = 0.15484331790357828\n",
      "classification ---- 1140 metric train_loss = 0.1047211691737175\n",
      "classification ---- 1150 metric train_loss = 0.13101424556225538\n",
      "classification ---- 1160 metric train_loss = 0.12600704059004783\n",
      "classification ---- 1170 metric train_loss = 0.12194450795650483\n",
      "classification ---- 1180 metric train_loss = 0.10486578624695539\n",
      "classification ---- 1190 metric train_loss = 0.09655903093516827\n",
      "classification ---- 1200 metric train_loss = 0.11357755176723003\n",
      "classification ---- 1210 metric train_loss = 0.11416167765855789\n",
      "classification ---- 1220 metric train_loss = 0.1299163319170475\n",
      "classification ---- 1230 metric train_loss = 0.12342060878872871\n",
      "classification ---- 1240 metric train_loss = 0.11247022487223149\n",
      "classification ---- 1250 metric train_loss = 0.12919328436255456\n",
      "classification ---- 1260 metric train_loss = 0.08404841832816601\n",
      "classification ---- 1270 metric train_loss = 0.1613430567085743\n",
      "classification ---- 1280 metric train_loss = 0.08129482828080654\n",
      "classification ---- 1290 metric train_loss = 0.1070847800001502\n",
      "classification ---- 1300 metric train_loss = 0.11133911795914173\n",
      "classification ---- 1310 metric train_loss = 0.07550799995660781\n",
      "classification ---- 1320 metric train_loss = 0.10673684366047383\n",
      "classification ---- 1330 metric train_loss = 0.08864179737865925\n",
      "classification ---- 1340 metric train_loss = 0.12398663479834796\n",
      "classification ---- 1350 metric train_loss = 0.06036555655300617\n",
      "classification ---- 1360 metric train_loss = 0.1001508479937911\n",
      "classification ---- 1370 metric train_loss = 0.14744388349354268\n",
      "classification ---- 1380 metric train_loss = 0.1392499040812254\n",
      "classification ---- 1390 metric train_loss = 0.07390738353133201\n",
      "classification ---- 1400 metric train_loss = 0.1107028616592288\n",
      "classification ---- 1410 metric train_loss = 0.09743718151003122\n",
      "classification ---- 1420 metric train_loss = 0.11945252306759357\n",
      "classification ---- 1430 metric train_loss = 0.08850391209125519\n",
      "classification ---- 1440 metric train_loss = 0.13129540346562862\n",
      "classification ---- 1450 metric train_loss = 0.07400656789541245\n",
      "classification ---- 1460 metric train_loss = 0.060866588912904265\n",
      "classification ---- 1470 metric train_loss = 0.07857629731297493\n",
      "classification ---- 1480 metric train_loss = 0.13108406886458396\n",
      "classification ---- 1490 metric train_loss = 0.0808598544448614\n",
      "classification ---- 1500 metric train_loss = 0.11019987296313047\n",
      "classification ---- 4 metric test_f1 = 0.9795090816891034\n",
      "classification ---- 1510 metric train_loss = 0.0903185537084937\n",
      "classification ---- 1520 metric train_loss = 0.09777609147131443\n",
      "classification ---- 1530 metric train_loss = 0.07392103858292103\n",
      "classification ---- 1540 metric train_loss = 0.10091575663536786\n",
      "classification ---- 1550 metric train_loss = 0.12026400920003652\n",
      "classification ---- 1560 metric train_loss = 0.07644209321588277\n",
      "classification ---- 1570 metric train_loss = 0.14194020964205264\n",
      "classification ---- 1580 metric train_loss = 0.07907861825078726\n",
      "classification ---- 1590 metric train_loss = 0.07864398993551731\n",
      "classification ---- 1600 metric train_loss = 0.08394277729094028\n",
      "classification ---- 1610 metric train_loss = 0.08522639833390713\n",
      "classification ---- 1620 metric train_loss = 0.08213009871542454\n",
      "classification ---- 1630 metric train_loss = 0.08336852174252271\n",
      "classification ---- 1640 metric train_loss = 0.07760830726474524\n",
      "classification ---- 1650 metric train_loss = 0.09683294240385294\n",
      "classification ---- 1660 metric train_loss = 0.10271416557952762\n",
      "classification ---- 1670 metric train_loss = 0.07333843149244786\n",
      "classification ---- 1680 metric train_loss = 0.11585462149232625\n",
      "classification ---- 1690 metric train_loss = 0.06763225849717855\n",
      "classification ---- 1700 metric train_loss = 0.0959638861939311\n",
      "classification ---- 1710 metric train_loss = 0.07025364711880684\n",
      "classification ---- 1720 metric train_loss = 0.07161424132063984\n",
      "classification ---- 1730 metric train_loss = 0.06603893060237169\n",
      "classification ---- 1740 metric train_loss = 0.1083468584343791\n",
      "classification ---- 1750 metric train_loss = 0.11428913436830043\n",
      "classification ---- 1760 metric train_loss = 0.1108198719099164\n",
      "classification ---- 1770 metric train_loss = 0.12688490077853204\n",
      "classification ---- 1780 metric train_loss = 0.08424774874001742\n",
      "classification ---- 1790 metric train_loss = 0.08874622639268637\n",
      "classification ---- 1800 metric train_loss = 0.11735983192920685\n",
      "classification ---- 1810 metric train_loss = 0.10359629075974226\n",
      "classification ---- 1820 metric train_loss = 0.06690366696566344\n",
      "classification ---- 1830 metric train_loss = 0.05360268391668797\n",
      "classification ---- 1840 metric train_loss = 0.07467990033328534\n",
      "classification ---- 1850 metric train_loss = 0.08296823669224977\n",
      "classification ---- 1860 metric train_loss = 0.12037450596690177\n",
      "classification ---- 1870 metric train_loss = 0.08343890244141221\n",
      "classification ---- 1880 metric train_loss = 0.07189184110611677\n",
      "classification ---- 1890 metric train_loss = 0.09668665509670973\n",
      "classification ---- 1900 metric train_loss = 0.09781719762831927\n",
      "classification ---- 1910 metric train_loss = 0.05480611491948366\n",
      "classification ---- 1920 metric train_loss = 0.10348689332604408\n",
      "classification ---- 1930 metric train_loss = 0.07814918104559183\n",
      "classification ---- 1940 metric train_loss = 0.1040784491226077\n",
      "classification ---- 1950 metric train_loss = 0.08650356242433191\n",
      "classification ---- 1960 metric train_loss = 0.07513362467288971\n",
      "classification ---- 1970 metric train_loss = 0.08926383219659328\n",
      "classification ---- 1980 metric train_loss = 0.11058643795549869\n",
      "classification ---- 1990 metric train_loss = 0.0946262676268816\n",
      "classification ---- 2000 metric train_loss = 0.07430238630622625\n",
      "classification ---- 5 metric test_f1 = 0.9818800811936554\n",
      "classification ---- 3 metric train_f1 = 0.9817295859422405\n",
      "classification ---- 2010 metric train_loss = 0.071748963650316\n",
      "classification ---- 2020 metric train_loss = 0.11215466186404228\n",
      "classification ---- 2030 metric train_loss = 0.10109918881207705\n",
      "classification ---- 2040 metric train_loss = 0.05348119959235191\n",
      "classification ---- 2050 metric train_loss = 0.08178095817565918\n",
      "classification ---- 2060 metric train_loss = 0.1480522958561778\n",
      "classification ---- 2070 metric train_loss = 0.05697337677702308\n",
      "classification ---- 2080 metric train_loss = 0.07111104121431708\n",
      "classification ---- 2090 metric train_loss = 0.05132160345092416\n",
      "classification ---- 2100 metric train_loss = 0.06747512500733137\n",
      "classification ---- 2110 metric train_loss = 0.11013741083443165\n",
      "classification ---- 2120 metric train_loss = 0.08128745378926397\n",
      "classification ---- 2130 metric train_loss = 0.05982212712988257\n",
      "classification ---- 2140 metric train_loss = 0.08082915861159563\n",
      "classification ---- 2150 metric train_loss = 0.12347727101296187\n",
      "classification ---- 2160 metric train_loss = 0.08142382763326168\n",
      "classification ---- 2170 metric train_loss = 0.08292769445106388\n",
      "classification ---- 2180 metric train_loss = 0.09039081931114197\n",
      "classification ---- 2190 metric train_loss = 0.07106187418103219\n",
      "classification ---- 2200 metric train_loss = 0.05272286999970675\n",
      "classification ---- 2210 metric train_loss = 0.057714172266423704\n",
      "classification ---- 2220 metric train_loss = 0.0526771972887218\n",
      "classification ---- 2230 metric train_loss = 0.1249233189970255\n",
      "classification ---- 2240 metric train_loss = 0.06572220036759972\n",
      "classification ---- 2250 metric train_loss = 0.07048799004405737\n",
      "classification ---- 2260 metric train_loss = 0.06899210503324867\n",
      "classification ---- 2270 metric train_loss = 0.09686742275953293\n",
      "classification ---- 2280 metric train_loss = 0.09791564643383026\n",
      "classification ---- 2290 metric train_loss = 0.03858852544799447\n",
      "classification ---- 2300 metric train_loss = 0.0648249950259924\n",
      "classification ---- 2310 metric train_loss = 0.05575016504153609\n",
      "classification ---- 2320 metric train_loss = 0.14213140001520513\n",
      "classification ---- 2330 metric train_loss = 0.08058639988303185\n",
      "classification ---- 2340 metric train_loss = 0.09116246346384287\n",
      "classification ---- 2350 metric train_loss = 0.05164788141846657\n",
      "classification ---- 2360 metric train_loss = 0.12937311958521605\n",
      "classification ---- 2370 metric train_loss = 0.09494626251980662\n",
      "classification ---- 2380 metric train_loss = 0.04035550467669964\n",
      "classification ---- 2390 metric train_loss = 0.06566608576104045\n",
      "classification ---- 2400 metric train_loss = 0.12063589226454496\n",
      "classification ---- 2410 metric train_loss = 0.08401229446753859\n",
      "classification ---- 2420 metric train_loss = 0.12632435020059346\n",
      "classification ---- 2430 metric train_loss = 0.07941556172445416\n",
      "classification ---- 2440 metric train_loss = 0.09941148329526187\n",
      "classification ---- 2450 metric train_loss = 0.05778887085616589\n",
      "classification ---- 2460 metric train_loss = 0.15315986890345812\n",
      "classification ---- 2470 metric train_loss = 0.09799731709063053\n",
      "classification ---- 2480 metric train_loss = 0.08517866805195809\n",
      "classification ---- 2490 metric train_loss = 0.03927635177969933\n",
      "classification ---- 2500 metric train_loss = 0.12394457012414932\n",
      "classification ---- 6 metric test_f1 = 0.9823335166026732\n",
      "classification ---- 2510 metric train_loss = 0.062439491786062715\n",
      "classification ---- 2520 metric train_loss = 0.12642761040478945\n",
      "classification ---- 2530 metric train_loss = 0.07124568726867438\n",
      "classification ---- 2540 metric train_loss = 0.08266597855836152\n",
      "classification ---- 2550 metric train_loss = 0.10686682220548391\n",
      "classification ---- 2560 metric train_loss = 0.0735717692412436\n",
      "classification ---- 2570 metric train_loss = 0.08949948661029339\n",
      "classification ---- 2580 metric train_loss = 0.08944584438577294\n",
      "classification ---- 2590 metric train_loss = 0.07615964151918889\n",
      "classification ---- 2600 metric train_loss = 0.08227387620136142\n",
      "classification ---- 2610 metric train_loss = 0.06929010013118386\n",
      "classification ---- 2620 metric train_loss = 0.08933132980018854\n",
      "classification ---- 2630 metric train_loss = 0.09689065776765346\n",
      "classification ---- 2640 metric train_loss = 0.09624340943992138\n",
      "classification ---- 2650 metric train_loss = 0.11859866343438626\n",
      "classification ---- 2660 metric train_loss = 0.0747085951268673\n",
      "classification ---- 2670 metric train_loss = 0.11679796986281872\n",
      "classification ---- 2680 metric train_loss = 0.08650183714926243\n",
      "classification ---- 2690 metric train_loss = 0.10394815905019641\n",
      "classification ---- 2700 metric train_loss = 0.06731983702629804\n",
      "classification ---- 2710 metric train_loss = 0.092908273357898\n",
      "classification ---- 2720 metric train_loss = 0.068690069578588\n",
      "classification ---- 2730 metric train_loss = 0.10024542193859816\n",
      "classification ---- 2740 metric train_loss = 0.09263367727398872\n",
      "classification ---- 2750 metric train_loss = 0.0837511034682393\n",
      "classification ---- 2760 metric train_loss = 0.09247538689523935\n",
      "classification ---- 2770 metric train_loss = 0.05743790064007044\n",
      "classification ---- 2780 metric train_loss = 0.0817281236872077\n",
      "classification ---- 2790 metric train_loss = 0.08470531236380338\n",
      "classification ---- 2800 metric train_loss = 0.10153673123568296\n",
      "classification ---- 2810 metric train_loss = 0.08013257775455714\n",
      "classification ---- 2820 metric train_loss = 0.04722448159009218\n",
      "classification ---- 2830 metric train_loss = 0.07162464000284671\n",
      "classification ---- 2840 metric train_loss = 0.0808804857544601\n",
      "classification ---- 2850 metric train_loss = 0.11485646683722735\n",
      "classification ---- 2860 metric train_loss = 0.0799827091395855\n",
      "classification ---- 2870 metric train_loss = 0.0674203548580408\n",
      "classification ---- 2880 metric train_loss = 0.0988293368369341\n",
      "classification ---- 2890 metric train_loss = 0.07744811037555337\n",
      "classification ---- 2900 metric train_loss = 0.06718898881226779\n",
      "classification ---- 2910 metric train_loss = 0.07570809237658978\n",
      "classification ---- 2920 metric train_loss = 0.09356191297993063\n",
      "classification ---- 2930 metric train_loss = 0.09611405823379755\n",
      "classification ---- 2940 metric train_loss = 0.08280763551592826\n",
      "classification ---- 2950 metric train_loss = 0.07567882342264057\n",
      "classification ---- 2960 metric train_loss = 0.07591773411259055\n",
      "classification ---- 2970 metric train_loss = 0.0653589766472578\n",
      "classification ---- 2980 metric train_loss = 0.10206452338024974\n",
      "classification ---- 2990 metric train_loss = 0.09105036808177829\n",
      "classification ---- 3000 metric train_loss = 0.05981358299031854\n",
      "classification ---- 7 metric test_f1 = 0.9838530334021197\n",
      "classification ---- 4 metric train_f1 = 0.983368329577864\n",
      "classification ---- 3010 metric train_loss = 0.07874167244881392\n",
      "classification ---- 3020 metric train_loss = 0.08449534038081766\n",
      "classification ---- 3030 metric train_loss = 0.11890881638973952\n",
      "classification ---- 3040 metric train_loss = 0.0985191660001874\n",
      "classification ---- 3050 metric train_loss = 0.07183954957872629\n",
      "classification ---- 3060 metric train_loss = 0.10006939992308617\n",
      "classification ---- 3070 metric train_loss = 0.10743440948426723\n",
      "classification ---- 3080 metric train_loss = 0.0644029600545764\n",
      "classification ---- 3090 metric train_loss = 0.05970924319699407\n",
      "classification ---- 3100 metric train_loss = 0.10389189878478647\n",
      "classification ---- 3110 metric train_loss = 0.045306856837123634\n",
      "classification ---- 3120 metric train_loss = 0.06897640377283096\n",
      "classification ---- 3130 metric train_loss = 0.09535181252285838\n",
      "classification ---- 3140 metric train_loss = 0.0780862694606185\n",
      "classification ---- 3150 metric train_loss = 0.0573909793049097\n",
      "classification ---- 3160 metric train_loss = 0.08389338180422783\n",
      "classification ---- 3170 metric train_loss = 0.08334458135068416\n",
      "classification ---- 3180 metric train_loss = 0.09100155998021364\n",
      "classification ---- 3190 metric train_loss = 0.04485713122412562\n",
      "classification ---- 3200 metric train_loss = 0.057605139911174774\n",
      "classification ---- 3210 metric train_loss = 0.05660544391721487\n",
      "classification ---- 3220 metric train_loss = 0.08297816021367907\n",
      "classification ---- 3230 metric train_loss = 0.06589945331215859\n",
      "classification ---- 3240 metric train_loss = 0.09041701285168528\n",
      "classification ---- 3250 metric train_loss = 0.05746114756911993\n",
      "classification ---- 3260 metric train_loss = 0.06018713768571615\n",
      "classification ---- 3270 metric train_loss = 0.09460522700101137\n",
      "classification ---- 3280 metric train_loss = 0.11880376022309065\n",
      "classification ---- 3290 metric train_loss = 0.08437917567789555\n",
      "classification ---- 3300 metric train_loss = 0.14661293625831603\n",
      "classification ---- 3310 metric train_loss = 0.10681870076805353\n",
      "classification ---- 3320 metric train_loss = 0.05571210328489542\n",
      "classification ---- 3330 metric train_loss = 0.08911259360611438\n",
      "classification ---- 3340 metric train_loss = 0.09299963489174842\n",
      "classification ---- 3350 metric train_loss = 0.0760259222239256\n",
      "classification ---- 3360 metric train_loss = 0.09780587945133448\n",
      "classification ---- 3370 metric train_loss = 0.0627020863816142\n",
      "classification ---- 3380 metric train_loss = 0.08155654286965727\n",
      "classification ---- 3390 metric train_loss = 0.07050278708338738\n",
      "classification ---- 3400 metric train_loss = 0.06924481857568025\n",
      "classification ---- 3410 metric train_loss = 0.0592867155559361\n",
      "classification ---- 3420 metric train_loss = 0.05943353297188878\n",
      "classification ---- 3430 metric train_loss = 0.03600633107125759\n",
      "classification ---- 3440 metric train_loss = 0.05435367468744516\n",
      "classification ---- 3450 metric train_loss = 0.08536492036655545\n",
      "classification ---- 3460 metric train_loss = 0.08579550907015801\n",
      "classification ---- 3470 metric train_loss = 0.08037652503699064\n",
      "classification ---- 3480 metric train_loss = 0.08792881285771728\n",
      "classification ---- 3490 metric train_loss = 0.08505674563348294\n",
      "classification ---- 3500 metric train_loss = 0.06974842082709073\n",
      "classification ---- 8 metric test_f1 = 0.9836630564223818\n",
      "classification ---- 3510 metric train_loss = 0.10020991042256355\n",
      "classification ---- 3520 metric train_loss = 0.060441437922418115\n",
      "classification ---- 3530 metric train_loss = 0.10728305904194713\n",
      "classification ---- 3540 metric train_loss = 0.04837990691885352\n",
      "classification ---- 3550 metric train_loss = 0.12475818116217852\n",
      "classification ---- 3560 metric train_loss = 0.053184269554913045\n",
      "classification ---- 3570 metric train_loss = 0.07019629674032331\n",
      "classification ---- 3580 metric train_loss = 0.08160182302817702\n",
      "classification ---- 3590 metric train_loss = 0.0469891007989645\n",
      "classification ---- 3600 metric train_loss = 0.06265908228233456\n",
      "classification ---- 3610 metric train_loss = 0.07589602004736662\n",
      "classification ---- 3620 metric train_loss = 0.05322684561833739\n",
      "classification ---- 3630 metric train_loss = 0.08769494043663144\n",
      "classification ---- 3640 metric train_loss = 0.08878271467983723\n",
      "classification ---- 3650 metric train_loss = 0.09645439498126507\n",
      "classification ---- 3660 metric train_loss = 0.08388094250112772\n",
      "classification ---- 3670 metric train_loss = 0.11826631454750895\n",
      "classification ---- 3680 metric train_loss = 0.043884634599089624\n",
      "classification ---- 3690 metric train_loss = 0.089716112986207\n",
      "classification ---- 3700 metric train_loss = 0.032358741387724876\n",
      "classification ---- 3710 metric train_loss = 0.06416571149602532\n",
      "classification ---- 3720 metric train_loss = 0.11297494350001216\n",
      "classification ---- 3730 metric train_loss = 0.09392615705728531\n",
      "classification ---- 3740 metric train_loss = 0.06391960503533482\n",
      "classification ---- 3750 metric train_loss = 0.0748089225962758\n",
      "classification ---- 3760 metric train_loss = 0.09343836968764663\n",
      "classification ---- 3770 metric train_loss = 0.06546149868518114\n",
      "classification ---- 3780 metric train_loss = 0.05583037855103612\n",
      "classification ---- 3790 metric train_loss = 0.10700126588344575\n",
      "classification ---- 3800 metric train_loss = 0.05302963498979807\n",
      "classification ---- 3810 metric train_loss = 0.10362502355128526\n",
      "classification ---- 3820 metric train_loss = 0.09438848551362752\n",
      "classification ---- 3830 metric train_loss = 0.046084385551512244\n",
      "classification ---- 3840 metric train_loss = 0.0752729876898229\n",
      "classification ---- 3850 metric train_loss = 0.09096409967169165\n",
      "classification ---- 3860 metric train_loss = 0.05894830422475934\n",
      "classification ---- 3870 metric train_loss = 0.06196340462192893\n",
      "classification ---- 3880 metric train_loss = 0.0819029750302434\n",
      "classification ---- 3890 metric train_loss = 0.07738516069948673\n",
      "classification ---- 3900 metric train_loss = 0.08029662668704987\n",
      "classification ---- 3910 metric train_loss = 0.05821702955290675\n",
      "classification ---- 3920 metric train_loss = 0.084785461332649\n",
      "classification ---- 3930 metric train_loss = 0.051831288170069455\n",
      "classification ---- 3940 metric train_loss = 0.054643064923584464\n",
      "classification ---- 3950 metric train_loss = 0.0694998380728066\n",
      "classification ---- 3960 metric train_loss = 0.09615079183131456\n",
      "classification ---- 3970 metric train_loss = 0.10578491538763046\n",
      "classification ---- 3980 metric train_loss = 0.07526752948760987\n",
      "classification ---- 3990 metric train_loss = 0.06016887864097953\n",
      "classification ---- 4000 metric train_loss = 0.06776931937783956\n",
      "classification ---- 9 metric test_f1 = 0.9852536495210207\n",
      "classification ---- 5 metric train_f1 = 0.9853207747022238\n",
      "classification ---- 4010 metric train_loss = 0.07659784033894539\n",
      "classification ---- 4020 metric train_loss = 0.05789417698979378\n",
      "classification ---- 4030 metric train_loss = 0.06767092077061534\n",
      "classification ---- 4040 metric train_loss = 0.0857839840464294\n",
      "classification ---- 4050 metric train_loss = 0.09639270100742578\n",
      "classification ---- 4060 metric train_loss = 0.04903347687795758\n",
      "classification ---- 4070 metric train_loss = 0.06525242980569601\n",
      "classification ---- 4080 metric train_loss = 0.04132959777489305\n",
      "classification ---- 4090 metric train_loss = 0.045775563549250366\n",
      "classification ---- 4100 metric train_loss = 0.08396671898663044\n",
      "classification ---- 4110 metric train_loss = 0.06399155128747225\n",
      "classification ---- 4120 metric train_loss = 0.06405563959851861\n",
      "classification ---- 4130 metric train_loss = 0.12040755990892649\n",
      "classification ---- 4140 metric train_loss = 0.07369777709245681\n",
      "classification ---- 4150 metric train_loss = 0.10369886653497815\n",
      "classification ---- 4160 metric train_loss = 0.07048496576026082\n",
      "classification ---- 4170 metric train_loss = 0.05288356570526957\n",
      "classification ---- 4180 metric train_loss = 0.06380877466872334\n",
      "classification ---- 4190 metric train_loss = 0.1060872783884406\n",
      "classification ---- 4200 metric train_loss = 0.04502441789954901\n",
      "classification ---- 4210 metric train_loss = 0.051310271583497526\n",
      "classification ---- 4220 metric train_loss = 0.0877585775218904\n",
      "classification ---- 4230 metric train_loss = 0.08347708526998758\n",
      "classification ---- 4240 metric train_loss = 0.05073672430589795\n",
      "classification ---- 4250 metric train_loss = 0.10268305260688067\n",
      "classification ---- 4260 metric train_loss = 0.08358392398804426\n",
      "classification ---- 4270 metric train_loss = 0.05926038073375821\n",
      "classification ---- 4280 metric train_loss = 0.08564854953438043\n",
      "classification ---- 4290 metric train_loss = 0.08763233227655291\n",
      "classification ---- 4300 metric train_loss = 0.04853242551907897\n",
      "classification ---- 4310 metric train_loss = 0.07830876065418124\n",
      "classification ---- 4320 metric train_loss = 0.06717709954828024\n",
      "classification ---- 4330 metric train_loss = 0.1001749124377966\n",
      "classification ---- 4340 metric train_loss = 0.04540333366021514\n",
      "classification ---- 4350 metric train_loss = 0.08979290062561632\n",
      "classification ---- 4360 metric train_loss = 0.05459939250722527\n",
      "classification ---- 4370 metric train_loss = 0.05918368212878704\n",
      "classification ---- 4380 metric train_loss = 0.07466085059568286\n",
      "classification ---- 4390 metric train_loss = 0.09499217998236417\n",
      "classification ---- 4400 metric train_loss = 0.05878029884770512\n",
      "classification ---- 4410 metric train_loss = 0.06764153754338623\n",
      "classification ---- 4420 metric train_loss = 0.039531403221189976\n",
      "classification ---- 4430 metric train_loss = 0.07369003174826502\n",
      "classification ---- 4440 metric train_loss = 0.06270145829766989\n",
      "classification ---- 4450 metric train_loss = 0.07064824821427465\n",
      "classification ---- 4460 metric train_loss = 0.0583551594056189\n",
      "classification ---- 4470 metric train_loss = 0.05991092966869473\n",
      "classification ---- 4480 metric train_loss = 0.029749708343297245\n",
      "classification ---- 4490 metric train_loss = 0.05071947006508708\n",
      "classification ---- 4500 metric train_loss = 0.05995034677907825\n",
      "classification ---- 10 metric test_f1 = 0.9854019955783725\n",
      "classification ---- 4510 metric train_loss = 0.06289479695260525\n",
      "classification ---- 4520 metric train_loss = 0.04316626964136958\n",
      "classification ---- 4530 metric train_loss = 0.08294450603425503\n",
      "classification ---- 4540 metric train_loss = 0.0424039133824408\n",
      "classification ---- 4550 metric train_loss = 0.04922426156699657\n",
      "classification ---- 4560 metric train_loss = 0.06677468894049525\n",
      "classification ---- 4570 metric train_loss = 0.04765154751949012\n",
      "classification ---- 4580 metric train_loss = 0.051352718565613034\n",
      "classification ---- 4590 metric train_loss = 0.11377028515562415\n",
      "classification ---- 4600 metric train_loss = 0.05117854215204716\n",
      "classification ---- 4610 metric train_loss = 0.0362527484074235\n",
      "classification ---- 4620 metric train_loss = 0.06120104528963566\n",
      "classification ---- 4630 metric train_loss = 0.032307954877614974\n",
      "classification ---- 4640 metric train_loss = 0.04696463234722614\n",
      "classification ---- 4650 metric train_loss = 0.07891182145103812\n",
      "classification ---- 4660 metric train_loss = 0.07399468352086842\n",
      "classification ---- 4670 metric train_loss = 0.0534225401468575\n",
      "classification ---- 4680 metric train_loss = 0.08218312123790383\n",
      "classification ---- 4690 metric train_loss = 0.08478491241112351\n",
      "classification ---- 4700 metric train_loss = 0.06952444948256016\n",
      "classification ---- 4710 metric train_loss = 0.06151164453476667\n",
      "classification ---- 4720 metric train_loss = 0.05131118036806583\n",
      "classification ---- 4730 metric train_loss = 0.07028448143973946\n",
      "classification ---- 4740 metric train_loss = 0.06481204070150852\n",
      "classification ---- 4750 metric train_loss = 0.048294263519346715\n",
      "classification ---- 4760 metric train_loss = 0.0729655234143138\n",
      "classification ---- 4770 metric train_loss = 0.05084991259500384\n",
      "classification ---- 4780 metric train_loss = 0.07025372488424182\n",
      "classification ---- 4790 metric train_loss = 0.09409888237714767\n",
      "classification ---- 4800 metric train_loss = 0.04710783548653126\n",
      "classification ---- 4810 metric train_loss = 0.0668454235419631\n",
      "classification ---- 4820 metric train_loss = 0.06784342657774686\n",
      "classification ---- 4830 metric train_loss = 0.04491869937628508\n",
      "classification ---- 4840 metric train_loss = 0.03371154479682446\n",
      "classification ---- 4850 metric train_loss = 0.06690309140831233\n",
      "classification ---- 4860 metric train_loss = 0.04883524775505066\n",
      "classification ---- 4870 metric train_loss = 0.07899192329496145\n",
      "classification ---- 4880 metric train_loss = 0.04393386282026768\n",
      "classification ---- 4890 metric train_loss = 0.08429611837491394\n",
      "classification ---- 4900 metric train_loss = 0.07941282335668802\n",
      "classification ---- 4910 metric train_loss = 0.019257573690265418\n",
      "classification ---- 4920 metric train_loss = 0.07328583090566099\n",
      "classification ---- 4930 metric train_loss = 0.0637246573343873\n",
      "classification ---- 4940 metric train_loss = 0.05418204832822084\n",
      "classification ---- 4950 metric train_loss = 0.06149189090356231\n",
      "classification ---- 4960 metric train_loss = 0.07645531026646495\n",
      "classification ---- 4970 metric train_loss = 0.0625504283234477\n",
      "classification ---- 4980 metric train_loss = 0.05249324641190469\n",
      "classification ---- 4990 metric train_loss = 0.07893764283508062\n",
      "classification ---- 5000 metric train_loss = 0.08465347494930028\n",
      "classification ---- 11 metric test_f1 = 0.9863939781667272\n",
      "classification ---- 6 metric train_f1 = 0.9863322689361458\n",
      "classification ---- 5010 metric train_loss = 0.06352752977982164\n",
      "classification ---- 5020 metric train_loss = 0.05621695071458817\n",
      "classification ---- 5030 metric train_loss = 0.03983408780768514\n",
      "classification ---- 5040 metric train_loss = 0.05279284724965692\n",
      "classification ---- 5050 metric train_loss = 0.07618910195305943\n",
      "classification ---- 5060 metric train_loss = 0.07099642055109143\n",
      "classification ---- 5070 metric train_loss = 0.06736277621239424\n",
      "classification ---- 5080 metric train_loss = 0.05230957930907607\n",
      "classification ---- 5090 metric train_loss = 0.06334902942180634\n",
      "classification ---- 5100 metric train_loss = 0.06369058322161436\n",
      "classification ---- 5110 metric train_loss = 0.05232351515442133\n",
      "classification ---- 5120 metric train_loss = 0.0533934828825295\n",
      "classification ---- 5130 metric train_loss = 0.04202497201040387\n",
      "classification ---- 5140 metric train_loss = 0.06377385188825428\n",
      "classification ---- 5150 metric train_loss = 0.0770633613690734\n",
      "classification ---- 5160 metric train_loss = 0.09298980608582497\n",
      "classification ---- 5170 metric train_loss = 0.04451371757313609\n",
      "classification ---- 5180 metric train_loss = 0.04605031833052635\n",
      "classification ---- 5190 metric train_loss = 0.12232418702915311\n",
      "classification ---- 5200 metric train_loss = 0.08550802180543542\n",
      "classification ---- 5210 metric train_loss = 0.057030309550464155\n",
      "classification ---- 5220 metric train_loss = 0.11137609519064426\n",
      "classification ---- 5230 metric train_loss = 0.0690973655320704\n",
      "classification ---- 5240 metric train_loss = 0.07497309381142259\n",
      "classification ---- 5250 metric train_loss = 0.07468646131455899\n",
      "classification ---- 5260 metric train_loss = 0.05917163006961346\n",
      "classification ---- 5270 metric train_loss = 0.06004935232922435\n",
      "classification ---- 5280 metric train_loss = 0.07708479119464755\n",
      "classification ---- 5290 metric train_loss = 0.04980916501954198\n",
      "classification ---- 5300 metric train_loss = 0.07375466623343527\n",
      "classification ---- 5310 metric train_loss = 0.0789048437960446\n",
      "classification ---- 5320 metric train_loss = 0.030443035066127777\n",
      "classification ---- 5330 metric train_loss = 0.06555141406133771\n",
      "classification ---- 5340 metric train_loss = 0.08313835365697742\n",
      "classification ---- 5350 metric train_loss = 0.0632908427156508\n",
      "classification ---- 5360 metric train_loss = 0.08059179857373237\n",
      "classification ---- 5370 metric train_loss = 0.0795931987464428\n",
      "classification ---- 5380 metric train_loss = 0.068786003254354\n",
      "classification ---- 5390 metric train_loss = 0.07286323709413409\n",
      "classification ---- 5400 metric train_loss = 0.0569547463208437\n",
      "classification ---- 5410 metric train_loss = 0.12434147316962481\n",
      "classification ---- 5420 metric train_loss = 0.053202949557453395\n",
      "classification ---- 5430 metric train_loss = 0.061796959862113\n",
      "classification ---- 5440 metric train_loss = 0.05995026174932718\n",
      "classification ---- 5450 metric train_loss = 0.04232784127816558\n",
      "classification ---- 5460 metric train_loss = 0.060699986293911934\n",
      "classification ---- 5470 metric train_loss = 0.08397323973476886\n",
      "classification ---- 5480 metric train_loss = 0.03666302082128823\n",
      "classification ---- 5490 metric train_loss = 0.04660376780666411\n",
      "classification ---- 5500 metric train_loss = 0.06844722968526185\n",
      "classification ---- 12 metric test_f1 = 0.9868909797778902\n",
      "classification ---- 5510 metric train_loss = 0.03715037629008293\n",
      "classification ---- 5520 metric train_loss = 0.04690898791886866\n",
      "classification ---- 5530 metric train_loss = 0.06677212165668607\n",
      "classification ---- 5540 metric train_loss = 0.04051343807950616\n",
      "classification ---- 5550 metric train_loss = 0.06897977795451879\n",
      "classification ---- 5560 metric train_loss = 0.041997685376554725\n",
      "classification ---- 5570 metric train_loss = 0.06264690137468279\n",
      "classification ---- 5580 metric train_loss = 0.059443853050470355\n",
      "classification ---- 5590 metric train_loss = 0.07882875567302108\n",
      "classification ---- 5600 metric train_loss = 0.04319403204135597\n",
      "classification ---- 5610 metric train_loss = 0.03279390386305749\n",
      "classification ---- 5620 metric train_loss = 0.0523283907212317\n",
      "classification ---- 5630 metric train_loss = 0.04386920686811209\n",
      "classification ---- 5640 metric train_loss = 0.06938816327601671\n",
      "classification ---- 5650 metric train_loss = 0.06354184378869832\n",
      "classification ---- 5660 metric train_loss = 0.06737818354740739\n",
      "classification ---- 5670 metric train_loss = 0.08199374377727509\n",
      "classification ---- 5680 metric train_loss = 0.0614771718159318\n",
      "classification ---- 5690 metric train_loss = 0.05136120566166937\n",
      "classification ---- 5700 metric train_loss = 0.06282400330528617\n",
      "classification ---- 5710 metric train_loss = 0.042554224655032155\n",
      "classification ---- 5720 metric train_loss = 0.06511792177334427\n",
      "classification ---- 5730 metric train_loss = 0.052518607769161466\n",
      "classification ---- 5740 metric train_loss = 0.050834544003009796\n",
      "classification ---- 5750 metric train_loss = 0.055315398657694456\n",
      "classification ---- 5760 metric train_loss = 0.06624660221859813\n",
      "classification ---- 5770 metric train_loss = 0.05424021985381842\n",
      "classification ---- 5780 metric train_loss = 0.06515772463753819\n",
      "classification ---- 5790 metric train_loss = 0.037753020599484446\n",
      "classification ---- 5800 metric train_loss = 0.04840099890716374\n",
      "classification ---- 5810 metric train_loss = 0.0724797637667507\n",
      "classification ---- 5820 metric train_loss = 0.06260966202244163\n",
      "classification ---- 5830 metric train_loss = 0.06559230387210846\n",
      "classification ---- 5840 metric train_loss = 0.05913801388815045\n",
      "classification ---- 5850 metric train_loss = 0.03868303033523261\n",
      "classification ---- 5860 metric train_loss = 0.04056745730340481\n",
      "classification ---- 5870 metric train_loss = 0.09460132643580436\n",
      "classification ---- 5880 metric train_loss = 0.06293960586190224\n",
      "classification ---- 5890 metric train_loss = 0.039980785362422465\n",
      "classification ---- 5900 metric train_loss = 0.06919839242473244\n",
      "classification ---- 5910 metric train_loss = 0.08947205781005323\n",
      "classification ---- 5920 metric train_loss = 0.03002139227464795\n",
      "classification ---- 5930 metric train_loss = 0.08941164119169116\n",
      "classification ---- 5940 metric train_loss = 0.07924387985840439\n",
      "classification ---- 5950 metric train_loss = 0.03618678897619247\n",
      "classification ---- 5960 metric train_loss = 0.034305932465940714\n",
      "classification ---- 5970 metric train_loss = 0.06118405684828758\n",
      "classification ---- 5980 metric train_loss = 0.031957536051049826\n",
      "classification ---- 5990 metric train_loss = 0.04320842367596924\n",
      "classification ---- 6000 metric train_loss = 0.03923620567657053\n",
      "classification ---- 13 metric test_f1 = 0.9865940346185504\n",
      "classification ---- 7 metric train_f1 = 0.9869075447324377\n",
      "classification ---- 6010 metric train_loss = 0.043728857720270756\n",
      "classification ---- 6020 metric train_loss = 0.06243372312746942\n",
      "classification ---- 6030 metric train_loss = 0.053018342889845374\n",
      "classification ---- 6040 metric train_loss = 0.055022033862769604\n",
      "classification ---- 6050 metric train_loss = 0.08480921159498393\n",
      "classification ---- 6060 metric train_loss = 0.05411820495501161\n",
      "classification ---- 6070 metric train_loss = 0.04358779285103083\n",
      "classification ---- 6080 metric train_loss = 0.0630289435852319\n",
      "classification ---- 6090 metric train_loss = 0.06376566002145409\n",
      "classification ---- 6100 metric train_loss = 0.06190847577527166\n",
      "classification ---- 6110 metric train_loss = 0.06421736828051508\n",
      "classification ---- 6120 metric train_loss = 0.034280927013605834\n",
      "classification ---- 6130 metric train_loss = 0.06469799126498402\n",
      "classification ---- 6140 metric train_loss = 0.08605285324156284\n",
      "classification ---- 6150 metric train_loss = 0.05049424087628722\n",
      "classification ---- 6160 metric train_loss = 0.05456510093063116\n",
      "classification ---- 6170 metric train_loss = 0.05899341981858015\n",
      "classification ---- 6180 metric train_loss = 0.064362625265494\n",
      "classification ---- 6190 metric train_loss = 0.05925601520575583\n",
      "classification ---- 6200 metric train_loss = 0.04490631995722651\n",
      "classification ---- 6210 metric train_loss = 0.038533620070666075\n",
      "classification ---- 6220 metric train_loss = 0.07522727251052856\n",
      "classification ---- 6230 metric train_loss = 0.06276830099523067\n",
      "classification ---- 6240 metric train_loss = 0.05230801538564265\n",
      "classification ---- 6250 metric train_loss = 0.0631707794032991\n",
      "classification ---- 6260 metric train_loss = 0.09286330817267299\n",
      "classification ---- 6270 metric train_loss = 0.0669349467381835\n",
      "classification ---- 6280 metric train_loss = 0.06006256649270654\n",
      "classification ---- 6290 metric train_loss = 0.0614860825240612\n",
      "classification ---- 6300 metric train_loss = 0.052023020945489405\n",
      "classification ---- 6310 metric train_loss = 0.05333535745739937\n",
      "classification ---- 6320 metric train_loss = 0.04001525342464447\n",
      "classification ---- 6330 metric train_loss = 0.07270207684487104\n",
      "classification ---- 6340 metric train_loss = 0.04920863090083003\n",
      "classification ---- 6350 metric train_loss = 0.05485831191763282\n",
      "classification ---- 6360 metric train_loss = 0.05542293097823858\n",
      "classification ---- 6370 metric train_loss = 0.050325772259384394\n",
      "classification ---- 6380 metric train_loss = 0.0672484147362411\n",
      "classification ---- 6390 metric train_loss = 0.05914745982736349\n",
      "classification ---- 6400 metric train_loss = 0.06285417405888438\n",
      "classification ---- 6410 metric train_loss = 0.0450990891084075\n",
      "classification ---- 6420 metric train_loss = 0.07213735845871269\n",
      "classification ---- 6430 metric train_loss = 0.07014068569988012\n",
      "classification ---- 6440 metric train_loss = 0.06281270328909158\n",
      "classification ---- 6450 metric train_loss = 0.04889587564393878\n",
      "classification ---- 6460 metric train_loss = 0.02660314356908202\n",
      "classification ---- 6470 metric train_loss = 0.05154835050925612\n",
      "classification ---- 6480 metric train_loss = 0.029087895667180418\n",
      "classification ---- 6490 metric train_loss = 0.04054344217292964\n",
      "classification ---- 6500 metric train_loss = 0.07103942101821303\n",
      "classification ---- 14 metric test_f1 = 0.9876212127587704\n",
      "classification ---- 6510 metric train_loss = 0.07107853470370173\n",
      "classification ---- 6520 metric train_loss = 0.055847308691591026\n",
      "classification ---- 6530 metric train_loss = 0.02675868379883468\n",
      "classification ---- 6540 metric train_loss = 0.05333794741891325\n",
      "classification ---- 6550 metric train_loss = 0.06482376670464873\n",
      "classification ---- 6560 metric train_loss = 0.02864414071664214\n",
      "classification ---- 6570 metric train_loss = 0.06565232137218117\n",
      "classification ---- 6580 metric train_loss = 0.049071076046675446\n",
      "classification ---- 6590 metric train_loss = 0.03380723781883717\n",
      "classification ---- 6600 metric train_loss = 0.034847563272342084\n",
      "classification ---- 6610 metric train_loss = 0.04411004246212542\n",
      "classification ---- 6620 metric train_loss = 0.03947191121987999\n",
      "classification ---- 6630 metric train_loss = 0.08641280541196465\n",
      "classification ---- 6640 metric train_loss = 0.06231401991099119\n",
      "classification ---- 6650 metric train_loss = 0.027599338814616202\n",
      "classification ---- 6660 metric train_loss = 0.06404950027354062\n",
      "classification ---- 6670 metric train_loss = 0.07198015307076275\n",
      "classification ---- 6680 metric train_loss = 0.04207413783296943\n",
      "classification ---- 6690 metric train_loss = 0.045661556208506225\n",
      "classification ---- 6700 metric train_loss = 0.055655205901712176\n",
      "classification ---- 6710 metric train_loss = 0.05097790998406708\n",
      "classification ---- 6720 metric train_loss = 0.03812247500754893\n",
      "classification ---- 6730 metric train_loss = 0.031292102811858055\n",
      "classification ---- 6740 metric train_loss = 0.07503787437453865\n",
      "classification ---- 6750 metric train_loss = 0.044494699919596314\n",
      "classification ---- 6760 metric train_loss = 0.05690262010321021\n",
      "classification ---- 6770 metric train_loss = 0.061425118101760746\n",
      "classification ---- 6780 metric train_loss = 0.046904793055728075\n",
      "classification ---- 6790 metric train_loss = 0.03441391931846738\n",
      "classification ---- 6800 metric train_loss = 0.060811602603644134\n",
      "classification ---- 6810 metric train_loss = 0.03959880117326975\n",
      "classification ---- 6820 metric train_loss = 0.03998357206583023\n",
      "classification ---- 6830 metric train_loss = 0.06137474505230785\n",
      "classification ---- 6840 metric train_loss = 0.02872247905470431\n",
      "classification ---- 6850 metric train_loss = 0.03419050588272512\n",
      "classification ---- 6860 metric train_loss = 0.08101686583831906\n",
      "classification ---- 6870 metric train_loss = 0.05812609838321805\n",
      "classification ---- 6880 metric train_loss = 0.03483070251531899\n",
      "classification ---- 6890 metric train_loss = 0.0672551709227264\n",
      "classification ---- 6900 metric train_loss = 0.03371451278217137\n",
      "classification ---- 6910 metric train_loss = 0.020074006728827955\n",
      "classification ---- 6920 metric train_loss = 0.03953658170066774\n",
      "classification ---- 6930 metric train_loss = 0.05800215546041727\n",
      "classification ---- 6940 metric train_loss = 0.05581835308112204\n",
      "classification ---- 6950 metric train_loss = 0.058850351814180614\n",
      "classification ---- 6960 metric train_loss = 0.04972859984263778\n",
      "classification ---- 6970 metric train_loss = 0.10229138275608421\n",
      "classification ---- 6980 metric train_loss = 0.04739919761195779\n",
      "classification ---- 6990 metric train_loss = 0.062047891225665806\n",
      "classification ---- 7000 metric train_loss = 0.05570563520304859\n",
      "classification ---- 15 metric test_f1 = 0.987324226619305\n",
      "classification ---- 8 metric train_f1 = 0.9871194179545241\n",
      "classification ---- 7010 metric train_loss = 0.04035595594905317\n",
      "classification ---- 7020 metric train_loss = 0.041787368198856714\n",
      "classification ---- 7030 metric train_loss = 0.07398733710870146\n",
      "classification ---- 7040 metric train_loss = 0.049414051417261365\n",
      "classification ---- 7050 metric train_loss = 0.07616326957941055\n",
      "classification ---- 7060 metric train_loss = 0.05758190890774131\n",
      "classification ---- 7070 metric train_loss = 0.022551196999847888\n",
      "classification ---- 7080 metric train_loss = 0.05812739338725805\n",
      "classification ---- 7090 metric train_loss = 0.06357290660962463\n",
      "classification ---- 7100 metric train_loss = 0.0406427382491529\n",
      "classification ---- 7110 metric train_loss = 0.03240668624639511\n",
      "classification ---- 7120 metric train_loss = 0.07887671524658799\n",
      "classification ---- 7130 metric train_loss = 0.027338251285254956\n",
      "classification ---- 7140 metric train_loss = 0.06641064127907156\n",
      "classification ---- 7150 metric train_loss = 0.07296504732221365\n",
      "classification ---- 7160 metric train_loss = 0.030250873137265443\n",
      "classification ---- 7170 metric train_loss = 0.028139100689440966\n",
      "classification ---- 7180 metric train_loss = 0.07033679569140076\n",
      "classification ---- 7190 metric train_loss = 0.048529815999791025\n",
      "classification ---- 7200 metric train_loss = 0.06943404683843254\n",
      "classification ---- 7210 metric train_loss = 0.04603723068721592\n",
      "classification ---- 7220 metric train_loss = 0.05396699337288737\n",
      "classification ---- 7230 metric train_loss = 0.058901934511959554\n",
      "classification ---- 7240 metric train_loss = 0.07646604711189867\n",
      "classification ---- 7250 metric train_loss = 0.03688702387735247\n",
      "classification ---- 7260 metric train_loss = 0.039260470494627955\n",
      "classification ---- 7270 metric train_loss = 0.038186459941789506\n",
      "classification ---- 7280 metric train_loss = 0.05479568839073181\n",
      "classification ---- 7290 metric train_loss = 0.028704454144462942\n",
      "classification ---- 7300 metric train_loss = 0.03177133216522634\n",
      "classification ---- 7310 metric train_loss = 0.04237546524964273\n",
      "classification ---- 7320 metric train_loss = 0.0458267567679286\n",
      "classification ---- 7330 metric train_loss = 0.08803402381017804\n",
      "classification ---- 7340 metric train_loss = 0.0688074216246605\n",
      "classification ---- 7350 metric train_loss = 0.049001035559922454\n",
      "classification ---- 7360 metric train_loss = 0.056763273943215606\n",
      "classification ---- 7370 metric train_loss = 0.060160351498052475\n",
      "classification ---- 7380 metric train_loss = 0.06101121483370662\n",
      "classification ---- 7390 metric train_loss = 0.043018827121704815\n",
      "classification ---- 7400 metric train_loss = 0.07799247265793383\n",
      "classification ---- 7410 metric train_loss = 0.04008593568578363\n",
      "classification ---- 7420 metric train_loss = 0.03741281544789672\n",
      "classification ---- 7430 metric train_loss = 0.06039448454976082\n",
      "classification ---- 7440 metric train_loss = 0.04875704254955053\n",
      "classification ---- 7450 metric train_loss = 0.033321529626846313\n",
      "classification ---- 7460 metric train_loss = 0.05178844030015171\n",
      "classification ---- 7470 metric train_loss = 0.053831524681299925\n",
      "classification ---- 7480 metric train_loss = 0.03824345264583826\n",
      "classification ---- 7490 metric train_loss = 0.03451270023360849\n",
      "classification ---- 7500 metric train_loss = 0.053288594260811804\n",
      "classification ---- 16 metric test_f1 = 0.988636961503405\n",
      "classification ---- 7510 metric train_loss = 0.043152056029066445\n",
      "classification ---- 7520 metric train_loss = 0.03615085817873478\n",
      "classification ---- 7530 metric train_loss = 0.048953716503456236\n",
      "classification ---- 7540 metric train_loss = 0.0854299756232649\n",
      "classification ---- 7550 metric train_loss = 0.060581106878817084\n",
      "classification ---- 7560 metric train_loss = 0.06988055999390781\n",
      "classification ---- 7570 metric train_loss = 0.06640400453470648\n",
      "classification ---- 7580 metric train_loss = 0.07519928691908717\n",
      "classification ---- 7590 metric train_loss = 0.06163394469767809\n",
      "classification ---- 7600 metric train_loss = 0.06337819425389171\n",
      "classification ---- 7610 metric train_loss = 0.05519316694699228\n",
      "classification ---- 7620 metric train_loss = 0.059820494474843146\n",
      "classification ---- 7630 metric train_loss = 0.04751969403587282\n",
      "classification ---- 7640 metric train_loss = 0.08242324246093631\n",
      "classification ---- 7650 metric train_loss = 0.05579712172038853\n",
      "classification ---- 7660 metric train_loss = 0.07752656005322933\n",
      "classification ---- 7670 metric train_loss = 0.05158799570053816\n",
      "classification ---- 7680 metric train_loss = 0.04050517068244517\n",
      "classification ---- 7690 metric train_loss = 0.05439107595011592\n",
      "classification ---- 7700 metric train_loss = 0.03790571736171842\n",
      "classification ---- 7710 metric train_loss = 0.027904788637533783\n",
      "classification ---- 7720 metric train_loss = 0.05403709611855447\n",
      "classification ---- 7730 metric train_loss = 0.060416995035484436\n",
      "classification ---- 7740 metric train_loss = 0.07385380053892732\n",
      "classification ---- 7750 metric train_loss = 0.07140645673498511\n",
      "classification ---- 7760 metric train_loss = 0.03766401233151555\n",
      "classification ---- 7770 metric train_loss = 0.07532170321792364\n",
      "classification ---- 7780 metric train_loss = 0.05715777250006795\n",
      "classification ---- 7790 metric train_loss = 0.050120291672647\n",
      "classification ---- 7800 metric train_loss = 0.03272134768776595\n",
      "classification ---- 7810 metric train_loss = 0.06730428170412779\n",
      "classification ---- 7820 metric train_loss = 0.07321886094287038\n",
      "classification ---- 7830 metric train_loss = 0.06958060478791595\n",
      "classification ---- 7840 metric train_loss = 0.024366274103522302\n",
      "classification ---- 7850 metric train_loss = 0.06748009012080729\n",
      "classification ---- 7860 metric train_loss = 0.03818712760694325\n",
      "classification ---- 7870 metric train_loss = 0.06279009953141212\n",
      "classification ---- 7880 metric train_loss = 0.062147276010364294\n",
      "classification ---- 7890 metric train_loss = 0.04510750863701105\n",
      "classification ---- 7900 metric train_loss = 0.06708414019085467\n",
      "classification ---- 7910 metric train_loss = 0.08843070203438401\n",
      "classification ---- 7920 metric train_loss = 0.05876059038564563\n",
      "classification ---- 7930 metric train_loss = 0.07182128597050905\n",
      "classification ---- 7940 metric train_loss = 0.0600660540163517\n",
      "classification ---- 7950 metric train_loss = 0.055199875682592395\n",
      "classification ---- 7960 metric train_loss = 0.051888156402856114\n",
      "classification ---- 7970 metric train_loss = 0.052416935376822946\n",
      "classification ---- 7980 metric train_loss = 0.05998571030795574\n",
      "classification ---- 7990 metric train_loss = 0.03113028220832348\n",
      "classification ---- 8000 metric train_loss = 0.06603265861049294\n",
      "classification ---- 17 metric test_f1 = 0.9885458215099038\n",
      "classification ---- 9 metric train_f1 = 0.9883642287373358\n",
      "classification ---- 8010 metric train_loss = 0.05331078222952783\n",
      "classification ---- 8020 metric train_loss = 0.05402389271184802\n",
      "classification ---- 8030 metric train_loss = 0.027407587878406046\n",
      "classification ---- 8040 metric train_loss = 0.03406820483505726\n",
      "classification ---- 8050 metric train_loss = 0.03421128117479384\n",
      "classification ---- 8060 metric train_loss = 0.02662404072470963\n",
      "classification ---- 8070 metric train_loss = 0.03586744265630841\n",
      "classification ---- 8080 metric train_loss = 0.06601947732269764\n",
      "classification ---- 8090 metric train_loss = 0.05288941138423979\n",
      "classification ---- 8100 metric train_loss = 0.08231873582117259\n",
      "classification ---- 8110 metric train_loss = 0.03276492143049836\n",
      "classification ---- 8120 metric train_loss = 0.0665232490748167\n",
      "classification ---- 8130 metric train_loss = 0.062110749166458847\n",
      "classification ---- 8140 metric train_loss = 0.02764544291421771\n",
      "classification ---- 8150 metric train_loss = 0.03634428521618247\n",
      "classification ---- 8160 metric train_loss = 0.05145167033188045\n",
      "classification ---- 8170 metric train_loss = 0.05220345463603735\n",
      "classification ---- 8180 metric train_loss = 0.08508027726784348\n",
      "classification ---- 8190 metric train_loss = 0.07252872660756111\n",
      "classification ---- 8200 metric train_loss = 0.03358530383557081\n",
      "classification ---- 8210 metric train_loss = 0.04774416261352599\n",
      "classification ---- 8220 metric train_loss = 0.0282925508916378\n",
      "classification ---- 8230 metric train_loss = 0.04343981631100178\n",
      "classification ---- 8240 metric train_loss = 0.05620709843933582\n",
      "classification ---- 8250 metric train_loss = 0.07339539118111134\n",
      "classification ---- 8260 metric train_loss = 0.057559700217098\n",
      "classification ---- 8270 metric train_loss = 0.04850349146872759\n",
      "classification ---- 8280 metric train_loss = 0.04687746949493885\n",
      "classification ---- 8290 metric train_loss = 0.06525744558312\n",
      "classification ---- 8300 metric train_loss = 0.04764531776309013\n",
      "classification ---- 8310 metric train_loss = 0.017815779941156507\n",
      "classification ---- 8320 metric train_loss = 0.05246586496941745\n",
      "classification ---- 8330 metric train_loss = 0.024368066200986506\n",
      "classification ---- 8340 metric train_loss = 0.04844881622120738\n",
      "classification ---- 8350 metric train_loss = 0.03307605255395174\n",
      "classification ---- 8360 metric train_loss = 0.04586934819817543\n",
      "classification ---- 8370 metric train_loss = 0.05180284730158746\n",
      "classification ---- 8380 metric train_loss = 0.04493342856876552\n",
      "classification ---- 8390 metric train_loss = 0.0711191994138062\n",
      "classification ---- 8400 metric train_loss = 0.029394200490787625\n",
      "classification ---- 8410 metric train_loss = 0.07080638562329114\n",
      "classification ---- 8420 metric train_loss = 0.05840679090470076\n",
      "classification ---- 8430 metric train_loss = 0.06111937863752246\n",
      "classification ---- 8440 metric train_loss = 0.03044055886566639\n",
      "classification ---- 8450 metric train_loss = 0.03569510825909674\n",
      "classification ---- 8460 metric train_loss = 0.048866492975503203\n",
      "classification ---- 8470 metric train_loss = 0.03099140776321292\n",
      "classification ---- 8480 metric train_loss = 0.014019483420997857\n",
      "classification ---- 8490 metric train_loss = 0.03573410836979747\n",
      "classification ---- 8500 metric train_loss = 0.05205542766489089\n",
      "classification ---- 18 metric test_f1 = 0.9896041623220786\n",
      "classification ---- 8510 metric train_loss = 0.04422556236386299\n",
      "classification ---- 8520 metric train_loss = 0.022245249710977077\n",
      "classification ---- 8530 metric train_loss = 0.033781929733231665\n",
      "classification ---- 8540 metric train_loss = 0.04409132148139179\n",
      "classification ---- 8550 metric train_loss = 0.04384650255087763\n",
      "classification ---- 8560 metric train_loss = 0.01665348410606384\n",
      "classification ---- 8570 metric train_loss = 0.017082590563222765\n",
      "classification ---- 8580 metric train_loss = 0.053687893832102415\n",
      "classification ---- 8590 metric train_loss = 0.04599653242621571\n",
      "classification ---- 8600 metric train_loss = 0.03183590359985829\n",
      "classification ---- 8610 metric train_loss = 0.06852494082413614\n",
      "classification ---- 8620 metric train_loss = 0.06131688505411148\n",
      "classification ---- 8630 metric train_loss = 0.021888331230729818\n",
      "classification ---- 8640 metric train_loss = 0.033149234065786\n",
      "classification ---- 8650 metric train_loss = 0.048137965984642504\n",
      "classification ---- 8660 metric train_loss = 0.046034139860421416\n",
      "classification ---- 8670 metric train_loss = 0.03689527614042163\n",
      "classification ---- 8680 metric train_loss = 0.05363977318629622\n",
      "classification ---- 8690 metric train_loss = 0.04473768644966185\n",
      "classification ---- 8700 metric train_loss = 0.06886469442397355\n",
      "classification ---- 8710 metric train_loss = 0.05583480130881071\n",
      "classification ---- 8720 metric train_loss = 0.024139765463769437\n",
      "classification ---- 8730 metric train_loss = 0.03042158796451986\n",
      "classification ---- 8740 metric train_loss = 0.05073457178659737\n",
      "classification ---- 8750 metric train_loss = 0.05290759396739304\n",
      "classification ---- 8760 metric train_loss = 0.019900991721078752\n",
      "classification ---- 8770 metric train_loss = 0.05117036555893719\n",
      "classification ---- 8780 metric train_loss = 0.06740363584831358\n",
      "classification ---- 8790 metric train_loss = 0.07733789710327983\n",
      "classification ---- 8800 metric train_loss = 0.0419257499743253\n",
      "classification ---- 8810 metric train_loss = 0.030943856993690132\n",
      "classification ---- 8820 metric train_loss = 0.02770638931542635\n",
      "classification ---- 8830 metric train_loss = 0.07209240384399891\n",
      "classification ---- 8840 metric train_loss = 0.05513209069613367\n",
      "classification ---- 8850 metric train_loss = 0.09759148862212896\n",
      "classification ---- 8860 metric train_loss = 0.07924956167116762\n",
      "classification ---- 8870 metric train_loss = 0.054135283641517165\n",
      "classification ---- 8880 metric train_loss = 0.04489410435780883\n",
      "classification ---- 8890 metric train_loss = 0.06642158352769911\n",
      "classification ---- 8900 metric train_loss = 0.07342309718951583\n",
      "classification ---- 8910 metric train_loss = 0.07292854283004999\n",
      "classification ---- 8920 metric train_loss = 0.03239253610372543\n",
      "classification ---- 8930 metric train_loss = 0.07210921095684171\n",
      "classification ---- 8940 metric train_loss = 0.030387610010802747\n",
      "classification ---- 8950 metric train_loss = 0.08312036171555519\n",
      "classification ---- 8960 metric train_loss = 0.028684154571965336\n",
      "classification ---- 8970 metric train_loss = 0.03571425196714699\n",
      "classification ---- 8980 metric train_loss = 0.03916748482733965\n",
      "classification ---- 8990 metric train_loss = 0.0673775321803987\n",
      "classification ---- 9000 metric train_loss = 0.0287992374971509\n",
      "classification ---- 19 metric test_f1 = 0.9898340052786967\n",
      "classification ---- 10 metric train_f1 = 0.9900575308536411\n",
      "classification ---- 9010 metric train_loss = 0.033983420999720695\n",
      "classification ---- 9020 metric train_loss = 0.05677777044475078\n",
      "classification ---- 9030 metric train_loss = 0.029110665433108807\n",
      "classification ---- 9040 metric train_loss = 0.023401577863842248\n",
      "classification ---- 9050 metric train_loss = 0.015641335328109563\n",
      "classification ---- 9060 metric train_loss = 0.05003222040832043\n",
      "classification ---- 9070 metric train_loss = 0.05725477682426572\n",
      "classification ---- 9080 metric train_loss = 0.05433823801577091\n",
      "classification ---- 9090 metric train_loss = 0.0321980725042522\n",
      "classification ---- 9100 metric train_loss = 0.034430901519954205\n",
      "classification ---- 9110 metric train_loss = 0.04528764085844159\n",
      "classification ---- 9120 metric train_loss = 0.023350189346820115\n",
      "classification ---- 9130 metric train_loss = 0.04699037778191269\n",
      "classification ---- 9140 metric train_loss = 0.03979292036965489\n",
      "classification ---- 9150 metric train_loss = 0.060466906521469355\n",
      "classification ---- 9160 metric train_loss = 0.021548298932611943\n",
      "classification ---- 9170 metric train_loss = 0.03246718430891633\n",
      "classification ---- 9180 metric train_loss = 0.049908910505473615\n",
      "classification ---- 9190 metric train_loss = 0.035177734680473804\n",
      "classification ---- 9200 metric train_loss = 0.05792306419461966\n",
      "classification ---- 9210 metric train_loss = 0.02872025449760258\n",
      "classification ---- 9220 metric train_loss = 0.04725604015402496\n",
      "classification ---- 9230 metric train_loss = 0.07828920581378043\n",
      "classification ---- 9240 metric train_loss = 0.04316321685910225\n",
      "classification ---- 9250 metric train_loss = 0.03709486490115523\n",
      "classification ---- 9260 metric train_loss = 0.038854305446147916\n",
      "classification ---- 9270 metric train_loss = 0.026988916285336017\n",
      "classification ---- 9280 metric train_loss = 0.013458938896656036\n",
      "classification ---- 9290 metric train_loss = 0.0363773507066071\n",
      "classification ---- 9300 metric train_loss = 0.02662816531956196\n",
      "classification ---- 9310 metric train_loss = 0.044638143852353096\n",
      "classification ---- 9320 metric train_loss = 0.05333854202181101\n",
      "classification ---- 9330 metric train_loss = 0.1150766359642148\n",
      "classification ---- 9340 metric train_loss = 0.04739967356435955\n",
      "classification ---- 9350 metric train_loss = 0.05120063382200897\n",
      "classification ---- 9360 metric train_loss = 0.03786363732069731\n",
      "classification ---- 9370 metric train_loss = 0.06922411303967238\n",
      "classification ---- 9380 metric train_loss = 0.042114245845004915\n",
      "classification ---- 9390 metric train_loss = 0.034053205605596305\n",
      "classification ---- 9400 metric train_loss = 0.07735146363265813\n",
      "classification ---- 9410 metric train_loss = 0.038508287630975246\n",
      "classification ---- 9420 metric train_loss = 0.040895653143525126\n",
      "classification ---- 9430 metric train_loss = 0.029118345584720373\n",
      "classification ---- 9440 metric train_loss = 0.030652123037725687\n",
      "classification ---- 9450 metric train_loss = 0.03306029085069895\n",
      "classification ---- 9460 metric train_loss = 0.017120322817936538\n",
      "classification ---- 9470 metric train_loss = 0.013865357078611851\n",
      "classification ---- 9480 metric train_loss = 0.034894608147442344\n",
      "classification ---- 9490 metric train_loss = 0.04989229184575379\n",
      "classification ---- 9500 metric train_loss = 0.04423497307579964\n",
      "classification ---- 20 metric test_f1 = 0.9903269753308989\n",
      "classification ---- 9510 metric train_loss = 0.013628529151901603\n",
      "classification ---- 9520 metric train_loss = 0.045503472653217615\n",
      "classification ---- 9530 metric train_loss = 0.056232481310144064\n",
      "classification ---- 9540 metric train_loss = 0.05849570434074849\n",
      "classification ---- 9550 metric train_loss = 0.044452054891735315\n",
      "classification ---- 9560 metric train_loss = 0.02753305630758405\n",
      "classification ---- 9570 metric train_loss = 0.06355358082801103\n",
      "classification ---- 9580 metric train_loss = 0.056214344687759876\n",
      "classification ---- 9590 metric train_loss = 0.05392465838231146\n",
      "classification ---- 9600 metric train_loss = 0.038108667638152835\n",
      "classification ---- 9610 metric train_loss = 0.03807595376856625\n",
      "classification ---- 9620 metric train_loss = 0.06718161469325423\n",
      "classification ---- 9630 metric train_loss = 0.03965218500234187\n",
      "classification ---- 9640 metric train_loss = 0.04213895271532238\n",
      "classification ---- 9650 metric train_loss = 0.029631513077765702\n",
      "classification ---- 9660 metric train_loss = 0.03782208482734859\n",
      "classification ---- 9670 metric train_loss = 0.03975951988250017\n",
      "classification ---- 9680 metric train_loss = 0.04700506837107241\n",
      "classification ---- 9690 metric train_loss = 0.03609575289301574\n",
      "classification ---- 9700 metric train_loss = 0.03759978455491364\n",
      "classification ---- 9710 metric train_loss = 0.02365693389438093\n",
      "classification ---- 9720 metric train_loss = 0.052008925331756474\n",
      "classification ---- 9730 metric train_loss = 0.06980952182784676\n",
      "classification ---- 9740 metric train_loss = 0.03898139907978475\n",
      "classification ---- 9750 metric train_loss = 0.03673913129605353\n",
      "classification ---- 9760 metric train_loss = 0.04702248994726688\n",
      "classification ---- 9770 metric train_loss = 0.053390723653137684\n",
      "classification ---- 9780 metric train_loss = 0.02987973066046834\n",
      "classification ---- 9790 metric train_loss = 0.054526261938735845\n",
      "classification ---- 9800 metric train_loss = 0.035538717079907654\n",
      "classification ---- 9810 metric train_loss = 0.043217006418853995\n",
      "classification ---- 9820 metric train_loss = 0.054417862789705396\n",
      "classification ---- 9830 metric train_loss = 0.06504391185007989\n",
      "classification ---- 9840 metric train_loss = 0.06046961219981313\n",
      "classification ---- 9850 metric train_loss = 0.027552309213206172\n",
      "classification ---- 9860 metric train_loss = 0.029589987266808747\n",
      "classification ---- 9870 metric train_loss = 0.03562092864885926\n",
      "classification ---- 9880 metric train_loss = 0.06680877176113427\n",
      "classification ---- 9890 metric train_loss = 0.04276521997526288\n",
      "classification ---- 9900 metric train_loss = 0.055383470840752125\n",
      "classification ---- 9910 metric train_loss = 0.03238288471475244\n",
      "classification ---- 9920 metric train_loss = 0.044732791977003215\n",
      "classification ---- 9930 metric train_loss = 0.04334501489065588\n",
      "classification ---- 9940 metric train_loss = 0.03514616386964917\n",
      "classification ---- 9950 metric train_loss = 0.032692142529413104\n",
      "classification ---- 9960 metric train_loss = 0.03142936988733709\n",
      "classification ---- 9970 metric train_loss = 0.046272093802690505\n",
      "classification ---- 9980 metric train_loss = 0.05296038626693189\n",
      "classification ---- 9990 metric train_loss = 0.04824076034128666\n",
      "classification ---- 10000 metric train_loss = 0.020298694469965996\n",
      "classification ---- 21 metric test_f1 = 0.9908211072743734\n",
      "classification ---- 11 metric train_f1 = 0.9902073725735181\n",
      "classification ---- 10010 metric train_loss = 0.07543639126233756\n",
      "classification ---- 10020 metric train_loss = 0.043821080564521254\n",
      "classification ---- 10030 metric train_loss = 0.04190406440757215\n",
      "classification ---- 10040 metric train_loss = 0.043401315016672014\n",
      "classification ---- 10050 metric train_loss = 0.02141882460564375\n",
      "classification ---- 10060 metric train_loss = 0.04675029255449772\n",
      "classification ---- 10070 metric train_loss = 0.0830901593901217\n",
      "classification ---- 10080 metric train_loss = 0.04778525554575026\n",
      "classification ---- 10090 metric train_loss = 0.05577799766324461\n",
      "classification ---- 10100 metric train_loss = 0.04456056728959083\n",
      "classification ---- 10110 metric train_loss = 0.021112557547166944\n",
      "classification ---- 10120 metric train_loss = 0.01696990756317973\n",
      "classification ---- 10130 metric train_loss = 0.03268543058075011\n",
      "classification ---- 10140 metric train_loss = 0.052081697667017576\n",
      "classification ---- 10150 metric train_loss = 0.04409552151337266\n",
      "classification ---- 10160 metric train_loss = 0.04071124591864646\n",
      "classification ---- 10170 metric train_loss = 0.05079950932413339\n",
      "classification ---- 10180 metric train_loss = 0.08227916462346911\n",
      "classification ---- 10190 metric train_loss = 0.04766066395677626\n",
      "classification ---- 10200 metric train_loss = 0.05173305664211512\n",
      "classification ---- 10210 metric train_loss = 0.03639888153411448\n",
      "classification ---- 10220 metric train_loss = 0.033159861853346226\n",
      "classification ---- 10230 metric train_loss = 0.02323161461390555\n",
      "classification ---- 10240 metric train_loss = 0.03607455610763281\n",
      "classification ---- 10250 metric train_loss = 0.047499409737065436\n",
      "classification ---- 10260 metric train_loss = 0.035087835509330034\n",
      "classification ---- 10270 metric train_loss = 0.01955710304901004\n",
      "classification ---- 10280 metric train_loss = 0.027693508099764584\n",
      "classification ---- 10290 metric train_loss = 0.017510858504101633\n",
      "classification ---- 10300 metric train_loss = 0.055560717545449737\n",
      "classification ---- 10310 metric train_loss = 0.03229191335849464\n",
      "classification ---- 10320 metric train_loss = 0.054597626067698\n",
      "classification ---- 10330 metric train_loss = 0.08441699952818453\n",
      "classification ---- 10340 metric train_loss = 0.03824560446664691\n",
      "classification ---- 10350 metric train_loss = 0.04696018914692104\n",
      "classification ---- 10360 metric train_loss = 0.015507202269509435\n",
      "classification ---- 10370 metric train_loss = 0.009994440176524223\n",
      "classification ---- 10380 metric train_loss = 0.03287742519751191\n",
      "classification ---- 10390 metric train_loss = 0.02214335473254323\n",
      "classification ---- 10400 metric train_loss = 0.052357067633420226\n",
      "classification ---- 10410 metric train_loss = 0.041391824698075654\n",
      "classification ---- 10420 metric train_loss = 0.04023715555667877\n",
      "classification ---- 10430 metric train_loss = 0.04874731539748609\n",
      "classification ---- 10440 metric train_loss = 0.053896953468210995\n",
      "classification ---- 10450 metric train_loss = 0.05798968197777867\n",
      "classification ---- 10460 metric train_loss = 0.04597669746726751\n",
      "classification ---- 10470 metric train_loss = 0.01608686367981136\n",
      "classification ---- 10480 metric train_loss = 0.02243401613086462\n",
      "classification ---- 10490 metric train_loss = 0.031481974828056994\n",
      "classification ---- 10500 metric train_loss = 0.07484235232695938\n",
      "classification ---- 22 metric test_f1 = 0.9861376829025826\n",
      "classification ---- 10510 metric train_loss = 0.04602840389125049\n",
      "classification ---- 10520 metric train_loss = 0.03485484472475946\n",
      "classification ---- 10530 metric train_loss = 0.030990008357912303\n",
      "classification ---- 10540 metric train_loss = 0.039677589852362874\n",
      "classification ---- 10550 metric train_loss = 0.01922815148718655\n",
      "classification ---- 10560 metric train_loss = 0.04191070096567273\n",
      "classification ---- 10570 metric train_loss = 0.054204375250265\n",
      "classification ---- 10580 metric train_loss = 0.038841018080711366\n",
      "classification ---- 10590 metric train_loss = 0.03953406368382275\n",
      "classification ---- 10600 metric train_loss = 0.04671741984784603\n",
      "classification ---- 10610 metric train_loss = 0.02811431819573045\n",
      "classification ---- 10620 metric train_loss = 0.018075635912828146\n",
      "classification ---- 10630 metric train_loss = 0.043476254865527156\n",
      "classification ---- 10640 metric train_loss = 0.020743725774809717\n",
      "classification ---- 10650 metric train_loss = 0.03361319936811924\n",
      "classification ---- 10660 metric train_loss = 0.05151476007886231\n",
      "classification ---- 10670 metric train_loss = 0.0349496244918555\n",
      "classification ---- 10680 metric train_loss = 0.03342758386861533\n",
      "classification ---- 10690 metric train_loss = 0.03692806749604642\n",
      "classification ---- 10700 metric train_loss = 0.022160884412005543\n",
      "classification ---- 10710 metric train_loss = 0.05688397069461644\n",
      "classification ---- 10720 metric train_loss = 0.045201726444065574\n",
      "classification ---- 10730 metric train_loss = 0.05130152686033398\n",
      "classification ---- 10740 metric train_loss = 0.04332985607907176\n",
      "classification ---- 10750 metric train_loss = 0.028687495039775968\n",
      "classification ---- 10760 metric train_loss = 0.06230601016432047\n",
      "classification ---- 10770 metric train_loss = 0.05367792742326856\n",
      "classification ---- 10780 metric train_loss = 0.05210125166922808\n",
      "classification ---- 10790 metric train_loss = 0.05900474861264229\n",
      "classification ---- 10800 metric train_loss = 0.03942781277000904\n",
      "classification ---- 10810 metric train_loss = 0.015412950236350299\n",
      "classification ---- 10820 metric train_loss = 0.052709990506991745\n",
      "classification ---- 10830 metric train_loss = 0.03004860922228545\n",
      "classification ---- 10840 metric train_loss = 0.029175081895664333\n",
      "classification ---- 10850 metric train_loss = 0.03860367201268673\n",
      "classification ---- 10860 metric train_loss = 0.05088712512515485\n",
      "classification ---- 10870 metric train_loss = 0.051301247673109174\n",
      "classification ---- 10880 metric train_loss = 0.027860858617350458\n",
      "classification ---- 10890 metric train_loss = 0.04325736030004919\n",
      "classification ---- 10900 metric train_loss = 0.03625361085869372\n",
      "classification ---- 10910 metric train_loss = 0.03393292394466698\n",
      "classification ---- 10920 metric train_loss = 0.0287920827511698\n",
      "classification ---- 10930 metric train_loss = 0.043968153465539214\n",
      "classification ---- 10940 metric train_loss = 0.03700824487023056\n",
      "classification ---- 10950 metric train_loss = 0.056380126345902684\n",
      "classification ---- 10960 metric train_loss = 0.019054890889674426\n",
      "classification ---- 10970 metric train_loss = 0.0518418894149363\n",
      "classification ---- 10980 metric train_loss = 0.03807413913309574\n",
      "classification ---- 10990 metric train_loss = 0.0402378116035834\n",
      "classification ---- 11000 metric train_loss = 0.03717355369590223\n",
      "classification ---- 23 metric test_f1 = 0.9906611484565073\n",
      "classification ---- 12 metric train_f1 = 0.9906997451218937\n",
      "classification ---- 11010 metric train_loss = 0.0742948008235544\n",
      "classification ---- 11020 metric train_loss = 0.037609792035073045\n",
      "classification ---- 11030 metric train_loss = 0.03571608527563512\n",
      "classification ---- 11040 metric train_loss = 0.046927744848653676\n",
      "classification ---- 11050 metric train_loss = 0.06646345620974899\n",
      "classification ---- 11060 metric train_loss = 0.0148405252257362\n",
      "classification ---- 11070 metric train_loss = 0.047186647448688744\n",
      "classification ---- 11080 metric train_loss = 0.03199886993970722\n",
      "classification ---- 11090 metric train_loss = 0.046092581422999504\n",
      "classification ---- 11100 metric train_loss = 0.02841637427918613\n",
      "classification ---- 11110 metric train_loss = 0.028766559762880207\n",
      "classification ---- 11120 metric train_loss = 0.025563578866422177\n",
      "classification ---- 11130 metric train_loss = 0.026563602429814637\n",
      "classification ---- 11140 metric train_loss = 0.050993232615292074\n",
      "classification ---- 11150 metric train_loss = 0.03443803247064352\n",
      "classification ---- 11160 metric train_loss = 0.026206004433333874\n",
      "classification ---- 11170 metric train_loss = 0.05179624720476568\n",
      "classification ---- 11180 metric train_loss = 0.034208607161417603\n",
      "classification ---- 11190 metric train_loss = 0.037435847753658893\n",
      "classification ---- 11200 metric train_loss = 0.03046705378219485\n",
      "classification ---- 11210 metric train_loss = 0.037786793219856916\n",
      "classification ---- 11220 metric train_loss = 0.046249290485866366\n",
      "classification ---- 11230 metric train_loss = 0.0457017314620316\n",
      "classification ---- 11240 metric train_loss = 0.03129871734417975\n",
      "classification ---- 11250 metric train_loss = 0.027703369269147515\n",
      "classification ---- 11260 metric train_loss = 0.04251511716283858\n",
      "classification ---- 11270 metric train_loss = 0.03672775330487639\n",
      "classification ---- 11280 metric train_loss = 0.02480742409825325\n",
      "classification ---- 11290 metric train_loss = 0.030326118785887958\n",
      "classification ---- 11300 metric train_loss = 0.02074657999910414\n",
      "classification ---- 11310 metric train_loss = 0.043128705699928105\n",
      "classification ---- 11320 metric train_loss = 0.036586718866601586\n",
      "classification ---- 11330 metric train_loss = 0.01880280184559524\n",
      "classification ---- 11340 metric train_loss = 0.0503711455501616\n",
      "classification ---- 11350 metric train_loss = 0.04118466340005398\n",
      "classification ---- 11360 metric train_loss = 0.04035163396038115\n",
      "classification ---- 11370 metric train_loss = 0.025426377588883043\n",
      "classification ---- 11380 metric train_loss = 0.037799585098400715\n",
      "classification ---- 11390 metric train_loss = 0.041294431500136855\n",
      "classification ---- 11400 metric train_loss = 0.044582227943465114\n",
      "classification ---- 11410 metric train_loss = 0.05998590844683349\n",
      "classification ---- 11420 metric train_loss = 0.06356482142582535\n",
      "classification ---- 11430 metric train_loss = 0.0638169402256608\n",
      "classification ---- 11440 metric train_loss = 0.047853004187345505\n",
      "classification ---- 11450 metric train_loss = 0.03638334120623767\n",
      "classification ---- 11460 metric train_loss = 0.04670087154954672\n",
      "classification ---- 11470 metric train_loss = 0.05470962091349065\n",
      "classification ---- 11480 metric train_loss = 0.031321486667729916\n",
      "classification ---- 11490 metric train_loss = 0.041819427814334634\n",
      "classification ---- 11500 metric train_loss = 0.0365337859839201\n",
      "classification ---- 24 metric test_f1 = 0.9913726792811532\n",
      "classification ---- 11510 metric train_loss = 0.03835681434720754\n",
      "classification ---- 11520 metric train_loss = 0.02546394248493016\n",
      "classification ---- 11530 metric train_loss = 0.03657404375262559\n",
      "classification ---- 11540 metric train_loss = 0.044022215018048885\n",
      "classification ---- 11550 metric train_loss = 0.060949734412133694\n",
      "classification ---- 11560 metric train_loss = 0.025458488473668696\n",
      "classification ---- 11570 metric train_loss = 0.0330306600779295\n",
      "classification ---- 11580 metric train_loss = 0.041685283789411186\n",
      "classification ---- 11590 metric train_loss = 0.010729093942791224\n",
      "classification ---- 11600 metric train_loss = 0.03162596262991428\n",
      "classification ---- 11610 metric train_loss = 0.06582662682048976\n",
      "classification ---- 11620 metric train_loss = 0.04078588327392936\n",
      "classification ---- 11630 metric train_loss = 0.020071569853462277\n",
      "classification ---- 11640 metric train_loss = 0.03140697660855949\n",
      "classification ---- 11650 metric train_loss = 0.029580601654015482\n",
      "classification ---- 11660 metric train_loss = 0.039562680013477805\n",
      "classification ---- 11670 metric train_loss = 0.03938150580506772\n",
      "classification ---- 11680 metric train_loss = 0.04048433897551149\n",
      "classification ---- 11690 metric train_loss = 0.030932916747406124\n",
      "classification ---- 11700 metric train_loss = 0.07130409716628491\n",
      "classification ---- 11710 metric train_loss = 0.04183082869276404\n",
      "classification ---- 11720 metric train_loss = 0.02713223695755005\n",
      "classification ---- 11730 metric train_loss = 0.0574707055464387\n",
      "classification ---- 11740 metric train_loss = 0.029381070379167797\n",
      "classification ---- 11750 metric train_loss = 0.02540544040966779\n",
      "classification ---- 11760 metric train_loss = 0.041246208688244225\n",
      "classification ---- 11770 metric train_loss = 0.03090061414986849\n",
      "classification ---- 11780 metric train_loss = 0.06566184419207274\n",
      "classification ---- 11790 metric train_loss = 0.02883736095391214\n",
      "classification ---- 11800 metric train_loss = 0.05338107175193727\n",
      "classification ---- 11810 metric train_loss = 0.04426740687340498\n",
      "classification ---- 11820 metric train_loss = 0.040852844971232115\n",
      "classification ---- 11830 metric train_loss = 0.031707929191179575\n",
      "classification ---- 11840 metric train_loss = 0.05195901836268604\n",
      "classification ---- 11850 metric train_loss = 0.025464766845107078\n",
      "classification ---- 11860 metric train_loss = 0.0365609226282686\n",
      "classification ---- 11870 metric train_loss = 0.03039581496268511\n",
      "classification ---- 11880 metric train_loss = 0.030417602811940014\n",
      "classification ---- 11890 metric train_loss = 0.05052080368623137\n",
      "classification ---- 11900 metric train_loss = 0.0549919622251764\n",
      "classification ---- 11910 metric train_loss = 0.04319849284365773\n",
      "classification ---- 11920 metric train_loss = 0.01572751854546368\n",
      "classification ---- 11930 metric train_loss = 0.049918835936114195\n",
      "classification ---- 11940 metric train_loss = 0.03482506717555225\n",
      "classification ---- 11950 metric train_loss = 0.01568371569737792\n",
      "classification ---- 11960 metric train_loss = 0.041443869657814505\n",
      "classification ---- 11970 metric train_loss = 0.012295125098899008\n",
      "classification ---- 11980 metric train_loss = 0.05632144473493099\n",
      "classification ---- 11990 metric train_loss = 0.050267217215150595\n",
      "classification ---- 12000 metric train_loss = 0.044338523875921965\n",
      "classification ---- 25 metric test_f1 = 0.9913219580474497\n",
      "classification ---- 13 metric train_f1 = 0.9915568837269783\n",
      "classification ---- 12010 metric train_loss = 0.052616096753627065\n",
      "classification ---- 12020 metric train_loss = 0.046175929624587296\n",
      "classification ---- 12030 metric train_loss = 0.03509564381092787\n",
      "classification ---- 12040 metric train_loss = 0.01748348861001432\n",
      "classification ---- 12050 metric train_loss = 0.021194365667179228\n",
      "classification ---- 12060 metric train_loss = 0.013623734749853612\n",
      "classification ---- 12070 metric train_loss = 0.03629174388479441\n",
      "classification ---- 12080 metric train_loss = 0.04484248566441238\n",
      "classification ---- 12090 metric train_loss = 0.03907701487187296\n",
      "classification ---- 12100 metric train_loss = 0.054380652704276146\n",
      "classification ---- 12110 metric train_loss = 0.041285995347425344\n",
      "classification ---- 12120 metric train_loss = 0.02694994923658669\n",
      "classification ---- 12130 metric train_loss = 0.04488104118499905\n",
      "classification ---- 12140 metric train_loss = 0.02291329447180033\n",
      "classification ---- 12150 metric train_loss = 0.058470881544053555\n",
      "classification ---- 12160 metric train_loss = 0.03323421999812126\n",
      "classification ---- 12170 metric train_loss = 0.03320500650443137\n",
      "classification ---- 12180 metric train_loss = 0.03109720894135535\n",
      "classification ---- 12190 metric train_loss = 0.03307767065707594\n",
      "classification ---- 12200 metric train_loss = 0.04536154391244054\n",
      "classification ---- 12210 metric train_loss = 0.03867448265664279\n",
      "classification ---- 12220 metric train_loss = 0.029434306360781193\n",
      "classification ---- 12230 metric train_loss = 0.03503198758699\n",
      "classification ---- 12240 metric train_loss = 0.03779495893977582\n",
      "classification ---- 12250 metric train_loss = 0.02273222794756293\n",
      "classification ---- 12260 metric train_loss = 0.02796132513321936\n",
      "classification ---- 12270 metric train_loss = 0.052830650960095225\n",
      "classification ---- 12280 metric train_loss = 0.04027333064004779\n",
      "classification ---- 12290 metric train_loss = 0.03442041289526969\n",
      "classification ---- 12300 metric train_loss = 0.06771579841151834\n",
      "classification ---- 12310 metric train_loss = 0.041757138120010494\n",
      "classification ---- 12320 metric train_loss = 0.052490125456824896\n",
      "classification ---- 12330 metric train_loss = 0.022291641030460597\n",
      "classification ---- 12340 metric train_loss = 0.02745558413444087\n",
      "classification ---- 12350 metric train_loss = 0.0387617441592738\n",
      "classification ---- 12360 metric train_loss = 0.011533159203827381\n",
      "classification ---- 12370 metric train_loss = 0.047948599653318526\n",
      "classification ---- 12380 metric train_loss = 0.038797798985615375\n",
      "classification ---- 12390 metric train_loss = 0.025331322033889592\n",
      "classification ---- 12400 metric train_loss = 0.032769801863469186\n",
      "classification ---- 12410 metric train_loss = 0.03192508581560105\n",
      "classification ---- 12420 metric train_loss = 0.05978177289944142\n",
      "classification ---- 12430 metric train_loss = 0.05492797971237451\n",
      "classification ---- 12440 metric train_loss = 0.055636453256011006\n",
      "classification ---- 12450 metric train_loss = 0.05150313675403595\n",
      "classification ---- 12460 metric train_loss = 0.03687077963259071\n",
      "classification ---- 12470 metric train_loss = 0.031677107745781544\n",
      "classification ---- 12480 metric train_loss = 0.020573324616998435\n",
      "classification ---- 12490 metric train_loss = 0.03514017038978636\n",
      "classification ---- 12500 metric train_loss = 0.028004095377400517\n",
      "classification ---- 26 metric test_f1 = 0.9915261683106783\n",
      "classification ---- 12510 metric train_loss = 0.035821907222270966\n",
      "classification ---- 12520 metric train_loss = 0.06667718561366201\n",
      "classification ---- 12530 metric train_loss = 0.04959215950220823\n",
      "classification ---- 12540 metric train_loss = 0.040167607739567755\n",
      "classification ---- 12550 metric train_loss = 0.0335416117683053\n",
      "classification ---- 12560 metric train_loss = 0.048957765521481636\n",
      "classification ---- 12570 metric train_loss = 0.017810038616880776\n",
      "classification ---- 12580 metric train_loss = 0.032279384322464466\n",
      "classification ---- 12590 metric train_loss = 0.01616577785462141\n",
      "classification ---- 12600 metric train_loss = 0.032926972256973386\n",
      "classification ---- 12610 metric train_loss = 0.031307823257520795\n",
      "classification ---- 12620 metric train_loss = 0.02939223190769553\n",
      "classification ---- 12630 metric train_loss = 0.04337465629214421\n",
      "classification ---- 12640 metric train_loss = 0.02891675094142556\n",
      "classification ---- 12650 metric train_loss = 0.025164310564287008\n",
      "classification ---- 12660 metric train_loss = 0.027102084690704943\n",
      "classification ---- 12670 metric train_loss = 0.027099958085455\n",
      "classification ---- 12680 metric train_loss = 0.037765853386372325\n",
      "classification ---- 12690 metric train_loss = 0.07643667347729206\n",
      "classification ---- 12700 metric train_loss = 0.042693232838064435\n",
      "classification ---- 12710 metric train_loss = 0.026545538939535616\n",
      "classification ---- 12720 metric train_loss = 0.05414905620273203\n",
      "classification ---- 12730 metric train_loss = 0.027644560346379875\n",
      "classification ---- 12740 metric train_loss = 0.030919071286916733\n",
      "classification ---- 12750 metric train_loss = 0.011401249235495925\n",
      "classification ---- 12760 metric train_loss = 0.028415596997365355\n",
      "classification ---- 12770 metric train_loss = 0.031680123833939436\n",
      "classification ---- 12780 metric train_loss = 0.020764594431966543\n",
      "classification ---- 12790 metric train_loss = 0.07566184296738357\n",
      "classification ---- 12800 metric train_loss = 0.04273466393351555\n",
      "classification ---- 12810 metric train_loss = 0.0315167352091521\n",
      "classification ---- 12820 metric train_loss = 0.0259652235545218\n",
      "classification ---- 12830 metric train_loss = 0.02996307290159166\n",
      "classification ---- 12840 metric train_loss = 0.04799740021117031\n",
      "classification ---- 12850 metric train_loss = 0.044753400748595594\n",
      "classification ---- 12860 metric train_loss = 0.04464229173026979\n",
      "classification ---- 12870 metric train_loss = 0.0314317955635488\n",
      "classification ---- 12880 metric train_loss = 0.026532463217154145\n",
      "classification ---- 12890 metric train_loss = 0.0328044451540336\n",
      "classification ---- 12900 metric train_loss = 0.04249028493650257\n",
      "classification ---- 12910 metric train_loss = 0.05702238203957677\n",
      "classification ---- 12920 metric train_loss = 0.044722074177116156\n",
      "classification ---- 12930 metric train_loss = 0.02448303857818246\n",
      "classification ---- 12940 metric train_loss = 0.038487580791115764\n",
      "classification ---- 12950 metric train_loss = 0.033598842471837996\n",
      "classification ---- 12960 metric train_loss = 0.01922657173126936\n",
      "classification ---- 12970 metric train_loss = 0.03020830578170717\n",
      "classification ---- 12980 metric train_loss = 0.022067346284165977\n",
      "classification ---- 12990 metric train_loss = 0.05045698415488005\n",
      "classification ---- 13000 metric train_loss = 0.03323174577672035\n",
      "classification ---- 27 metric test_f1 = 0.9916553862832096\n",
      "classification ---- 14 metric train_f1 = 0.9913091401291665\n",
      "classification ---- 13010 metric train_loss = 0.037712874938733876\n",
      "classification ---- 13020 metric train_loss = 0.0375842731911689\n",
      "classification ---- 13030 metric train_loss = 0.04479387253522873\n",
      "classification ---- 13040 metric train_loss = 0.05247424994595349\n",
      "classification ---- 13050 metric train_loss = 0.04315574790816754\n",
      "classification ---- 13060 metric train_loss = 0.015706751588732006\n",
      "classification ---- 13070 metric train_loss = 0.025692108436487615\n",
      "classification ---- 13080 metric train_loss = 0.04606280983425677\n",
      "classification ---- 13090 metric train_loss = 0.012336627836339176\n",
      "classification ---- 13100 metric train_loss = 0.0169392574345693\n",
      "classification ---- 13110 metric train_loss = 0.047127436939626934\n",
      "classification ---- 13120 metric train_loss = 0.025913383136503397\n",
      "classification ---- 13130 metric train_loss = 0.04179124478250742\n",
      "classification ---- 13140 metric train_loss = 0.048665095074102285\n",
      "classification ---- 13150 metric train_loss = 0.02258385266177356\n",
      "classification ---- 13160 metric train_loss = 0.030253719678148626\n",
      "classification ---- 13170 metric train_loss = 0.037367147696204486\n",
      "classification ---- 13180 metric train_loss = 0.02904347386211157\n",
      "classification ---- 13190 metric train_loss = 0.0229366845684126\n",
      "classification ---- 13200 metric train_loss = 0.03436092799529433\n",
      "classification ---- 13210 metric train_loss = 0.011516304197721183\n",
      "classification ---- 13220 metric train_loss = 0.03682965994812548\n",
      "classification ---- 13230 metric train_loss = 0.033096764446236195\n",
      "classification ---- 13240 metric train_loss = 0.028991115861572325\n",
      "classification ---- 13250 metric train_loss = 0.03929117014631629\n",
      "classification ---- 13260 metric train_loss = 0.0354758745059371\n",
      "classification ---- 13270 metric train_loss = 0.048672788217663764\n",
      "classification ---- 13280 metric train_loss = 0.053537625609897076\n",
      "classification ---- 13290 metric train_loss = 0.012372180330567062\n",
      "classification ---- 13300 metric train_loss = 0.04249143726192415\n",
      "classification ---- 13310 metric train_loss = 0.03548296662047505\n",
      "classification ---- 13320 metric train_loss = 0.02174652605317533\n",
      "classification ---- 13330 metric train_loss = 0.051267842203378676\n",
      "classification ---- 13340 metric train_loss = 0.054296578653156756\n",
      "classification ---- 13350 metric train_loss = 0.03750429041683674\n",
      "classification ---- 13360 metric train_loss = 0.05504622752778232\n",
      "classification ---- 13370 metric train_loss = 0.020956704299896954\n",
      "classification ---- 13380 metric train_loss = 0.027128216112032533\n",
      "classification ---- 13390 metric train_loss = 0.07604413637891412\n",
      "classification ---- 13400 metric train_loss = 0.02336986190639436\n",
      "classification ---- 13410 metric train_loss = 0.03879252038896084\n",
      "classification ---- 13420 metric train_loss = 0.033659533224999906\n",
      "classification ---- 13430 metric train_loss = 0.011352486652322113\n",
      "classification ---- 13440 metric train_loss = 0.02633444736711681\n",
      "classification ---- 13450 metric train_loss = 0.04042893797159195\n",
      "classification ---- 13460 metric train_loss = 0.03715926641598344\n",
      "classification ---- 13470 metric train_loss = 0.03739322423934936\n",
      "classification ---- 13480 metric train_loss = 0.02028943065088242\n",
      "classification ---- 13490 metric train_loss = 0.03529668957926333\n",
      "classification ---- 13500 metric train_loss = 0.04758091636467725\n",
      "classification ---- 28 metric test_f1 = 0.99107359633965\n",
      "classification ---- 13510 metric train_loss = 0.028197461436502636\n",
      "classification ---- 13520 metric train_loss = 0.01707861144095659\n",
      "classification ---- 13530 metric train_loss = 0.036883405316621064\n",
      "classification ---- 13540 metric train_loss = 0.03149985403288156\n",
      "classification ---- 13550 metric train_loss = 0.015177897363901138\n",
      "classification ---- 13560 metric train_loss = 0.026243786746636032\n",
      "classification ---- 13570 metric train_loss = 0.04958582459948957\n",
      "classification ---- 13580 metric train_loss = 0.036155075021088126\n",
      "classification ---- 13590 metric train_loss = 0.0393554300069809\n",
      "classification ---- 13600 metric train_loss = 0.02205286973621696\n",
      "classification ---- 13610 metric train_loss = 0.06425739070400596\n",
      "classification ---- 13620 metric train_loss = 0.04433705061674118\n",
      "classification ---- 13630 metric train_loss = 0.0623632644303143\n",
      "classification ---- 13640 metric train_loss = 0.05638371994718909\n",
      "classification ---- 13650 metric train_loss = 0.049992859130725265\n",
      "classification ---- 13660 metric train_loss = 0.056542154075577854\n",
      "classification ---- 13670 metric train_loss = 0.040559631981886925\n",
      "classification ---- 13680 metric train_loss = 0.03690094552002847\n",
      "classification ---- 13690 metric train_loss = 0.0425575006287545\n",
      "classification ---- 13700 metric train_loss = 0.0561549679376185\n",
      "classification ---- 13710 metric train_loss = 0.035471724392846224\n",
      "classification ---- 13720 metric train_loss = 0.04728474784642458\n",
      "classification ---- 13730 metric train_loss = 0.04588403564412147\n",
      "classification ---- 13740 metric train_loss = 0.053043094696477056\n",
      "classification ---- 13750 metric train_loss = 0.030246907146647573\n",
      "classification ---- 13760 metric train_loss = 0.037735519418492916\n",
      "classification ---- 13770 metric train_loss = 0.028877209266647698\n",
      "classification ---- 13780 metric train_loss = 0.07452661418356002\n",
      "classification ---- 13790 metric train_loss = 0.03814047258347273\n",
      "classification ---- 13800 metric train_loss = 0.03729962813667953\n",
      "classification ---- 13810 metric train_loss = 0.05090448115952313\n",
      "classification ---- 13820 metric train_loss = 0.0193821232765913\n",
      "classification ---- 13830 metric train_loss = 0.054203667235560714\n",
      "classification ---- 13840 metric train_loss = 0.0681045358069241\n",
      "classification ---- 13850 metric train_loss = 0.00920660027768463\n",
      "classification ---- 13860 metric train_loss = 0.013392823422327638\n",
      "classification ---- 13870 metric train_loss = 0.017258612252771855\n",
      "classification ---- 13880 metric train_loss = 0.013767555402591825\n",
      "classification ---- 13890 metric train_loss = 0.03852740488946438\n",
      "classification ---- 13900 metric train_loss = 0.027331760339438915\n",
      "classification ---- 13910 metric train_loss = 0.02283725468441844\n",
      "classification ---- 13920 metric train_loss = 0.03440559725277126\n",
      "classification ---- 13930 metric train_loss = 0.0573946304153651\n",
      "classification ---- 13940 metric train_loss = 0.04031470157206059\n",
      "classification ---- 13950 metric train_loss = 0.019768928922712804\n",
      "classification ---- 13960 metric train_loss = 0.057682699197903274\n",
      "classification ---- 13970 metric train_loss = 0.012613074900582433\n",
      "classification ---- 13980 metric train_loss = 0.0375864636618644\n",
      "classification ---- 13990 metric train_loss = 0.034721295814961195\n",
      "classification ---- 14000 metric train_loss = 0.02797547725494951\n",
      "classification ---- 29 metric test_f1 = 0.9918386158973646\n",
      "classification ---- 15 metric train_f1 = 0.9911329136384868\n",
      "classification ---- 14010 metric train_loss = 0.0314086424652487\n",
      "classification ---- 14020 metric train_loss = 0.047787651023827496\n",
      "classification ---- 14030 metric train_loss = 0.030896714865230025\n",
      "classification ---- 14040 metric train_loss = 0.027915971004404128\n",
      "classification ---- 14050 metric train_loss = 0.027924828138202427\n",
      "classification ---- 14060 metric train_loss = 0.04775632005184889\n",
      "classification ---- 14070 metric train_loss = 0.03924351378809661\n",
      "classification ---- 14080 metric train_loss = 0.04844748587347567\n",
      "classification ---- 14090 metric train_loss = 0.04456841694191098\n",
      "classification ---- 14100 metric train_loss = 0.04864072883501649\n",
      "classification ---- 14110 metric train_loss = 0.02564945756457746\n",
      "classification ---- 14120 metric train_loss = 0.031457698345184325\n",
      "classification ---- 14130 metric train_loss = 0.030986723396927117\n",
      "classification ---- 14140 metric train_loss = 0.035632196720689534\n",
      "classification ---- 14150 metric train_loss = 0.016282105166465045\n",
      "classification ---- 14160 metric train_loss = 0.02734228428453207\n",
      "classification ---- 14170 metric train_loss = 0.03309786459431052\n",
      "classification ---- 14180 metric train_loss = 0.03226588799152523\n",
      "classification ---- 14190 metric train_loss = 0.048637039959430695\n",
      "classification ---- 14200 metric train_loss = 0.02923624129034579\n",
      "classification ---- 14210 metric train_loss = 0.05396350601222366\n",
      "classification ---- 14220 metric train_loss = 0.025803941767662764\n",
      "classification ---- 14230 metric train_loss = 0.027576696267351507\n",
      "classification ---- 14240 metric train_loss = 0.03490679382812232\n",
      "classification ---- 14250 metric train_loss = 0.036379481689073144\n",
      "classification ---- 14260 metric train_loss = 0.03669130741618574\n",
      "classification ---- 14270 metric train_loss = 0.03128033962566405\n",
      "classification ---- 14280 metric train_loss = 0.02970787843223661\n",
      "classification ---- 14290 metric train_loss = 0.05031442176550627\n",
      "classification ---- 14300 metric train_loss = 0.03553009396418929\n",
      "classification ---- 14310 metric train_loss = 0.04183245685417205\n",
      "classification ---- 14320 metric train_loss = 0.03201008001342416\n",
      "classification ---- 14330 metric train_loss = 0.04066238706000149\n",
      "classification ---- 14340 metric train_loss = 0.06659767439123243\n",
      "classification ---- 14350 metric train_loss = 0.04629685166291893\n",
      "classification ---- 14360 metric train_loss = 0.0533861581236124\n",
      "classification ---- 14370 metric train_loss = 0.044338536961004135\n",
      "classification ---- 14380 metric train_loss = 0.025325210811570287\n",
      "classification ---- 14390 metric train_loss = 0.05094596934504807\n",
      "classification ---- 14400 metric train_loss = 0.051847425661981104\n",
      "classification ---- 14410 metric train_loss = 0.04234831486828625\n",
      "classification ---- 14420 metric train_loss = 0.0281250667758286\n",
      "classification ---- 14430 metric train_loss = 0.028205380379222333\n",
      "classification ---- 14440 metric train_loss = 0.03139473255723715\n",
      "classification ---- 14450 metric train_loss = 0.03715577544644475\n",
      "classification ---- 14460 metric train_loss = 0.027201929362490773\n",
      "classification ---- 14470 metric train_loss = 0.027197491540573538\n",
      "classification ---- 14480 metric train_loss = 0.03386260946281254\n",
      "classification ---- 14490 metric train_loss = 0.02794388569891453\n",
      "classification ---- 14500 metric train_loss = 0.030210253410041334\n",
      "classification ---- 30 metric test_f1 = 0.9924228667396557\n",
      "classification ---- 14510 metric train_loss = 0.0332185625564307\n",
      "classification ---- 14520 metric train_loss = 0.02499453811906278\n",
      "classification ---- 14530 metric train_loss = 0.04700848273932934\n",
      "classification ---- 14540 metric train_loss = 0.0354549144860357\n",
      "classification ---- 14550 metric train_loss = 0.016591213084757327\n",
      "classification ---- 14560 metric train_loss = 0.03674655989743769\n",
      "classification ---- 14570 metric train_loss = 0.025807740213349462\n",
      "classification ---- 14580 metric train_loss = 0.019156629871577024\n",
      "classification ---- 14590 metric train_loss = 0.03559904657304287\n",
      "classification ---- 14600 metric train_loss = 0.029476690758019685\n",
      "classification ---- 14610 metric train_loss = 0.02733971094712615\n",
      "classification ---- 14620 metric train_loss = 0.022314420412294568\n",
      "classification ---- 14630 metric train_loss = 0.007096084323711694\n",
      "classification ---- 14640 metric train_loss = 0.02806277824565768\n",
      "classification ---- 14650 metric train_loss = 0.03365719558205456\n",
      "classification ---- 14660 metric train_loss = 0.04211629640776664\n",
      "classification ---- 14670 metric train_loss = 0.023388439998961986\n",
      "classification ---- 14680 metric train_loss = 0.028754381043836475\n",
      "classification ---- 14690 metric train_loss = 0.0410050761885941\n",
      "classification ---- 14700 metric train_loss = 0.0411130883730948\n",
      "classification ---- 14710 metric train_loss = 0.041638365993276236\n",
      "classification ---- 14720 metric train_loss = 0.02444425181020051\n",
      "classification ---- 14730 metric train_loss = 0.035054221656173466\n",
      "classification ---- 14740 metric train_loss = 0.02805551723577082\n",
      "classification ---- 14750 metric train_loss = 0.05863203261978924\n",
      "classification ---- 14760 metric train_loss = 0.03670465517789125\n",
      "classification ---- 14770 metric train_loss = 0.05914752716198564\n",
      "classification ---- 14780 metric train_loss = 0.05816504415124655\n",
      "classification ---- 14790 metric train_loss = 0.022207726538181306\n",
      "classification ---- 14800 metric train_loss = 0.04307489523198456\n",
      "classification ---- 14810 metric train_loss = 0.04531127824448049\n",
      "classification ---- 14820 metric train_loss = 0.03997097983956337\n",
      "classification ---- 14830 metric train_loss = 0.01313851298764348\n",
      "classification ---- 14840 metric train_loss = 0.04645575780887157\n",
      "classification ---- 14850 metric train_loss = 0.07646330446004868\n",
      "classification ---- 14860 metric train_loss = 0.02895342737901956\n",
      "classification ---- 14870 metric train_loss = 0.03681100392714143\n",
      "classification ---- 14880 metric train_loss = 0.05710487924516201\n",
      "classification ---- 14890 metric train_loss = 0.042978834360837936\n",
      "classification ---- 14900 metric train_loss = 0.034100069757550956\n",
      "classification ---- 14910 metric train_loss = 0.06180917024612427\n",
      "classification ---- 14920 metric train_loss = 0.015363398846238852\n",
      "classification ---- 14930 metric train_loss = 0.03307029134593904\n",
      "classification ---- 14940 metric train_loss = 0.024799580127000807\n",
      "classification ---- 14950 metric train_loss = 0.04556101537309587\n",
      "classification ---- 14960 metric train_loss = 0.05385132182855159\n",
      "classification ---- 14970 metric train_loss = 0.04970191889442503\n",
      "classification ---- 14980 metric train_loss = 0.04169307937845588\n",
      "classification ---- 14990 metric train_loss = 0.05790507458150387\n",
      "classification ---- 15000 metric train_loss = 0.03197516798973084\n",
      "classification ---- 31 metric test_f1 = 0.992227097026136\n",
      "classification ---- 16 metric train_f1 = 0.9919527910945661\n",
      "classification ---- 15010 metric train_loss = 0.03518770565278828\n",
      "classification ---- 15020 metric train_loss = 0.028354850062169135\n",
      "classification ---- 15030 metric train_loss = 0.028388547850772737\n",
      "classification ---- 15040 metric train_loss = 0.02756463224068284\n",
      "classification ---- 15050 metric train_loss = 0.051214486034587024\n",
      "classification ---- 15060 metric train_loss = 0.030156981106847523\n",
      "classification ---- 15070 metric train_loss = 0.03405777625739574\n",
      "classification ---- 15080 metric train_loss = 0.023815766116604208\n",
      "classification ---- 15090 metric train_loss = 0.050907728634774684\n",
      "classification ---- 15100 metric train_loss = 0.029372926475480197\n",
      "classification ---- 15110 metric train_loss = 0.025660980166867374\n",
      "classification ---- 15120 metric train_loss = 0.034461978496983646\n",
      "classification ---- 15130 metric train_loss = 0.036179368291050194\n",
      "classification ---- 15140 metric train_loss = 0.05809403657913208\n",
      "classification ---- 15150 metric train_loss = 0.02569444924592972\n",
      "classification ---- 15160 metric train_loss = 0.026916699833236635\n",
      "classification ---- 15170 metric train_loss = 0.01388231327291578\n",
      "classification ---- 15180 metric train_loss = 0.04533694430720061\n",
      "classification ---- 15190 metric train_loss = 0.05177763481624424\n",
      "classification ---- 15200 metric train_loss = 0.03317900057882071\n",
      "classification ---- 15210 metric train_loss = 0.01822693939320743\n",
      "classification ---- 15220 metric train_loss = 0.03310136965010315\n",
      "classification ---- 15230 metric train_loss = 0.018353981524705888\n",
      "classification ---- 15240 metric train_loss = 0.028579404042102397\n",
      "classification ---- 15250 metric train_loss = 0.04581184042617679\n",
      "classification ---- 15260 metric train_loss = 0.01830756354611367\n",
      "classification ---- 15270 metric train_loss = 0.02711597951129079\n",
      "classification ---- 15280 metric train_loss = 0.022920114686712623\n",
      "classification ---- 15290 metric train_loss = 0.027476382232271133\n",
      "classification ---- 15300 metric train_loss = 0.03375239237211645\n",
      "classification ---- 15310 metric train_loss = 0.04804909119848162\n",
      "classification ---- 15320 metric train_loss = 0.041724709002301096\n",
      "classification ---- 15330 metric train_loss = 0.03797037326730788\n",
      "classification ---- 15340 metric train_loss = 0.035993999196216465\n",
      "classification ---- 15350 metric train_loss = 0.03174749044701457\n",
      "classification ---- 15360 metric train_loss = 0.021368482243269683\n",
      "classification ---- 15370 metric train_loss = 0.01800694519188255\n",
      "classification ---- 15380 metric train_loss = 0.030663450248539448\n",
      "classification ---- 15390 metric train_loss = 0.02791741769760847\n",
      "classification ---- 15400 metric train_loss = 0.042788793775253\n",
      "classification ---- 15410 metric train_loss = 0.06247763368301094\n",
      "classification ---- 15420 metric train_loss = 0.02483338927850127\n",
      "classification ---- 15430 metric train_loss = 0.029260553233325482\n",
      "classification ---- 15440 metric train_loss = 0.06680723791942\n",
      "classification ---- 15450 metric train_loss = 0.016440355498343707\n",
      "classification ---- 15460 metric train_loss = 0.04378952435217798\n",
      "classification ---- 15470 metric train_loss = 0.03709139595739543\n",
      "classification ---- 15480 metric train_loss = 0.02444582674652338\n",
      "classification ---- 15490 metric train_loss = 0.018966444837860762\n",
      "classification ---- 15500 metric train_loss = 0.023412100970745087\n",
      "classification ---- 32 metric test_f1 = 0.9904787754461402\n",
      "classification ---- 15510 metric train_loss = 0.013423424772918224\n",
      "classification ---- 15520 metric train_loss = 0.06163211283273995\n",
      "classification ---- 15530 metric train_loss = 0.015903921239078044\n",
      "classification ---- 15540 metric train_loss = 0.021277916454710066\n",
      "classification ---- 15550 metric train_loss = 0.03209420146886259\n",
      "classification ---- 15560 metric train_loss = 0.038625013642013074\n",
      "classification ---- 15570 metric train_loss = 0.0319597941590473\n",
      "classification ---- 15580 metric train_loss = 0.03964661657810211\n",
      "classification ---- 15590 metric train_loss = 0.029383852798491716\n",
      "classification ---- 15600 metric train_loss = 0.05444907769560814\n",
      "classification ---- 15610 metric train_loss = 0.03147707914467901\n",
      "classification ---- 15620 metric train_loss = 0.01817021439783275\n",
      "classification ---- 15630 metric train_loss = 0.059127823007293044\n",
      "classification ---- 15640 metric train_loss = 0.028319400968030095\n",
      "classification ---- 15650 metric train_loss = 0.01614752975292504\n",
      "classification ---- 15660 metric train_loss = 0.054795876471325757\n",
      "classification ---- 15670 metric train_loss = 0.024551393883302807\n",
      "classification ---- 15680 metric train_loss = 0.04942367007024586\n",
      "classification ---- 15690 metric train_loss = 0.04089076020754874\n",
      "classification ---- 15700 metric train_loss = 0.03778845383785665\n",
      "classification ---- 15710 metric train_loss = 0.04513159226626158\n",
      "classification ---- 15720 metric train_loss = 0.018448300054296853\n",
      "classification ---- 15730 metric train_loss = 0.035369087988510725\n",
      "classification ---- 15740 metric train_loss = 0.04158068494871259\n",
      "classification ---- 15750 metric train_loss = 0.04275870784185827\n",
      "classification ---- 15760 metric train_loss = 0.0635044121183455\n",
      "classification ---- 15770 metric train_loss = 0.03260679319500923\n",
      "classification ---- 15780 metric train_loss = 0.013450093194842338\n",
      "classification ---- 15790 metric train_loss = 0.0390369409462437\n",
      "classification ---- 15800 metric train_loss = 0.013902178616262972\n",
      "classification ---- 15810 metric train_loss = 0.05629213727079332\n",
      "classification ---- 15820 metric train_loss = 0.03814345793798566\n",
      "classification ---- 15830 metric train_loss = 0.025184381287544966\n",
      "classification ---- 15840 metric train_loss = 0.04445687062107027\n",
      "classification ---- 15850 metric train_loss = 0.03344139375258237\n",
      "classification ---- 15860 metric train_loss = 0.03153741196729243\n",
      "classification ---- 15870 metric train_loss = 0.026823967415839433\n",
      "classification ---- 15880 metric train_loss = 0.02135550738312304\n",
      "classification ---- 15890 metric train_loss = 0.03618040643632412\n",
      "classification ---- 15900 metric train_loss = 0.03219352338928729\n",
      "classification ---- 15910 metric train_loss = 0.017214017873629927\n",
      "classification ---- 15920 metric train_loss = 0.007988790574017912\n",
      "classification ---- 15930 metric train_loss = 0.01870714477263391\n",
      "classification ---- 15940 metric train_loss = 0.01629142863675952\n",
      "classification ---- 15950 metric train_loss = 0.03533467745874077\n",
      "classification ---- 15960 metric train_loss = 0.055804019491188225\n",
      "classification ---- 15970 metric train_loss = 0.05353885153308511\n",
      "classification ---- 15980 metric train_loss = 0.05681459447368979\n",
      "classification ---- 15990 metric train_loss = 0.032198573974892494\n",
      "classification ---- 16000 metric train_loss = 0.031421000207774344\n",
      "classification ---- 33 metric test_f1 = 0.9922073136510339\n",
      "classification ---- 17 metric train_f1 = 0.991885707199818\n",
      "classification ---- 16010 metric train_loss = 0.032779339724220334\n",
      "classification ---- 16020 metric train_loss = 0.029411845305003226\n",
      "classification ---- 16030 metric train_loss = 0.03851859886199236\n",
      "classification ---- 16040 metric train_loss = 0.03365926849655807\n",
      "classification ---- 16050 metric train_loss = 0.0384551749099046\n",
      "classification ---- 16060 metric train_loss = 0.035203831363469364\n",
      "classification ---- 16070 metric train_loss = 0.025379834976047278\n",
      "classification ---- 16080 metric train_loss = 0.027839768049307167\n",
      "classification ---- 16090 metric train_loss = 0.025191379990428687\n",
      "classification ---- 16100 metric train_loss = 0.03200258200522512\n",
      "classification ---- 16110 metric train_loss = 0.03530298499390483\n",
      "classification ---- 16120 metric train_loss = 0.04951004977338016\n",
      "classification ---- 16130 metric train_loss = 0.015612687962129713\n",
      "classification ---- 16140 metric train_loss = 0.026130228862166406\n",
      "classification ---- 16150 metric train_loss = 0.04639034173451364\n",
      "classification ---- 16160 metric train_loss = 0.046404150873422624\n",
      "classification ---- 16170 metric train_loss = 0.029449822613969445\n",
      "classification ---- 16180 metric train_loss = 0.04422730342485011\n",
      "classification ---- 16190 metric train_loss = 0.02682169054169208\n",
      "classification ---- 16200 metric train_loss = 0.020718774455599488\n",
      "classification ---- 16210 metric train_loss = 0.04667675876989961\n",
      "classification ---- 16220 metric train_loss = 0.044325627479702236\n",
      "classification ---- 16230 metric train_loss = 0.019978998810984194\n",
      "classification ---- 16240 metric train_loss = 0.0588729809038341\n",
      "classification ---- 16250 metric train_loss = 0.01829553972929716\n",
      "classification ---- 16260 metric train_loss = 0.0243816364556551\n",
      "classification ---- 16270 metric train_loss = 0.034103012969717385\n",
      "classification ---- 16280 metric train_loss = 0.03936316159088164\n",
      "classification ---- 16290 metric train_loss = 0.0374489888548851\n",
      "classification ---- 16300 metric train_loss = 0.07425574690569192\n",
      "classification ---- 16310 metric train_loss = 0.013105974742211401\n",
      "classification ---- 16320 metric train_loss = 0.045227786316536366\n",
      "classification ---- 16330 metric train_loss = 0.0512039378285408\n",
      "classification ---- 16340 metric train_loss = 0.04005661499686539\n",
      "classification ---- 16350 metric train_loss = 0.047687242226675156\n",
      "classification ---- 16360 metric train_loss = 0.032569873286411165\n",
      "classification ---- 16370 metric train_loss = 0.034764119703322646\n",
      "classification ---- 16380 metric train_loss = 0.0456198182888329\n",
      "classification ---- 16390 metric train_loss = 0.01679471889510751\n",
      "classification ---- 16400 metric train_loss = 0.043629254470579325\n",
      "classification ---- 16410 metric train_loss = 0.02534925767686218\n",
      "classification ---- 16420 metric train_loss = 0.03727486520074308\n",
      "classification ---- 16430 metric train_loss = 0.05539256539195776\n",
      "classification ---- 16440 metric train_loss = 0.017269950732588767\n",
      "classification ---- 16450 metric train_loss = 0.026053540781140326\n",
      "classification ---- 16460 metric train_loss = 0.022345313662663104\n",
      "classification ---- 16470 metric train_loss = 0.016398417321033774\n",
      "classification ---- 16480 metric train_loss = 0.0356839319691062\n",
      "classification ---- 16490 metric train_loss = 0.03201950401999056\n",
      "classification ---- 16500 metric train_loss = 0.014648042037151754\n",
      "classification ---- 34 metric test_f1 = 0.9920992015187167\n",
      "classification ---- 16510 metric train_loss = 0.04820461282506585\n",
      "classification ---- 16520 metric train_loss = 0.05522472923621535\n",
      "classification ---- 16530 metric train_loss = 0.03404923607595265\n",
      "classification ---- 16540 metric train_loss = 0.0315411890624091\n",
      "classification ---- 16550 metric train_loss = 0.021447175764478743\n",
      "classification ---- 16560 metric train_loss = 0.02830149307847023\n",
      "classification ---- 16570 metric train_loss = 0.04658361999318004\n",
      "classification ---- 16580 metric train_loss = 0.028347241505980492\n",
      "classification ---- 16590 metric train_loss = 0.05315033229999244\n",
      "classification ---- 16600 metric train_loss = 0.01230941703543067\n",
      "classification ---- 16610 metric train_loss = 0.04237357121892273\n",
      "classification ---- 16620 metric train_loss = 0.03080710375215858\n",
      "classification ---- 16630 metric train_loss = 0.027388769877143206\n",
      "classification ---- 16640 metric train_loss = 0.02570050498470664\n",
      "classification ---- 16650 metric train_loss = 0.03174462295137346\n",
      "classification ---- 16660 metric train_loss = 0.05655382033437491\n",
      "classification ---- 16670 metric train_loss = 0.040663256542757154\n",
      "classification ---- 16680 metric train_loss = 0.030151213984936476\n",
      "classification ---- 16690 metric train_loss = 0.04097026223316789\n",
      "classification ---- 16700 metric train_loss = 0.032824287889525294\n",
      "classification ---- 16710 metric train_loss = 0.05405192943289876\n",
      "classification ---- 16720 metric train_loss = 0.03691616482101381\n",
      "classification ---- 16730 metric train_loss = 0.03041878892108798\n",
      "classification ---- 16740 metric train_loss = 0.020678591495379806\n",
      "classification ---- 16750 metric train_loss = 0.03286924576386809\n",
      "classification ---- 16760 metric train_loss = 0.042793465522117914\n",
      "classification ---- 16770 metric train_loss = 0.03762543427292257\n",
      "classification ---- 16780 metric train_loss = 0.009887332888320088\n",
      "classification ---- 16790 metric train_loss = 0.03677861387841404\n",
      "classification ---- 16800 metric train_loss = 0.045020714937709275\n",
      "classification ---- 16810 metric train_loss = 0.024429726507514717\n",
      "classification ---- 16820 metric train_loss = 0.01619074889458716\n",
      "classification ---- 16830 metric train_loss = 0.025373461237177253\n",
      "classification ---- 16840 metric train_loss = 0.041304860007949175\n",
      "classification ---- 16850 metric train_loss = 0.04144786733668297\n",
      "classification ---- 16860 metric train_loss = 0.028308276808820663\n",
      "classification ---- 16870 metric train_loss = 0.01317253508605063\n",
      "classification ---- 16880 metric train_loss = 0.03068941649980843\n",
      "classification ---- 16890 metric train_loss = 0.030929178977385164\n",
      "classification ---- 16900 metric train_loss = 0.04132977994158864\n",
      "classification ---- 16910 metric train_loss = 0.01788256261497736\n",
      "classification ---- 16920 metric train_loss = 0.049240239430218935\n",
      "classification ---- 16930 metric train_loss = 0.02691991929896176\n",
      "classification ---- 16940 metric train_loss = 0.025315838772803544\n",
      "classification ---- 16950 metric train_loss = 0.05748750767670572\n",
      "classification ---- 16960 metric train_loss = 0.020377505314536394\n",
      "classification ---- 16970 metric train_loss = 0.051813317183405164\n",
      "classification ---- 16980 metric train_loss = 0.03318278198130429\n",
      "classification ---- 16990 metric train_loss = 0.03676088973879814\n",
      "classification ---- 17000 metric train_loss = 0.03222593481186777\n",
      "classification ---- 35 metric test_f1 = 0.9928533858776516\n",
      "classification ---- 18 metric train_f1 = 0.9922111792175792\n",
      "classification ---- 17010 metric train_loss = 0.05090070990845561\n",
      "classification ---- 17020 metric train_loss = 0.02927930378355086\n",
      "classification ---- 17030 metric train_loss = 0.03187686370220035\n",
      "classification ---- 17040 metric train_loss = 0.034409446315839885\n",
      "classification ---- 17050 metric train_loss = 0.024883903190493585\n",
      "classification ---- 17060 metric train_loss = 0.02877969816327095\n",
      "classification ---- 17070 metric train_loss = 0.036181089794263245\n",
      "classification ---- 17080 metric train_loss = 0.033549951994791626\n",
      "classification ---- 17090 metric train_loss = 0.033890153258107604\n",
      "classification ---- 17100 metric train_loss = 0.005186492041684687\n",
      "classification ---- 17110 metric train_loss = 0.021324903680942954\n",
      "classification ---- 17120 metric train_loss = 0.04644740822259337\n",
      "classification ---- 17130 metric train_loss = 0.01629970343783498\n",
      "classification ---- 17140 metric train_loss = 0.018769430834800004\n",
      "classification ---- 17150 metric train_loss = 0.051053790282458066\n",
      "classification ---- 17160 metric train_loss = 0.02904199354816228\n",
      "classification ---- 17170 metric train_loss = 0.042747032991610465\n",
      "classification ---- 17180 metric train_loss = 0.05607992056757212\n",
      "classification ---- 17190 metric train_loss = 0.020610477984882893\n",
      "classification ---- 17200 metric train_loss = 0.027323092229198664\n",
      "classification ---- 17210 metric train_loss = 0.02201338184531778\n",
      "classification ---- 17220 metric train_loss = 0.037631654925644395\n",
      "classification ---- 17230 metric train_loss = 0.04077434071805328\n",
      "classification ---- 17240 metric train_loss = 0.04016776313073933\n",
      "classification ---- 17250 metric train_loss = 0.010021381685510278\n",
      "classification ---- 17260 metric train_loss = 0.034398653218522666\n",
      "classification ---- 17270 metric train_loss = 0.025919627491384746\n",
      "classification ---- 17280 metric train_loss = 0.02554478213423863\n",
      "classification ---- 17290 metric train_loss = 0.024100710800848902\n",
      "classification ---- 17300 metric train_loss = 0.011537245428189635\n",
      "classification ---- 17310 metric train_loss = 0.04551280760206282\n",
      "classification ---- 17320 metric train_loss = 0.013438501232303678\n",
      "classification ---- 17330 metric train_loss = 0.04291360224597156\n",
      "classification ---- 17340 metric train_loss = 0.050950961629860105\n",
      "classification ---- 17350 metric train_loss = 0.02886418413836509\n",
      "classification ---- 17360 metric train_loss = 0.04961560321971774\n",
      "classification ---- 17370 metric train_loss = 0.03279462605714798\n",
      "classification ---- 17380 metric train_loss = 0.02855129453819245\n",
      "classification ---- 17390 metric train_loss = 0.03756699652876705\n",
      "classification ---- 17400 metric train_loss = 0.030919316573999822\n",
      "classification ---- 17410 metric train_loss = 0.053959767799824476\n",
      "classification ---- 17420 metric train_loss = 0.05034152516163885\n",
      "classification ---- 17430 metric train_loss = 0.029399840533733367\n",
      "classification ---- 17440 metric train_loss = 0.02109288638457656\n",
      "classification ---- 17450 metric train_loss = 0.026719563943333923\n",
      "classification ---- 17460 metric train_loss = 0.00936374836601317\n",
      "classification ---- 17470 metric train_loss = 0.030883405287750066\n",
      "classification ---- 17480 metric train_loss = 0.026856104400940238\n",
      "classification ---- 17490 metric train_loss = 0.051396614569239316\n",
      "classification ---- 17500 metric train_loss = 0.03158893422223628\n",
      "classification ---- 36 metric test_f1 = 0.9923745453605214\n",
      "classification ---- 17510 metric train_loss = 0.04582456338685006\n",
      "classification ---- 17520 metric train_loss = 0.019507308350875975\n",
      "classification ---- 17530 metric train_loss = 0.03364365799352527\n",
      "classification ---- 17540 metric train_loss = 0.04953994459938258\n",
      "classification ---- 17550 metric train_loss = 0.021213778294622898\n",
      "classification ---- 17560 metric train_loss = 0.02939030178822577\n",
      "classification ---- 17570 metric train_loss = 0.021079199586529283\n",
      "classification ---- 17580 metric train_loss = 0.03159731200430542\n",
      "classification ---- 17590 metric train_loss = 0.04158933020662516\n",
      "classification ---- 17600 metric train_loss = 0.03624495668336749\n",
      "classification ---- 17610 metric train_loss = 0.06469896438065917\n",
      "classification ---- 17620 metric train_loss = 0.029248939990065993\n",
      "classification ---- 17630 metric train_loss = 0.017688977881334722\n",
      "classification ---- 17640 metric train_loss = 0.06467018784023822\n",
      "classification ---- 17650 metric train_loss = 0.026868608104996382\n",
      "classification ---- 17660 metric train_loss = 0.023983708559535445\n",
      "classification ---- 17670 metric train_loss = 0.02804532160516828\n",
      "classification ---- 17680 metric train_loss = 0.04672151687555015\n",
      "classification ---- 17690 metric train_loss = 0.0278261088533327\n",
      "classification ---- 17700 metric train_loss = 0.0217303603887558\n",
      "classification ---- 17710 metric train_loss = 0.03676730301231146\n",
      "classification ---- 17720 metric train_loss = 0.06597368330694735\n",
      "classification ---- 17730 metric train_loss = 0.057036522519774736\n",
      "classification ---- 17740 metric train_loss = 0.015600368520244956\n",
      "classification ---- 17750 metric train_loss = 0.018480597506277265\n",
      "classification ---- 17760 metric train_loss = 0.01825792364543304\n",
      "classification ---- 17770 metric train_loss = 0.031650286400690675\n",
      "classification ---- 17780 metric train_loss = 0.046532023861072955\n",
      "classification ---- 17790 metric train_loss = 0.04199704914353788\n",
      "classification ---- 17800 metric train_loss = 0.056346239033155145\n",
      "classification ---- 17810 metric train_loss = 0.0696272675646469\n",
      "classification ---- 17820 metric train_loss = 0.02844780208542943\n",
      "classification ---- 17830 metric train_loss = 0.03578206771053374\n",
      "classification ---- 17840 metric train_loss = 0.02121281432919204\n",
      "classification ---- 17850 metric train_loss = 0.05989134528208524\n",
      "classification ---- 17860 metric train_loss = 0.03106919783167541\n",
      "classification ---- 17870 metric train_loss = 0.026939640240743756\n",
      "classification ---- 17880 metric train_loss = 0.029960318212397398\n",
      "classification ---- 17890 metric train_loss = 0.025989196356385948\n",
      "classification ---- 17900 metric train_loss = 0.020012471615336834\n",
      "classification ---- 17910 metric train_loss = 0.04459707238711417\n",
      "classification ---- 17920 metric train_loss = 0.05092942635528743\n",
      "classification ---- 17930 metric train_loss = 0.017287815175950526\n",
      "classification ---- 17940 metric train_loss = 0.0545082239434123\n",
      "classification ---- 17950 metric train_loss = 0.03949138186872005\n",
      "classification ---- 17960 metric train_loss = 0.023300786316394807\n",
      "classification ---- 17970 metric train_loss = 0.028036691434681415\n",
      "classification ---- 17980 metric train_loss = 0.027168383030220865\n",
      "classification ---- 17990 metric train_loss = 0.055037436867132784\n",
      "classification ---- 18000 metric train_loss = 0.025516616483218967\n",
      "classification ---- 37 metric test_f1 = 0.9927149213749107\n",
      "classification ---- 19 metric train_f1 = 0.9922852807466673\n",
      "classification ---- 18010 metric train_loss = 0.03871579058468342\n",
      "classification ---- 18020 metric train_loss = 0.03467459282837808\n",
      "classification ---- 18030 metric train_loss = 0.02242465573363006\n",
      "classification ---- 18040 metric train_loss = 0.03714485270902514\n",
      "classification ---- 18050 metric train_loss = 0.04558584426995367\n",
      "classification ---- 18060 metric train_loss = 0.043066416378133\n",
      "classification ---- 18070 metric train_loss = 0.0543605440761894\n",
      "classification ---- 18080 metric train_loss = 0.021184625290334224\n",
      "classification ---- 18090 metric train_loss = 0.029444633866660297\n",
      "classification ---- 18100 metric train_loss = 0.01425865557976067\n",
      "classification ---- 18110 metric train_loss = 0.015617805276997387\n",
      "classification ---- 18120 metric train_loss = 0.04600011520087719\n",
      "classification ---- 18130 metric train_loss = 0.040457190247252584\n",
      "classification ---- 18140 metric train_loss = 0.020264424779452384\n",
      "classification ---- 18150 metric train_loss = 0.05447399243712425\n",
      "classification ---- 18160 metric train_loss = 0.06549983802251517\n",
      "classification ---- 18170 metric train_loss = 0.020733073772862553\n",
      "classification ---- 18180 metric train_loss = 0.05268232109956443\n",
      "classification ---- 18190 metric train_loss = 0.015384998708032071\n",
      "classification ---- 18200 metric train_loss = 0.02986599844880402\n",
      "classification ---- 18210 metric train_loss = 0.020228005200624465\n",
      "classification ---- 18220 metric train_loss = 0.03597065282519907\n",
      "classification ---- 18230 metric train_loss = 0.02829415467567742\n",
      "classification ---- 18240 metric train_loss = 0.04041167113464326\n",
      "classification ---- 18250 metric train_loss = 0.02639165783766657\n",
      "classification ---- 18260 metric train_loss = 0.040843162196688355\n",
      "classification ---- 18270 metric train_loss = 0.04449148862622678\n",
      "classification ---- 18280 metric train_loss = 0.03087904709391296\n",
      "classification ---- 18290 metric train_loss = 0.02529368925606832\n",
      "classification ---- 18300 metric train_loss = 0.029101616144180296\n",
      "classification ---- 18310 metric train_loss = 0.017156869638711214\n",
      "classification ---- 18320 metric train_loss = 0.0385739984922111\n",
      "classification ---- 18330 metric train_loss = 0.013608465855941176\n",
      "classification ---- 18340 metric train_loss = 0.03388818269595504\n",
      "classification ---- 18350 metric train_loss = 0.04904660037718713\n",
      "classification ---- 18360 metric train_loss = 0.03701787698082626\n",
      "classification ---- 18370 metric train_loss = 0.023178078094497324\n",
      "classification ---- 18380 metric train_loss = 0.03982174885459244\n",
      "classification ---- 18390 metric train_loss = 0.036986970808357\n",
      "classification ---- 18400 metric train_loss = 0.03359357551671564\n",
      "classification ---- 18410 metric train_loss = 0.030630261939950287\n",
      "classification ---- 18420 metric train_loss = 0.026217454811558127\n",
      "classification ---- 18430 metric train_loss = 0.026786381052806972\n",
      "classification ---- 18440 metric train_loss = 0.0348269492154941\n",
      "classification ---- 18450 metric train_loss = 0.04889465316664428\n",
      "classification ---- 18460 metric train_loss = 0.03858733279630542\n",
      "classification ---- 18470 metric train_loss = 0.011664671916514635\n",
      "classification ---- 18480 metric train_loss = 0.03377139877993614\n",
      "classification ---- 18490 metric train_loss = 0.023795129265636206\n",
      "classification ---- 18500 metric train_loss = 0.01833705008029938\n",
      "classification ---- 38 metric test_f1 = 0.9913419432888696\n",
      "classification ---- 18510 metric train_loss = 0.021077068732120098\n",
      "classification ---- 18520 metric train_loss = 0.040699206129647794\n",
      "classification ---- 18530 metric train_loss = 0.03021797421388328\n",
      "classification ---- 18540 metric train_loss = 0.026027086144313216\n",
      "classification ---- 18550 metric train_loss = 0.01384759071515873\n",
      "classification ---- 18560 metric train_loss = 0.03199550851713866\n",
      "classification ---- 18570 metric train_loss = 0.032624800619669256\n",
      "classification ---- 18580 metric train_loss = 0.04091736990958452\n",
      "classification ---- 18590 metric train_loss = 0.02833477878011763\n",
      "classification ---- 18600 metric train_loss = 0.05331197183113545\n",
      "classification ---- 18610 metric train_loss = 0.02495281936135143\n",
      "classification ---- 18620 metric train_loss = 0.010928366193547845\n",
      "classification ---- 18630 metric train_loss = 0.034237868362106384\n",
      "classification ---- 18640 metric train_loss = 0.022937873704358937\n",
      "classification ---- 18650 metric train_loss = 0.01622649689670652\n",
      "classification ---- 18660 metric train_loss = 0.07442169552668929\n",
      "classification ---- 18670 metric train_loss = 0.04864512567874044\n",
      "classification ---- 18680 metric train_loss = 0.019293223833665253\n",
      "classification ---- 18690 metric train_loss = 0.033858372503891586\n",
      "classification ---- 18700 metric train_loss = 0.006233368976972997\n",
      "classification ---- 18710 metric train_loss = 0.030019874684512615\n",
      "classification ---- 18720 metric train_loss = 0.014507077797316015\n",
      "classification ---- 18730 metric train_loss = 0.034290911024436356\n",
      "classification ---- 18740 metric train_loss = 0.03130575674585998\n",
      "classification ---- 18750 metric train_loss = 0.05347623499110341\n",
      "classification ---- 18760 metric train_loss = 0.03401734239887446\n",
      "classification ---- 18770 metric train_loss = 0.034860990731976925\n",
      "classification ---- 18780 metric train_loss = 0.026269449898973108\n",
      "classification ---- 18790 metric train_loss = 0.06670330916531383\n",
      "classification ---- 18800 metric train_loss = 0.03033712529577315\n",
      "classification ---- 18810 metric train_loss = 0.0091221678070724\n",
      "classification ---- 18820 metric train_loss = 0.008149259630590678\n",
      "classification ---- 18830 metric train_loss = 0.02422355520538986\n",
      "classification ---- 18840 metric train_loss = 0.03717471688287333\n",
      "classification ---- 18850 metric train_loss = 0.05050151429604739\n",
      "classification ---- 18860 metric train_loss = 0.06724307043477892\n",
      "classification ---- 18870 metric train_loss = 0.025175102055072784\n",
      "classification ---- 18880 metric train_loss = 0.033682365366257724\n",
      "classification ---- 18890 metric train_loss = 0.03694139544386417\n",
      "classification ---- 18900 metric train_loss = 0.020525473752059042\n",
      "classification ---- 18910 metric train_loss = 0.02325155707076192\n",
      "classification ---- 18920 metric train_loss = 0.043325680098496376\n",
      "classification ---- 18930 metric train_loss = 0.030110717611387373\n",
      "classification ---- 18940 metric train_loss = 0.050514118862338364\n",
      "classification ---- 18950 metric train_loss = 0.040508849732577804\n",
      "classification ---- 18960 metric train_loss = 0.0235129572218284\n",
      "classification ---- 18970 metric train_loss = 0.01575167302507907\n",
      "classification ---- 18980 metric train_loss = 0.01575170671567321\n",
      "classification ---- 18990 metric train_loss = 0.02987610469572246\n",
      "classification ---- 19000 metric train_loss = 0.03383862206246704\n",
      "classification ---- 39 metric test_f1 = 0.9915669781876881\n",
      "classification ---- 20 metric train_f1 = 0.9920113224979474\n",
      "classification ---- 19010 metric train_loss = 0.019115837523713708\n",
      "classification ---- 19020 metric train_loss = 0.018542536976747216\n",
      "classification ---- 19030 metric train_loss = 0.025739973434247075\n",
      "classification ---- 19040 metric train_loss = 0.025289034890010952\n",
      "classification ---- 19050 metric train_loss = 0.04377681404585019\n",
      "classification ---- 19060 metric train_loss = 0.0193086510989815\n",
      "classification ---- 19070 metric train_loss = 0.02890414176508784\n",
      "classification ---- 19080 metric train_loss = 0.052405911730602384\n",
      "classification ---- 19090 metric train_loss = 0.013113779900595545\n",
      "classification ---- 19100 metric train_loss = 0.018031832040287556\n",
      "classification ---- 19110 metric train_loss = 0.03763909738045186\n",
      "classification ---- 19120 metric train_loss = 0.034473069850355385\n",
      "classification ---- 19130 metric train_loss = 0.041482416237704454\n",
      "classification ---- 19140 metric train_loss = 0.020133522199466826\n",
      "classification ---- 19150 metric train_loss = 0.014495793380774558\n",
      "classification ---- 19160 metric train_loss = 0.025213405233807863\n",
      "classification ---- 19170 metric train_loss = 0.036464785085991026\n",
      "classification ---- 19180 metric train_loss = 0.022931178379803897\n",
      "classification ---- 19190 metric train_loss = 0.04433817667886615\n",
      "classification ---- 19200 metric train_loss = 0.02110467553138733\n",
      "classification ---- 19210 metric train_loss = 0.041717616049572824\n",
      "classification ---- 19220 metric train_loss = 0.07787301947828382\n",
      "classification ---- 19230 metric train_loss = 0.031212280364707114\n",
      "classification ---- 19240 metric train_loss = 0.027626760536804795\n",
      "classification ---- 19250 metric train_loss = 0.04680640217848122\n",
      "classification ---- 19260 metric train_loss = 0.02900487568695098\n",
      "classification ---- 19270 metric train_loss = 0.06346712487284094\n",
      "classification ---- 19280 metric train_loss = 0.023766543879173697\n",
      "classification ---- 19290 metric train_loss = 0.050318377837538716\n",
      "classification ---- 19300 metric train_loss = 0.06200331575237215\n",
      "classification ---- 19310 metric train_loss = 0.03555147494189441\n",
      "classification ---- 19320 metric train_loss = 0.02413701594341546\n",
      "classification ---- 19330 metric train_loss = 0.04145224089734256\n",
      "classification ---- 19340 metric train_loss = 0.03204457540996373\n",
      "classification ---- 19350 metric train_loss = 0.025410066684708\n",
      "classification ---- 19360 metric train_loss = 0.03893163159955293\n",
      "classification ---- 19370 metric train_loss = 0.0349261537194252\n",
      "classification ---- 19380 metric train_loss = 0.02378226425498724\n",
      "classification ---- 19390 metric train_loss = 0.023000006936490534\n",
      "classification ---- 19400 metric train_loss = 0.02603319361805916\n",
      "classification ---- 19410 metric train_loss = 0.01845375818666071\n",
      "classification ---- 19420 metric train_loss = 0.03252772476989776\n",
      "classification ---- 19430 metric train_loss = 0.060267725680023435\n",
      "classification ---- 19440 metric train_loss = 0.057917728228494524\n",
      "classification ---- 19450 metric train_loss = 0.017693109391257168\n",
      "classification ---- 19460 metric train_loss = 0.02963531306013465\n",
      "classification ---- 19470 metric train_loss = 0.043941380362957715\n",
      "classification ---- 19480 metric train_loss = 0.021501306770369412\n",
      "classification ---- 19490 metric train_loss = 0.03960920423269272\n",
      "classification ---- 19500 metric train_loss = 0.035655506514012815\n",
      "classification ---- 40 metric test_f1 = 0.9913462521000493\n",
      "classification ---- 19510 metric train_loss = 0.0635879572480917\n",
      "classification ---- 19520 metric train_loss = 0.034315268881618975\n",
      "classification ---- 19530 metric train_loss = 0.03102093767374754\n",
      "classification ---- 19540 metric train_loss = 0.02151554189622402\n",
      "classification ---- 19550 metric train_loss = 0.030804745201021434\n",
      "classification ---- 19560 metric train_loss = 0.01370073554571718\n",
      "classification ---- 19570 metric train_loss = 0.052947639930062\n",
      "classification ---- 19580 metric train_loss = 0.056105854967609045\n",
      "classification ---- 19590 metric train_loss = 0.02728036588523537\n",
      "classification ---- 19600 metric train_loss = 0.036930470587685706\n",
      "classification ---- 19610 metric train_loss = 0.012069397955201566\n",
      "classification ---- 19620 metric train_loss = 0.06806622468866408\n",
      "classification ---- 19630 metric train_loss = 0.020024476991966366\n",
      "classification ---- 19640 metric train_loss = 0.017123692482709885\n",
      "classification ---- 19650 metric train_loss = 0.047001174453180285\n",
      "classification ---- 19660 metric train_loss = 0.006417713733389974\n",
      "classification ---- 19670 metric train_loss = 0.03427637352142483\n",
      "classification ---- 19680 metric train_loss = 0.05461436491459608\n",
      "classification ---- 19690 metric train_loss = 0.024130581610370426\n",
      "classification ---- 19700 metric train_loss = 0.029927714681252836\n",
      "classification ---- 19710 metric train_loss = 0.013934743986465038\n",
      "classification ---- 19720 metric train_loss = 0.04672506242059171\n",
      "classification ---- 19730 metric train_loss = 0.046407521585933864\n",
      "classification ---- 19740 metric train_loss = 0.0237930528819561\n",
      "classification ---- 19750 metric train_loss = 0.03665706350002438\n",
      "classification ---- 19760 metric train_loss = 0.031431141309440136\n",
      "classification ---- 19770 metric train_loss = 0.027422859403304755\n",
      "classification ---- 19780 metric train_loss = 0.009555368847213686\n",
      "classification ---- 19790 metric train_loss = 0.038422616105526684\n",
      "classification ---- 19800 metric train_loss = 0.04443976852344349\n",
      "classification ---- 19810 metric train_loss = 0.04348765490576625\n",
      "classification ---- 19820 metric train_loss = 0.060474866442382334\n",
      "classification ---- 19830 metric train_loss = 0.017151163937523962\n",
      "classification ---- 19840 metric train_loss = 0.014649897534400224\n",
      "classification ---- 19850 metric train_loss = 0.05439345799386501\n",
      "classification ---- 19860 metric train_loss = 0.04279794641770422\n",
      "classification ---- 19870 metric train_loss = 0.013350653229281306\n",
      "classification ---- 19880 metric train_loss = 0.013702035532332957\n",
      "classification ---- 19890 metric train_loss = 0.026604117383249103\n",
      "classification ---- 19900 metric train_loss = 0.04666396256070584\n",
      "classification ---- 19910 metric train_loss = 0.03009033603593707\n",
      "classification ---- 19920 metric train_loss = 0.03972339588217437\n",
      "classification ---- 19930 metric train_loss = 0.021881672902964056\n",
      "classification ---- 19940 metric train_loss = 0.035198976565152404\n",
      "classification ---- 19950 metric train_loss = 0.029436542349867523\n",
      "classification ---- 19960 metric train_loss = 0.017178502306342124\n",
      "classification ---- 19970 metric train_loss = 0.01816480008419603\n",
      "classification ---- 19980 metric train_loss = 0.034192919405177234\n",
      "classification ---- 19990 metric train_loss = 0.03038279665634036\n",
      "classification ---- 20000 metric train_loss = 0.01910962797701359\n",
      "classification ---- 41 metric test_f1 = 0.9906577012741438\n",
      "classification ---- 21 metric train_f1 = 0.9906191468198204\n",
      "classification ---- 20010 metric train_loss = 0.03211220591329038\n",
      "classification ---- 20020 metric train_loss = 0.03274642773903906\n",
      "classification ---- 20030 metric train_loss = 0.045338282361626625\n",
      "classification ---- 20040 metric train_loss = 0.028707634401507677\n",
      "classification ---- 20050 metric train_loss = 0.02964166775345802\n",
      "classification ---- 20060 metric train_loss = 0.03010795145528391\n",
      "classification ---- 20070 metric train_loss = 0.03609529852401465\n",
      "classification ---- 20080 metric train_loss = 0.033441268792375925\n",
      "classification ---- 20090 metric train_loss = 0.024777998425997795\n",
      "classification ---- 20100 metric train_loss = 0.03234034781344235\n",
      "classification ---- 20110 metric train_loss = 0.04912376147694886\n",
      "classification ---- 20120 metric train_loss = 0.02017492491286248\n",
      "classification ---- 20130 metric train_loss = 0.022558485716581346\n",
      "classification ---- 20140 metric train_loss = 0.03266362468712032\n",
      "classification ---- 20150 metric train_loss = 0.030144789488986135\n",
      "classification ---- 20160 metric train_loss = 0.010358785721473395\n",
      "classification ---- 20170 metric train_loss = 0.030183840403333308\n",
      "classification ---- 20180 metric train_loss = 0.03935899459756911\n",
      "classification ---- 20190 metric train_loss = 0.02195616587996483\n",
      "classification ---- 20200 metric train_loss = 0.039710479928180574\n",
      "classification ---- 20210 metric train_loss = 0.025988472765311597\n",
      "classification ---- 20220 metric train_loss = 0.03283205626066774\n",
      "classification ---- 20230 metric train_loss = 0.03531823100056499\n",
      "classification ---- 20240 metric train_loss = 0.019088412448763848\n",
      "classification ---- 20250 metric train_loss = 0.06076391648966819\n",
      "classification ---- 20260 metric train_loss = 0.03781364690512419\n",
      "classification ---- 20270 metric train_loss = 0.02186955721117556\n",
      "classification ---- 20280 metric train_loss = 0.043812469206750394\n",
      "classification ---- 20290 metric train_loss = 0.04094207426533103\n",
      "classification ---- 20300 metric train_loss = 0.026171094528399406\n",
      "classification ---- 20310 metric train_loss = 0.0436487490311265\n",
      "classification ---- 20320 metric train_loss = 0.03150615761987865\n",
      "classification ---- 20330 metric train_loss = 0.022948501748032868\n",
      "classification ---- 20340 metric train_loss = 0.035676869261078534\n",
      "classification ---- 20350 metric train_loss = 0.038515225145965816\n",
      "classification ---- 20360 metric train_loss = 0.01441224601585418\n",
      "classification ---- 20370 metric train_loss = 0.024285830464214085\n",
      "classification ---- 20380 metric train_loss = 0.024576228274963797\n",
      "classification ---- 20390 metric train_loss = 0.026177050638943912\n",
      "classification ---- 20400 metric train_loss = 0.02519282908178866\n",
      "classification ---- 20410 metric train_loss = 0.02093031011754647\n",
      "classification ---- 20420 metric train_loss = 0.015666933404281734\n",
      "classification ---- 20430 metric train_loss = 0.014901238610036671\n",
      "classification ---- 20440 metric train_loss = 0.03649564366787672\n",
      "classification ---- 20450 metric train_loss = 0.044080836547072975\n",
      "classification ---- 20460 metric train_loss = 0.045565475011244416\n",
      "classification ---- 20470 metric train_loss = 0.027987066376954316\n",
      "classification ---- 20480 metric train_loss = 0.011034337803721428\n",
      "classification ---- 20490 metric train_loss = 0.02320962424855679\n",
      "classification ---- 20500 metric train_loss = 0.0194790875306353\n",
      "classification ---- 42 metric test_f1 = 0.9927355586913833\n",
      "classification ---- 20510 metric train_loss = 0.057242392026819286\n",
      "classification ---- 20520 metric train_loss = 0.022499707899987696\n",
      "classification ---- 20530 metric train_loss = 0.05904776630923152\n",
      "classification ---- 20540 metric train_loss = 0.024136692751199007\n",
      "classification ---- 20550 metric train_loss = 0.04373161485418677\n",
      "classification ---- 20560 metric train_loss = 0.01936310427263379\n",
      "classification ---- 20570 metric train_loss = 0.026132158981636167\n",
      "classification ---- 20580 metric train_loss = 0.034036040352657435\n",
      "classification ---- 20590 metric train_loss = 0.022678600321523846\n",
      "classification ---- 20600 metric train_loss = 0.024767211079597472\n",
      "classification ---- 20610 metric train_loss = 0.04133444130420685\n",
      "classification ---- 20620 metric train_loss = 0.02914643057156354\n",
      "classification ---- 20630 metric train_loss = 0.025486912089399995\n",
      "classification ---- 20640 metric train_loss = 0.061788793513551356\n",
      "classification ---- 20650 metric train_loss = 0.04287562542594969\n",
      "classification ---- 20660 metric train_loss = 0.02585608937079087\n",
      "classification ---- 20670 metric train_loss = 0.046516968845389785\n",
      "classification ---- 20680 metric train_loss = 0.03031757567077875\n",
      "classification ---- 20690 metric train_loss = 0.044410782342310995\n",
      "classification ---- 20700 metric train_loss = 0.03441630038432777\n",
      "classification ---- 20710 metric train_loss = 0.039151577791199085\n",
      "classification ---- 20720 metric train_loss = 0.026991765666753053\n",
      "classification ---- 20730 metric train_loss = 0.03242882711347193\n",
      "classification ---- 20740 metric train_loss = 0.015816970681771635\n",
      "classification ---- 20750 metric train_loss = 0.038288271287456155\n",
      "classification ---- 20760 metric train_loss = 0.022471571667119862\n",
      "classification ---- 20770 metric train_loss = 0.028616376919671893\n",
      "classification ---- 20780 metric train_loss = 0.03943995635490864\n",
      "classification ---- 20790 metric train_loss = 0.014628327125683427\n",
      "classification ---- 20800 metric train_loss = 0.017952017462812365\n",
      "classification ---- 20810 metric train_loss = 0.03393713473342359\n",
      "classification ---- 20820 metric train_loss = 0.034818586567416786\n",
      "classification ---- 20830 metric train_loss = 0.01956931743770838\n",
      "classification ---- 20840 metric train_loss = 0.03739728678483516\n",
      "classification ---- 20850 metric train_loss = 0.049397322628647086\n",
      "classification ---- 20860 metric train_loss = 0.052099971449933946\n",
      "classification ---- 20870 metric train_loss = 0.02555080708116293\n",
      "classification ---- 20880 metric train_loss = 0.05077041250187904\n",
      "classification ---- 20890 metric train_loss = 0.0377926891669631\n",
      "classification ---- 20900 metric train_loss = 0.041959144780412315\n",
      "classification ---- 20910 metric train_loss = 0.0200788835529238\n",
      "classification ---- 20920 metric train_loss = 0.015479726111516356\n",
      "classification ---- 20930 metric train_loss = 0.019481645128689707\n",
      "classification ---- 20940 metric train_loss = 0.019534819340333343\n",
      "classification ---- 20950 metric train_loss = 0.030125924642197786\n",
      "classification ---- 20960 metric train_loss = 0.03553030624752864\n",
      "classification ---- 20970 metric train_loss = 0.03000315511599183\n",
      "classification ---- 20980 metric train_loss = 0.014718626951798796\n",
      "classification ---- 20990 metric train_loss = 0.026196318212896585\n",
      "classification ---- 21000 metric train_loss = 0.012008678982965648\n",
      "classification ---- 43 metric test_f1 = 0.9930978235004586\n",
      "classification ---- 22 metric train_f1 = 0.9929910917984116\n",
      "classification ---- 21010 metric train_loss = 0.023612995492294432\n",
      "classification ---- 21020 metric train_loss = 0.032611750555224715\n",
      "classification ---- 21030 metric train_loss = 0.025697584403678773\n",
      "classification ---- 21040 metric train_loss = 0.04876257674768567\n",
      "classification ---- 21050 metric train_loss = 0.05360218391288072\n",
      "classification ---- 21060 metric train_loss = 0.02412557527422905\n",
      "classification ---- 21070 metric train_loss = 0.03331093164160848\n",
      "classification ---- 21080 metric train_loss = 0.044008277193643155\n",
      "classification ---- 21090 metric train_loss = 0.034432580880820754\n",
      "classification ---- 21100 metric train_loss = 0.015901569928973912\n",
      "classification ---- 21110 metric train_loss = 0.03546902360394597\n",
      "classification ---- 21120 metric train_loss = 0.02039686026982963\n",
      "classification ---- 21130 metric train_loss = 0.017383140837773682\n",
      "classification ---- 21140 metric train_loss = 0.01663693436421454\n",
      "classification ---- 21150 metric train_loss = 0.02827774053439498\n",
      "classification ---- 21160 metric train_loss = 0.018531562550924717\n",
      "classification ---- 21170 metric train_loss = 0.014090367220342159\n",
      "classification ---- 21180 metric train_loss = 0.023524222639389337\n",
      "classification ---- 21190 metric train_loss = 0.024596726219169796\n",
      "classification ---- 21200 metric train_loss = 0.009345020772889257\n",
      "classification ---- 21210 metric train_loss = 0.026952635357156397\n",
      "classification ---- 21220 metric train_loss = 0.029054052708670498\n",
      "classification ---- 21230 metric train_loss = 0.03444955414161086\n",
      "classification ---- 21240 metric train_loss = 0.0292212029104121\n",
      "classification ---- 21250 metric train_loss = 0.057324436074122784\n",
      "classification ---- 21260 metric train_loss = 0.039362065680325034\n",
      "classification ---- 21270 metric train_loss = 0.023662142199464144\n",
      "classification ---- 21280 metric train_loss = 0.03690742868930101\n",
      "classification ---- 21290 metric train_loss = 0.0347556660650298\n",
      "classification ---- 21300 metric train_loss = 0.012577968137338757\n",
      "classification ---- 21310 metric train_loss = 0.07382782739587128\n",
      "classification ---- 21320 metric train_loss = 0.0414607948390767\n",
      "classification ---- 21330 metric train_loss = 0.020686125149950385\n",
      "classification ---- 21340 metric train_loss = 0.032658922299742696\n",
      "classification ---- 21350 metric train_loss = 0.00744502383749932\n",
      "classification ---- 21360 metric train_loss = 0.039236581907607614\n",
      "classification ---- 21370 metric train_loss = 0.03686074421275407\n",
      "classification ---- 21380 metric train_loss = 0.02062896528514102\n",
      "classification ---- 21390 metric train_loss = 0.016191932489164174\n",
      "classification ---- 21400 metric train_loss = 0.039789561578072605\n",
      "classification ---- 21410 metric train_loss = 0.04914899796713144\n",
      "classification ---- 21420 metric train_loss = 0.05166153069585562\n",
      "classification ---- 21430 metric train_loss = 0.02692476916126907\n",
      "classification ---- 21440 metric train_loss = 0.06706666052341462\n",
      "classification ---- 21450 metric train_loss = 0.025389536656439303\n",
      "classification ---- 21460 metric train_loss = 0.0423045898322016\n",
      "classification ---- 21470 metric train_loss = 0.037792129744775596\n",
      "classification ---- 21480 metric train_loss = 0.021528593311086298\n",
      "classification ---- 21490 metric train_loss = 0.041978091234341264\n",
      "classification ---- 21500 metric train_loss = 0.038217441528104246\n",
      "classification ---- 44 metric test_f1 = 0.9914136861144007\n",
      "classification ---- 21510 metric train_loss = 0.030188013426959516\n",
      "classification ---- 21520 metric train_loss = 0.05117384742479771\n",
      "classification ---- 21530 metric train_loss = 0.031541457795538005\n",
      "classification ---- 21540 metric train_loss = 0.01489679873920977\n",
      "classification ---- 21550 metric train_loss = 0.042066891212016344\n",
      "classification ---- 21560 metric train_loss = 0.024434880376793443\n",
      "classification ---- 21570 metric train_loss = 0.015773232327774167\n",
      "classification ---- 21580 metric train_loss = 0.047050024615600705\n",
      "classification ---- 21590 metric train_loss = 0.01597050726413727\n",
      "classification ---- 21600 metric train_loss = 0.023073023883625865\n",
      "classification ---- 21610 metric train_loss = 0.048662288067862394\n",
      "classification ---- 21620 metric train_loss = 0.03458139947615564\n",
      "classification ---- 21630 metric train_loss = 0.03741334625519812\n",
      "classification ---- 21640 metric train_loss = 0.019764031120575964\n",
      "classification ---- 21650 metric train_loss = 0.009393252874724567\n",
      "classification ---- 21660 metric train_loss = 0.027074194559827448\n",
      "classification ---- 21670 metric train_loss = 0.01378028728067875\n",
      "classification ---- 21680 metric train_loss = 0.008288113365415484\n",
      "classification ---- 21690 metric train_loss = 0.015352988219819962\n",
      "classification ---- 21700 metric train_loss = 0.04567874351050705\n",
      "classification ---- 21710 metric train_loss = 0.015531343407928944\n",
      "classification ---- 21720 metric train_loss = 0.016624507796950638\n",
      "classification ---- 21730 metric train_loss = 0.02633770931279287\n",
      "classification ---- 21740 metric train_loss = 0.030456403037533164\n",
      "classification ---- 21750 metric train_loss = 0.04736407501623034\n",
      "classification ---- 21760 metric train_loss = 0.031278270855546\n",
      "classification ---- 21770 metric train_loss = 0.029861650918610393\n",
      "classification ---- 21780 metric train_loss = 0.018810877902433278\n",
      "classification ---- 21790 metric train_loss = 0.04347095482517034\n",
      "classification ---- 21800 metric train_loss = 0.029724794928915797\n",
      "classification ---- 21810 metric train_loss = 0.022001390997320413\n",
      "classification ---- 21820 metric train_loss = 0.021679928549565373\n",
      "classification ---- 21830 metric train_loss = 0.016375500545836984\n",
      "classification ---- 21840 metric train_loss = 0.005048034153878689\n",
      "classification ---- 21850 metric train_loss = 0.020356976229231806\n",
      "classification ---- 21860 metric train_loss = 0.023587696207687258\n",
      "classification ---- 21870 metric train_loss = 0.014759671315550804\n",
      "classification ---- 21880 metric train_loss = 0.037472639605402945\n",
      "classification ---- 21890 metric train_loss = 0.011055801529437304\n",
      "classification ---- 21900 metric train_loss = 0.024617957836017012\n",
      "classification ---- 21910 metric train_loss = 0.060885124641936274\n",
      "classification ---- 21920 metric train_loss = 0.01559473613742739\n",
      "classification ---- 21930 metric train_loss = 0.023126895003952087\n",
      "classification ---- 21940 metric train_loss = 0.02465005456469953\n",
      "classification ---- 21950 metric train_loss = 0.036215151124633846\n",
      "classification ---- 21960 metric train_loss = 0.02707891892641783\n",
      "classification ---- 21970 metric train_loss = 0.02885877201333642\n",
      "classification ---- 21980 metric train_loss = 0.011159061407670378\n",
      "classification ---- 21990 metric train_loss = 0.024167891358956695\n",
      "classification ---- 22000 metric train_loss = 0.02721013881964609\n",
      "classification ---- 45 metric test_f1 = 0.9934779390315585\n",
      "classification ---- 23 metric train_f1 = 0.9929971204354587\n",
      "classification ---- 22010 metric train_loss = 0.04310386509168893\n",
      "classification ---- 22020 metric train_loss = 0.024650429491885006\n",
      "classification ---- 22030 metric train_loss = 0.05408052429556846\n",
      "classification ---- 22040 metric train_loss = 0.054057493805885315\n",
      "classification ---- 22050 metric train_loss = 0.05616320092231035\n",
      "classification ---- 22060 metric train_loss = 0.01869247420690954\n",
      "classification ---- 22070 metric train_loss = 0.033443931583315135\n",
      "classification ---- 22080 metric train_loss = 0.013873578747734427\n",
      "classification ---- 22090 metric train_loss = 0.01284850500524044\n",
      "classification ---- 22100 metric train_loss = 0.04239053468918428\n",
      "classification ---- 22110 metric train_loss = 0.02801412520930171\n",
      "classification ---- 22120 metric train_loss = 0.03735257020452991\n",
      "classification ---- 22130 metric train_loss = 0.007691863272339105\n",
      "classification ---- 22140 metric train_loss = 0.014341373136267066\n",
      "classification ---- 22150 metric train_loss = 0.05183438155800104\n",
      "classification ---- 22160 metric train_loss = 0.04241017987951636\n",
      "classification ---- 22170 metric train_loss = 0.016815818566828965\n",
      "classification ---- 22180 metric train_loss = 0.020668351044878363\n",
      "classification ---- 22190 metric train_loss = 0.023330338567029686\n",
      "classification ---- 22200 metric train_loss = 0.05261353580281138\n",
      "classification ---- 22210 metric train_loss = 0.03781319353729486\n",
      "classification ---- 22220 metric train_loss = 0.05175778013654053\n",
      "classification ---- 22230 metric train_loss = 0.025904452544637024\n",
      "classification ---- 22240 metric train_loss = 0.01492744095157832\n",
      "classification ---- 22250 metric train_loss = 0.030089555121958256\n",
      "classification ---- 22260 metric train_loss = 0.04459378458559513\n",
      "classification ---- 22270 metric train_loss = 0.016118820360861718\n",
      "classification ---- 22280 metric train_loss = 0.015151096042245626\n",
      "classification ---- 22290 metric train_loss = 0.013759218761697412\n",
      "classification ---- 22300 metric train_loss = 0.03906703747343272\n",
      "classification ---- 22310 metric train_loss = 0.03384750885888934\n",
      "classification ---- 22320 metric train_loss = 0.011092441086657346\n",
      "classification ---- 22330 metric train_loss = 0.022015754494350403\n",
      "classification ---- 22340 metric train_loss = 0.020784813491627575\n",
      "classification ---- 22350 metric train_loss = 0.016822075005620718\n",
      "classification ---- 22360 metric train_loss = 0.026886888849548995\n",
      "classification ---- 22370 metric train_loss = 0.01284734916407615\n",
      "classification ---- 22380 metric train_loss = 0.06151241990737617\n",
      "classification ---- 22390 metric train_loss = 0.051694157323800026\n",
      "classification ---- 22400 metric train_loss = 0.044597922987304625\n",
      "classification ---- 22410 metric train_loss = 0.013818406360223889\n",
      "classification ---- 22420 metric train_loss = 0.034819981455802916\n",
      "classification ---- 22430 metric train_loss = 0.036013380507938564\n",
      "classification ---- 22440 metric train_loss = 0.04987532841041684\n",
      "classification ---- 22450 metric train_loss = 0.02077184394001961\n",
      "classification ---- 22460 metric train_loss = 0.04327902044169605\n",
      "classification ---- 22470 metric train_loss = 0.03371072565205395\n",
      "classification ---- 22480 metric train_loss = 0.03621159442700446\n",
      "classification ---- 22490 metric train_loss = 0.03111424660310149\n",
      "classification ---- 22500 metric train_loss = 0.01634562180843204\n",
      "classification ---- 46 metric test_f1 = 0.9917755537046721\n",
      "classification ---- 22510 metric train_loss = 0.032175774499773976\n",
      "classification ---- 22520 metric train_loss = 0.04237691063899547\n",
      "classification ---- 22530 metric train_loss = 0.015062954858876765\n",
      "classification ---- 22540 metric train_loss = 0.025818365113809704\n",
      "classification ---- 22550 metric train_loss = 0.014299051323905587\n",
      "classification ---- 22560 metric train_loss = 0.03477186090312898\n",
      "classification ---- 22570 metric train_loss = 0.02865152764134109\n",
      "classification ---- 22580 metric train_loss = 0.04005517987534404\n",
      "classification ---- 22590 metric train_loss = 0.040440413635224104\n",
      "classification ---- 22600 metric train_loss = 0.035490440437570214\n",
      "classification ---- 22610 metric train_loss = 0.03487950540147722\n",
      "classification ---- 22620 metric train_loss = 0.027851762692444028\n",
      "classification ---- 22630 metric train_loss = 0.03863492868840694\n",
      "classification ---- 22640 metric train_loss = 0.04121429012157023\n",
      "classification ---- 22650 metric train_loss = 0.04260991895571351\n",
      "classification ---- 22660 metric train_loss = 0.045173945790156725\n",
      "classification ---- 22670 metric train_loss = 0.017266739765182136\n",
      "classification ---- 22680 metric train_loss = 0.01917831916362047\n",
      "classification ---- 22690 metric train_loss = 0.01323739958461374\n",
      "classification ---- 22700 metric train_loss = 0.02369970320723951\n",
      "classification ---- 22710 metric train_loss = 0.013559618615545332\n",
      "classification ---- 22720 metric train_loss = 0.01840627647470683\n",
      "classification ---- 22730 metric train_loss = 0.027699783840216697\n",
      "classification ---- 22740 metric train_loss = 0.007874227804131806\n",
      "classification ---- 22750 metric train_loss = 0.029259424470365048\n",
      "classification ---- 22760 metric train_loss = 0.02511318735778332\n",
      "classification ---- 22770 metric train_loss = 0.022131355933379383\n",
      "classification ---- 22780 metric train_loss = 0.02147779588121921\n",
      "classification ---- 22790 metric train_loss = 0.05366253685206175\n",
      "classification ---- 22800 metric train_loss = 0.02969330914784223\n",
      "classification ---- 22810 metric train_loss = 0.02451627906411886\n",
      "classification ---- 22820 metric train_loss = 0.020092412759549916\n",
      "classification ---- 22830 metric train_loss = 0.027670106431469322\n",
      "classification ---- 22840 metric train_loss = 0.08520415183156729\n",
      "classification ---- 22850 metric train_loss = 0.030439260136336087\n",
      "classification ---- 22860 metric train_loss = 0.027312437491491436\n",
      "classification ---- 22870 metric train_loss = 0.03167242349591106\n",
      "classification ---- 22880 metric train_loss = 0.04655594485811889\n",
      "classification ---- 22890 metric train_loss = 0.03718276943545788\n",
      "classification ---- 22900 metric train_loss = 0.039690817892551425\n",
      "classification ---- 22910 metric train_loss = 0.03464058684185147\n",
      "classification ---- 22920 metric train_loss = 0.03266411162912845\n",
      "classification ---- 22930 metric train_loss = 0.013867280283011495\n",
      "classification ---- 22940 metric train_loss = 0.03149258913472295\n",
      "classification ---- 22950 metric train_loss = 0.014944449253380298\n",
      "classification ---- 22960 metric train_loss = 0.03065832778811455\n",
      "classification ---- 22970 metric train_loss = 0.021958060492761434\n",
      "classification ---- 22980 metric train_loss = 0.023832401831168682\n",
      "classification ---- 22990 metric train_loss = 0.02464610048336908\n",
      "classification ---- 23000 metric train_loss = 0.037922832369804385\n",
      "classification ---- 47 metric test_f1 = 0.9927492182998654\n",
      "classification ---- 24 metric train_f1 = 0.9924151704976758\n",
      "classification ---- 23010 metric train_loss = 0.021594472182914614\n",
      "classification ---- 23020 metric train_loss = 0.03959707826143131\n",
      "classification ---- 23030 metric train_loss = 0.044785215985029936\n",
      "classification ---- 23040 metric train_loss = 0.018501086882315575\n",
      "classification ---- 23050 metric train_loss = 0.048962642531841995\n",
      "classification ---- 23060 metric train_loss = 0.03308381671085954\n",
      "classification ---- 23070 metric train_loss = 0.017456888989545404\n",
      "classification ---- 23080 metric train_loss = 0.03178533168975264\n",
      "classification ---- 23090 metric train_loss = 0.009809423331171274\n",
      "classification ---- 23100 metric train_loss = 0.043925501336343584\n",
      "classification ---- 23110 metric train_loss = 0.032494999142363666\n",
      "classification ---- 23120 metric train_loss = 0.041728158155456184\n",
      "classification ---- 23130 metric train_loss = 0.027536876522935926\n",
      "classification ---- 23140 metric train_loss = 0.02954966325778514\n",
      "classification ---- 23150 metric train_loss = 0.040459776762872934\n",
      "classification ---- 23160 metric train_loss = 0.01604751921258867\n",
      "classification ---- 23170 metric train_loss = 0.017265476216562093\n",
      "classification ---- 23180 metric train_loss = 0.011328944913111627\n",
      "classification ---- 23190 metric train_loss = 0.04568694420158863\n",
      "classification ---- 23200 metric train_loss = 0.022944862907752394\n",
      "classification ---- 23210 metric train_loss = 0.035832321946509184\n",
      "classification ---- 23220 metric train_loss = 0.037258257577195766\n",
      "classification ---- 23230 metric train_loss = 0.03064036085270345\n",
      "classification ---- 23240 metric train_loss = 0.022143026790581645\n",
      "classification ---- 23250 metric train_loss = 0.04052482016850263\n",
      "classification ---- 23260 metric train_loss = 0.041223192354664207\n",
      "classification ---- 23270 metric train_loss = 0.04007940953597426\n",
      "classification ---- 23280 metric train_loss = 0.06338810911402107\n",
      "classification ---- 23290 metric train_loss = 0.02471842085942626\n",
      "classification ---- 23300 metric train_loss = 0.04470339547842741\n",
      "classification ---- 23310 metric train_loss = 0.02710349645931274\n",
      "classification ---- 23320 metric train_loss = 0.04451017105020583\n",
      "classification ---- 23330 metric train_loss = 0.021269118832424282\n",
      "classification ---- 23340 metric train_loss = 0.023756450274959207\n",
      "classification ---- 23350 metric train_loss = 0.0346766437869519\n",
      "classification ---- 23360 metric train_loss = 0.01577849064487964\n",
      "classification ---- 23370 metric train_loss = 0.03491770415566862\n",
      "classification ---- 23380 metric train_loss = 0.03849125111009925\n",
      "classification ---- 23390 metric train_loss = 0.030090934317559003\n",
      "classification ---- 23400 metric train_loss = 0.03002051739022136\n",
      "classification ---- 23410 metric train_loss = 0.02978292026091367\n",
      "classification ---- 23420 metric train_loss = 0.030059963860549033\n",
      "classification ---- 23430 metric train_loss = 0.029827398993074895\n",
      "classification ---- 23440 metric train_loss = 0.01890062934253365\n",
      "classification ---- 23450 metric train_loss = 0.019434385537169873\n",
      "classification ---- 23460 metric train_loss = 0.03912799822865054\n",
      "classification ---- 23470 metric train_loss = 0.009866652870550751\n",
      "classification ---- 23480 metric train_loss = 0.047792930225841704\n",
      "classification ---- 23490 metric train_loss = 0.01912995260208845\n",
      "classification ---- 23500 metric train_loss = 0.032960715563967825\n",
      "classification ---- 48 metric test_f1 = 0.9934147601897036\n",
      "classification ---- 23510 metric train_loss = 0.03969020089134574\n",
      "classification ---- 23520 metric train_loss = 0.03227367468643934\n",
      "classification ---- 23530 metric train_loss = 0.027394165052101016\n",
      "classification ---- 23540 metric train_loss = 0.026835103030316533\n",
      "classification ---- 23550 metric train_loss = 0.015689145750366153\n",
      "classification ---- 23560 metric train_loss = 0.03096220826264471\n",
      "classification ---- 23570 metric train_loss = 0.02489712475799024\n",
      "classification ---- 23580 metric train_loss = 0.03776756844017655\n",
      "classification ---- 23590 metric train_loss = 0.05202142340131104\n",
      "classification ---- 23600 metric train_loss = 0.021860245848074555\n",
      "classification ---- 23610 metric train_loss = 0.04179043294861913\n",
      "classification ---- 23620 metric train_loss = 0.036621986469253895\n",
      "classification ---- 23630 metric train_loss = 0.012257279991172255\n",
      "classification ---- 23640 metric train_loss = 0.02270147402305156\n",
      "classification ---- 23650 metric train_loss = 0.04291605395264923\n",
      "classification ---- 23660 metric train_loss = 0.02072351344395429\n",
      "classification ---- 23670 metric train_loss = 0.018689975584857167\n",
      "classification ---- 23680 metric train_loss = 0.030151645455043764\n",
      "classification ---- 23690 metric train_loss = 0.0291309553431347\n",
      "classification ---- 23700 metric train_loss = 0.021393388276919723\n",
      "classification ---- 23710 metric train_loss = 0.01743767117150128\n",
      "classification ---- 23720 metric train_loss = 0.025646705529652536\n",
      "classification ---- 23730 metric train_loss = 0.03974494156427681\n",
      "classification ---- 23740 metric train_loss = 0.013928233063779771\n",
      "classification ---- 23750 metric train_loss = 0.01793347140774131\n",
      "classification ---- 23760 metric train_loss = 0.035964087629690764\n",
      "classification ---- 23770 metric train_loss = 0.04060137467458844\n",
      "classification ---- 23780 metric train_loss = 0.011269664764404297\n",
      "classification ---- 23790 metric train_loss = 0.014546151657123118\n",
      "classification ---- 23800 metric train_loss = 0.05093737385468557\n",
      "classification ---- 23810 metric train_loss = 0.012153579364530742\n",
      "classification ---- 23820 metric train_loss = 0.024528490984812377\n",
      "classification ---- 23830 metric train_loss = 0.03830841053277254\n",
      "classification ---- 23840 metric train_loss = 0.034200607682578266\n",
      "classification ---- 23850 metric train_loss = 0.02058264804072678\n",
      "classification ---- 23860 metric train_loss = 0.04165700495941564\n",
      "classification ---- 23870 metric train_loss = 0.01614672828000039\n",
      "classification ---- 23880 metric train_loss = 0.03925095517188311\n",
      "classification ---- 23890 metric train_loss = 0.045864740014076234\n",
      "classification ---- 23900 metric train_loss = 0.036305887484923006\n",
      "classification ---- 23910 metric train_loss = 0.03369906730949879\n",
      "classification ---- 23920 metric train_loss = 0.03847502723801881\n",
      "classification ---- 23930 metric train_loss = 0.014765811525285244\n",
      "classification ---- 23940 metric train_loss = 0.026230981759727\n",
      "classification ---- 23950 metric train_loss = 0.020268058124929666\n",
      "classification ---- 23960 metric train_loss = 0.019452536827884614\n",
      "classification ---- 23970 metric train_loss = 0.048394312430173156\n",
      "classification ---- 23980 metric train_loss = 0.044305656058713797\n",
      "classification ---- 23990 metric train_loss = 0.010414290614426136\n",
      "classification ---- 24000 metric train_loss = 0.028979173768311738\n",
      "classification ---- 49 metric test_f1 = 0.9933277341873857\n",
      "classification ---- 25 metric train_f1 = 0.9923575544873701\n",
      "classification ---- 24010 metric train_loss = 0.007468798873014748\n",
      "classification ---- 24020 metric train_loss = 0.02269490105099976\n",
      "classification ---- 24030 metric train_loss = 0.028375878767110407\n",
      "classification ---- 24040 metric train_loss = 0.03101744531886652\n",
      "classification ---- 24050 metric train_loss = 0.027064653078559785\n",
      "classification ---- 24060 metric train_loss = 0.042695458559319374\n",
      "classification ---- 24070 metric train_loss = 0.036239958251826465\n",
      "classification ---- 24080 metric train_loss = 0.01816775866318494\n",
      "classification ---- 24090 metric train_loss = 0.01850248718401417\n",
      "classification ---- 24100 metric train_loss = 0.027631538966670633\n",
      "classification ---- 24110 metric train_loss = 0.044342074636369945\n",
      "classification ---- 24120 metric train_loss = 0.03384296093136072\n",
      "classification ---- 24130 metric train_loss = 0.016807971661910413\n",
      "classification ---- 24140 metric train_loss = 0.04528437943663448\n",
      "classification ---- 24150 metric train_loss = 0.04637365590315312\n",
      "classification ---- 24160 metric train_loss = 0.015674523590132595\n",
      "classification ---- 24170 metric train_loss = 0.01712377150543034\n",
      "classification ---- 24180 metric train_loss = 0.016552418703213334\n",
      "classification ---- 24190 metric train_loss = 0.020837437291629613\n",
      "classification ---- 24200 metric train_loss = 0.03778116821777076\n",
      "classification ---- 24210 metric train_loss = 0.04156048418954015\n",
      "classification ---- 24220 metric train_loss = 0.05585734131745994\n",
      "classification ---- 24230 metric train_loss = 0.02542176095303148\n",
      "classification ---- 24240 metric train_loss = 0.02861305852420628\n",
      "classification ---- 24250 metric train_loss = 0.04883262610528618\n",
      "classification ---- 24260 metric train_loss = 0.016810783091932534\n",
      "classification ---- 24270 metric train_loss = 0.020241990801878273\n",
      "classification ---- 24280 metric train_loss = 0.047320691985078156\n",
      "classification ---- 24290 metric train_loss = 0.04227984263561666\n",
      "classification ---- 24300 metric train_loss = 0.025621023843996227\n",
      "classification ---- 24310 metric train_loss = 0.01522887097671628\n",
      "classification ---- 24320 metric train_loss = 0.021005965652875602\n",
      "classification ---- 24330 metric train_loss = 0.017225977242924274\n",
      "classification ---- 24340 metric train_loss = 0.0432713677175343\n",
      "classification ---- 24350 metric train_loss = 0.021474267775192856\n",
      "classification ---- 24360 metric train_loss = 0.02504495745524764\n",
      "classification ---- 24370 metric train_loss = 0.024681825935840607\n",
      "classification ---- 24380 metric train_loss = 0.029708134173415602\n",
      "classification ---- 24390 metric train_loss = 0.015342739166226239\n",
      "classification ---- 24400 metric train_loss = 0.023744753282517194\n",
      "classification ---- 24410 metric train_loss = 0.02908515154849738\n",
      "classification ---- 24420 metric train_loss = 0.023740543611347677\n",
      "classification ---- 24430 metric train_loss = 0.033465650677680966\n",
      "classification ---- 24440 metric train_loss = 0.04641139316372574\n",
      "classification ---- 24450 metric train_loss = 0.021015706076286732\n",
      "classification ---- 24460 metric train_loss = 0.020578586589545013\n",
      "classification ---- 24470 metric train_loss = 0.01889381364453584\n",
      "classification ---- 24480 metric train_loss = 0.026665073819458485\n",
      "classification ---- 24490 metric train_loss = 0.041951833828352394\n",
      "classification ---- 24500 metric train_loss = 0.017527690902352334\n",
      "classification ---- 50 metric test_f1 = 0.9922787199978216\n",
      "classification ---- 24510 metric train_loss = 0.030297021055594085\n",
      "classification ---- 24520 metric train_loss = 0.05350953198503703\n",
      "classification ---- 24530 metric train_loss = 0.023082961281761526\n",
      "classification ---- 24540 metric train_loss = 0.03032477255910635\n",
      "classification ---- 24550 metric train_loss = 0.02148552199359983\n",
      "classification ---- 24560 metric train_loss = 0.030783920595422388\n",
      "classification ---- 24570 metric train_loss = 0.03276843576459214\n",
      "classification ---- 24580 metric train_loss = 0.027303195581771432\n",
      "classification ---- 24590 metric train_loss = 0.018092081346549094\n",
      "classification ---- 24600 metric train_loss = 0.03190633923513815\n",
      "classification ---- 24610 metric train_loss = 0.02683744584210217\n",
      "classification ---- 24620 metric train_loss = 0.03938316509593278\n",
      "classification ---- 24630 metric train_loss = 0.01705394508317113\n",
      "classification ---- 24640 metric train_loss = 0.009035857720300555\n",
      "classification ---- 24650 metric train_loss = 0.03494893319439143\n",
      "classification ---- 24660 metric train_loss = 0.04210777645930648\n",
      "classification ---- 24670 metric train_loss = 0.02383774407207966\n",
      "classification ---- 24680 metric train_loss = 0.011687566270120442\n",
      "classification ---- 24690 metric train_loss = 0.03239949163980782\n",
      "classification ---- 24700 metric train_loss = 0.025902711506932973\n",
      "classification ---- 24710 metric train_loss = 0.03352027449291199\n",
      "classification ---- 24720 metric train_loss = 0.023907097964547576\n",
      "classification ---- 24730 metric train_loss = 0.021452241775114088\n",
      "classification ---- 24740 metric train_loss = 0.03797393547138199\n",
      "classification ---- 24750 metric train_loss = 0.02576936346013099\n",
      "classification ---- 24760 metric train_loss = 0.022890278394334017\n",
      "classification ---- 24770 metric train_loss = 0.05103691328549757\n",
      "classification ---- 24780 metric train_loss = 0.06699373205192387\n",
      "classification ---- 24790 metric train_loss = 0.024266504077240826\n",
      "classification ---- 24800 metric train_loss = 0.021708177216351033\n",
      "classification ---- 24810 metric train_loss = 0.04047037120908499\n",
      "classification ---- 24820 metric train_loss = 0.017492857784964143\n",
      "classification ---- 24830 metric train_loss = 0.010199498315341771\n",
      "classification ---- 24840 metric train_loss = 0.038119098031893374\n",
      "classification ---- 24850 metric train_loss = 0.03097890317440033\n",
      "classification ---- 24860 metric train_loss = 0.026007640012539922\n",
      "classification ---- 24870 metric train_loss = 0.04735013155732304\n",
      "classification ---- 24880 metric train_loss = 0.0349881689529866\n",
      "classification ---- 24890 metric train_loss = 0.018437380134128035\n",
      "classification ---- 24900 metric train_loss = 0.02197810625657439\n",
      "classification ---- 24910 metric train_loss = 0.022423168190289288\n",
      "classification ---- 24920 metric train_loss = 0.027837947569787504\n",
      "classification ---- 24930 metric train_loss = 0.015924368542619048\n",
      "classification ---- 24940 metric train_loss = 0.03525847981218248\n",
      "classification ---- 24950 metric train_loss = 0.03404236587230116\n",
      "classification ---- 24960 metric train_loss = 0.04544007387012243\n",
      "classification ---- 24970 metric train_loss = 0.04140281279105693\n",
      "classification ---- 24980 metric train_loss = 0.02395621663890779\n",
      "classification ---- 24990 metric train_loss = 0.05006549851968885\n",
      "classification ---- 25000 metric train_loss = 0.04483574270270765\n",
      "classification ---- 51 metric test_f1 = 0.9939391878949182\n",
      "classification ---- 26 metric train_f1 = 0.993609614354124\n",
      "classification ---- 25010 metric train_loss = 0.047328137094154954\n",
      "classification ---- 25020 metric train_loss = 0.04006204595789313\n",
      "classification ---- 25030 metric train_loss = 0.021828124998137355\n",
      "classification ---- 25040 metric train_loss = 0.04164343292359263\n",
      "classification ---- 25050 metric train_loss = 0.031142669916152953\n",
      "classification ---- 25060 metric train_loss = 0.01696336839813739\n",
      "classification ---- 25070 metric train_loss = 0.027326881280168892\n",
      "classification ---- 25080 metric train_loss = 0.01961278929375112\n",
      "classification ---- 25090 metric train_loss = 0.01949347755871713\n",
      "classification ---- 25100 metric train_loss = 0.03519595279358327\n",
      "classification ---- 25110 metric train_loss = 0.03693189865443856\n",
      "classification ---- 25120 metric train_loss = 0.03522563256556168\n",
      "classification ---- 25130 metric train_loss = 0.022958588693290947\n",
      "classification ---- 25140 metric train_loss = 0.04539498458616435\n",
      "classification ---- 25150 metric train_loss = 0.01585471532307565\n",
      "classification ---- 25160 metric train_loss = 0.022398695815354586\n",
      "classification ---- 25170 metric train_loss = 0.05576034220866859\n",
      "classification ---- 25180 metric train_loss = 0.023936279234476388\n",
      "classification ---- 25190 metric train_loss = 0.008204469946213067\n",
      "classification ---- 25200 metric train_loss = 0.04426868960727006\n",
      "classification ---- 25210 metric train_loss = 0.03464203625917435\n",
      "classification ---- 25220 metric train_loss = 0.04954935940913856\n",
      "classification ---- 25230 metric train_loss = 0.03529876344837248\n",
      "classification ---- 25240 metric train_loss = 0.011173792323097587\n",
      "classification ---- 25250 metric train_loss = 0.04279643560294062\n",
      "classification ---- 25260 metric train_loss = 0.0289705099305138\n",
      "classification ---- 25270 metric train_loss = 0.017766212974675\n",
      "classification ---- 25280 metric train_loss = 0.020344811771064997\n",
      "classification ---- 25290 metric train_loss = 0.02086763074621558\n",
      "classification ---- 25300 metric train_loss = 0.027815170213580133\n",
      "classification ---- 25310 metric train_loss = 0.01276809184346348\n",
      "classification ---- 25320 metric train_loss = 0.03088775051292032\n",
      "classification ---- 25330 metric train_loss = 0.015740800742059947\n",
      "classification ---- 25340 metric train_loss = 0.014132604491896928\n",
      "classification ---- 25350 metric train_loss = 0.02042515908833593\n",
      "classification ---- 25360 metric train_loss = 0.027797187841497362\n",
      "classification ---- 25370 metric train_loss = 0.03278298336081207\n",
      "classification ---- 25380 metric train_loss = 0.026867927168495953\n",
      "classification ---- 25390 metric train_loss = 0.03270641949493438\n",
      "classification ---- 25400 metric train_loss = 0.041946571646258236\n",
      "classification ---- 25410 metric train_loss = 0.014702168526127934\n",
      "classification ---- 25420 metric train_loss = 0.03305668402463198\n",
      "classification ---- 25430 metric train_loss = 0.0433526870328933\n",
      "classification ---- 25440 metric train_loss = 0.011533441627398134\n",
      "classification ---- 25450 metric train_loss = 0.03802729262970388\n",
      "classification ---- 25460 metric train_loss = 0.030566355609335006\n",
      "classification ---- 25470 metric train_loss = 0.028589596832171084\n",
      "classification ---- 25480 metric train_loss = 0.02185180358355865\n",
      "classification ---- 25490 metric train_loss = 0.02406269027851522\n",
      "classification ---- 25500 metric train_loss = 0.018009679042734206\n",
      "classification ---- 52 metric test_f1 = 0.9939865527841911\n",
      "classification ---- 25510 metric train_loss = 0.035650809318758546\n",
      "classification ---- 25520 metric train_loss = 0.016307176114059983\n",
      "classification ---- 25530 metric train_loss = 0.04862809325568378\n",
      "classification ---- 25540 metric train_loss = 0.02446891344152391\n",
      "classification ---- 25550 metric train_loss = 0.0065556765301153066\n",
      "classification ---- 25560 metric train_loss = 0.04322401799727231\n",
      "classification ---- 25570 metric train_loss = 0.060651358542963864\n",
      "classification ---- 25580 metric train_loss = 0.024361862568184733\n",
      "classification ---- 25590 metric train_loss = 0.031712470366619526\n",
      "classification ---- 25600 metric train_loss = 0.023440636903978886\n",
      "classification ---- 25610 metric train_loss = 0.013504364690743387\n",
      "classification ---- 25620 metric train_loss = 0.017949719820171593\n",
      "classification ---- 25630 metric train_loss = 0.038909946219064295\n",
      "classification ---- 25640 metric train_loss = 0.02473266322631389\n",
      "classification ---- 25650 metric train_loss = 0.021777949272654952\n",
      "classification ---- 25660 metric train_loss = 0.00690660128602758\n",
      "classification ---- 25670 metric train_loss = 0.03088903771713376\n",
      "classification ---- 25680 metric train_loss = 0.04570229859091342\n",
      "classification ---- 25690 metric train_loss = 0.022462189733050763\n",
      "classification ---- 25700 metric train_loss = 0.01904649136122316\n",
      "classification ---- 25710 metric train_loss = 0.0198947491357103\n",
      "classification ---- 25720 metric train_loss = 0.049605926196090874\n",
      "classification ---- 25730 metric train_loss = 0.022048316523432733\n",
      "classification ---- 25740 metric train_loss = 0.02636319869197905\n",
      "classification ---- 25750 metric train_loss = 0.006792615680024028\n",
      "classification ---- 25760 metric train_loss = 0.023671614192426204\n",
      "classification ---- 25770 metric train_loss = 0.007629794534295797\n",
      "classification ---- 25780 metric train_loss = 0.011711944651324302\n",
      "classification ---- 25790 metric train_loss = 0.038213637587614355\n",
      "classification ---- 25800 metric train_loss = 0.03125943928025663\n",
      "classification ---- 25810 metric train_loss = 0.0146805060794577\n",
      "classification ---- 25820 metric train_loss = 0.0339041794417426\n",
      "classification ---- 25830 metric train_loss = 0.01160740825580433\n",
      "classification ---- 25840 metric train_loss = 0.040585109614767134\n",
      "classification ---- 25850 metric train_loss = 0.05316807406488806\n",
      "classification ---- 25860 metric train_loss = 0.021827039774507284\n",
      "classification ---- 25870 metric train_loss = 0.03175863814540207\n",
      "classification ---- 25880 metric train_loss = 0.04860883837100118\n",
      "classification ---- 25890 metric train_loss = 0.02454901996534318\n",
      "classification ---- 25900 metric train_loss = 0.033891179226338865\n",
      "classification ---- 25910 metric train_loss = 0.02409925414249301\n",
      "classification ---- 25920 metric train_loss = 0.04442362037952989\n",
      "classification ---- 25930 metric train_loss = 0.009747386910021305\n",
      "classification ---- 25940 metric train_loss = 0.008817018382251263\n",
      "classification ---- 25950 metric train_loss = 0.01650904081761837\n",
      "classification ---- 25960 metric train_loss = 0.01209966066526249\n",
      "classification ---- 25970 metric train_loss = 0.015438094176352024\n",
      "classification ---- 25980 metric train_loss = 0.040425047918688506\n",
      "classification ---- 25990 metric train_loss = 0.03035247668158263\n",
      "classification ---- 26000 metric train_loss = 0.01487557680811733\n",
      "classification ---- 53 metric test_f1 = 0.9930741365983349\n",
      "classification ---- 27 metric train_f1 = 0.993337151144399\n",
      "classification ---- 26010 metric train_loss = 0.024240132281556727\n",
      "classification ---- 26020 metric train_loss = 0.013148257974535227\n",
      "classification ---- 26030 metric train_loss = 0.026405584858730437\n",
      "classification ---- 26040 metric train_loss = 0.04629033433739096\n",
      "classification ---- 26050 metric train_loss = 0.04104391711298376\n",
      "classification ---- 26060 metric train_loss = 0.07142939073964953\n",
      "classification ---- 26070 metric train_loss = 0.030980723351240157\n",
      "classification ---- 26080 metric train_loss = 0.027510502282530068\n",
      "classification ---- 26090 metric train_loss = 0.024794181156903506\n",
      "classification ---- 26100 metric train_loss = 0.026226041070185602\n",
      "classification ---- 26110 metric train_loss = 0.01724529939237982\n",
      "classification ---- 26120 metric train_loss = 0.030371942464262246\n",
      "classification ---- 26130 metric train_loss = 0.03079395617824048\n",
      "classification ---- 26140 metric train_loss = 0.029548484738916157\n",
      "classification ---- 26150 metric train_loss = 0.032434750627726314\n",
      "classification ---- 26160 metric train_loss = 0.011643143161199987\n",
      "classification ---- 26170 metric train_loss = 0.02034905687905848\n",
      "classification ---- 26180 metric train_loss = 0.011900720233097673\n",
      "classification ---- 26190 metric train_loss = 0.03208225634880364\n",
      "classification ---- 26200 metric train_loss = 0.026590564078651367\n",
      "classification ---- 26210 metric train_loss = 0.028820220101624727\n",
      "classification ---- 26220 metric train_loss = 0.01584183450322598\n",
      "classification ---- 26230 metric train_loss = 0.05123098164331168\n",
      "classification ---- 26240 metric train_loss = 0.02936553165782243\n",
      "classification ---- 26250 metric train_loss = 0.028615689207799732\n",
      "classification ---- 26260 metric train_loss = 0.02008971027098596\n",
      "classification ---- 26270 metric train_loss = 0.03621178911998868\n",
      "classification ---- 26280 metric train_loss = 0.018401810340583323\n",
      "classification ---- 26290 metric train_loss = 0.01677734488621354\n",
      "classification ---- 26300 metric train_loss = 0.029299065540544688\n",
      "classification ---- 26310 metric train_loss = 0.020272645447403193\n",
      "classification ---- 26320 metric train_loss = 0.019625246885698288\n",
      "classification ---- 26330 metric train_loss = 0.02626001329626888\n",
      "classification ---- 26340 metric train_loss = 0.03243576281238347\n",
      "classification ---- 26350 metric train_loss = 0.025891259056515993\n",
      "classification ---- 26360 metric train_loss = 0.05056835014838725\n",
      "classification ---- 26370 metric train_loss = 0.056647218042053284\n",
      "classification ---- 26380 metric train_loss = 0.022723041521385313\n",
      "classification ---- 26390 metric train_loss = 0.034834955469705164\n",
      "classification ---- 26400 metric train_loss = 0.02938296317588538\n",
      "classification ---- 26410 metric train_loss = 0.04092465508729219\n",
      "classification ---- 26420 metric train_loss = 0.03523032919038087\n",
      "classification ---- 26430 metric train_loss = 0.028901144000701606\n",
      "classification ---- 26440 metric train_loss = 0.03186102781910449\n",
      "classification ---- 26450 metric train_loss = 0.013882730389013886\n",
      "classification ---- 26460 metric train_loss = 0.03484783759340644\n",
      "classification ---- 26470 metric train_loss = 0.02861979454755783\n",
      "classification ---- 26480 metric train_loss = 0.023993905680254102\n",
      "classification ---- 26490 metric train_loss = 0.015805939887650312\n",
      "classification ---- 26500 metric train_loss = 0.030233854555990546\n",
      "classification ---- 54 metric test_f1 = 0.9932550598411318\n",
      "classification ---- 26510 metric train_loss = 0.011097721173427999\n",
      "classification ---- 26520 metric train_loss = 0.016553455498069525\n",
      "classification ---- 26530 metric train_loss = 0.04731654459610581\n",
      "classification ---- 26540 metric train_loss = 0.018026059959083796\n",
      "classification ---- 26550 metric train_loss = 0.02905705834273249\n",
      "classification ---- 26560 metric train_loss = 0.029486443841597064\n",
      "classification ---- 26570 metric train_loss = 0.053497565304860474\n",
      "classification ---- 26580 metric train_loss = 0.02783364476636052\n",
      "classification ---- 26590 metric train_loss = 0.016501260385848582\n",
      "classification ---- 26600 metric train_loss = 0.046771610737778246\n",
      "classification ---- 26610 metric train_loss = 0.025464538228698076\n",
      "classification ---- 26620 metric train_loss = 0.0191573467804119\n",
      "classification ---- 26630 metric train_loss = 0.0343541048001498\n",
      "classification ---- 26640 metric train_loss = 0.02636305212508887\n",
      "classification ---- 26650 metric train_loss = 0.029613207350485025\n",
      "classification ---- 26660 metric train_loss = 0.02737396121956408\n",
      "classification ---- 26670 metric train_loss = 0.02091300506144762\n",
      "classification ---- 26680 metric train_loss = 0.00825377571163699\n",
      "classification ---- 26690 metric train_loss = 0.013093063933774829\n",
      "classification ---- 26700 metric train_loss = 0.02527020506095141\n",
      "classification ---- 26710 metric train_loss = 0.013354306877590715\n",
      "classification ---- 26720 metric train_loss = 0.01807992309331894\n",
      "classification ---- 26730 metric train_loss = 0.024512461759150027\n",
      "classification ---- 26740 metric train_loss = 0.03036192376166582\n",
      "classification ---- 26750 metric train_loss = 0.04016537821153179\n",
      "classification ---- 26760 metric train_loss = 0.03202902656048536\n",
      "classification ---- 26770 metric train_loss = 0.01680576936341822\n",
      "classification ---- 26780 metric train_loss = 0.019257549208123236\n",
      "classification ---- 26790 metric train_loss = 0.014373900927603244\n",
      "classification ---- 26800 metric train_loss = 0.01251752992393449\n",
      "classification ---- 26810 metric train_loss = 0.02561763796256855\n",
      "classification ---- 26820 metric train_loss = 0.013628630549646914\n",
      "classification ---- 26830 metric train_loss = 0.01840156577527523\n",
      "classification ---- 26840 metric train_loss = 0.02239477203693241\n",
      "classification ---- 26850 metric train_loss = 0.01821992014301941\n",
      "classification ---- 26860 metric train_loss = 0.053647467074915765\n",
      "classification ---- 26870 metric train_loss = 0.01989221411058679\n",
      "classification ---- 26880 metric train_loss = 0.021236797142773867\n",
      "classification ---- 26890 metric train_loss = 0.028802288812585175\n",
      "classification ---- 26900 metric train_loss = 0.023469769721850754\n",
      "classification ---- 26910 metric train_loss = 0.044692347990348935\n",
      "classification ---- 26920 metric train_loss = 0.016913409810513258\n",
      "classification ---- 26930 metric train_loss = 0.044767972500994804\n",
      "classification ---- 26940 metric train_loss = 0.026361041353084147\n",
      "classification ---- 26950 metric train_loss = 0.017819157033227385\n",
      "classification ---- 26960 metric train_loss = 0.03240072689950466\n",
      "classification ---- 26970 metric train_loss = 0.024605296598747373\n",
      "classification ---- 26980 metric train_loss = 0.03305192035622895\n",
      "classification ---- 26990 metric train_loss = 0.056277430662885305\n",
      "classification ---- 27000 metric train_loss = 0.048171444330364466\n",
      "classification ---- 55 metric test_f1 = 0.9892642691618656\n",
      "classification ---- 28 metric train_f1 = 0.9885112348801286\n",
      "classification ---- 27010 metric train_loss = 0.028093140642158686\n",
      "classification ---- 27020 metric train_loss = 0.03676886190660298\n",
      "classification ---- 27030 metric train_loss = 0.02479029621463269\n",
      "classification ---- 27040 metric train_loss = 0.014229323354084045\n",
      "classification ---- 27050 metric train_loss = 0.01975922780111432\n",
      "classification ---- 27060 metric train_loss = 0.018292271997779608\n",
      "classification ---- 27070 metric train_loss = 0.03759516552090645\n",
      "classification ---- 27080 metric train_loss = 0.02653021365404129\n",
      "classification ---- 27090 metric train_loss = 0.04746972182765603\n",
      "classification ---- 27100 metric train_loss = 0.01500037768855691\n",
      "classification ---- 27110 metric train_loss = 0.03984531098976731\n",
      "classification ---- 27120 metric train_loss = 0.028845436591655015\n",
      "classification ---- 27130 metric train_loss = 0.03652761335251853\n",
      "classification ---- 27140 metric train_loss = 0.03309350423514843\n",
      "classification ---- 27150 metric train_loss = 0.029349040519446135\n",
      "classification ---- 27160 metric train_loss = 0.02465428365394473\n",
      "classification ---- 27170 metric train_loss = 0.06541567449457944\n",
      "classification ---- 27180 metric train_loss = 0.037113548582419754\n",
      "classification ---- 27190 metric train_loss = 0.016333505092188717\n",
      "classification ---- 27200 metric train_loss = 0.013986352668143808\n",
      "classification ---- 27210 metric train_loss = 0.04735222142189741\n",
      "classification ---- 27220 metric train_loss = 0.04160793269984424\n",
      "classification ---- 27230 metric train_loss = 0.032864353014156224\n",
      "classification ---- 27240 metric train_loss = 0.04182628565467894\n",
      "classification ---- 27250 metric train_loss = 0.033242318243719635\n",
      "classification ---- 27260 metric train_loss = 0.02613877581898123\n",
      "classification ---- 27270 metric train_loss = 0.025026206066831945\n",
      "classification ---- 27280 metric train_loss = 0.02104492821963504\n",
      "classification ---- 27290 metric train_loss = 0.007874318258836865\n",
      "classification ---- 27300 metric train_loss = 0.02026586653664708\n",
      "classification ---- 27310 metric train_loss = 0.012598315824288876\n",
      "classification ---- 27320 metric train_loss = 0.019183463091030716\n",
      "classification ---- 27330 metric train_loss = 0.007030371739529073\n",
      "classification ---- 27340 metric train_loss = 0.03253977078711614\n",
      "classification ---- 27350 metric train_loss = 0.03710125081706792\n",
      "classification ---- 27360 metric train_loss = 0.04012860058574006\n",
      "classification ---- 27370 metric train_loss = 0.010310675180517138\n",
      "classification ---- 27380 metric train_loss = 0.020004846365191044\n",
      "classification ---- 27390 metric train_loss = 0.027751850383356213\n",
      "classification ---- 27400 metric train_loss = 0.011493391520343722\n",
      "classification ---- 27410 metric train_loss = 0.03707862750161439\n",
      "classification ---- 27420 metric train_loss = 0.014729821542277931\n",
      "classification ---- 27430 metric train_loss = 0.04677611482329667\n",
      "classification ---- 27440 metric train_loss = 0.013392110983841122\n",
      "classification ---- 27450 metric train_loss = 0.016743697924539447\n",
      "classification ---- 27460 metric train_loss = 0.041216088575311005\n",
      "classification ---- 27470 metric train_loss = 0.02125683689955622\n",
      "classification ---- 27480 metric train_loss = 0.03547307949047536\n",
      "classification ---- 27490 metric train_loss = 0.030060180090367795\n",
      "classification ---- 27500 metric train_loss = 0.021804020926356315\n",
      "classification ---- 56 metric test_f1 = 0.9922232020983579\n",
      "classification ---- 27510 metric train_loss = 0.015807512181345372\n",
      "classification ---- 27520 metric train_loss = 0.03721618042327464\n",
      "classification ---- 27530 metric train_loss = 0.024084509431850164\n",
      "classification ---- 27540 metric train_loss = 0.04307785802520812\n",
      "classification ---- 27550 metric train_loss = 0.030275910929776727\n",
      "classification ---- 27560 metric train_loss = 0.06575872874818742\n",
      "classification ---- 27570 metric train_loss = 0.031002062605693936\n",
      "classification ---- 27580 metric train_loss = 0.030822413205169142\n",
      "classification ---- 27590 metric train_loss = 0.01597523402888328\n",
      "classification ---- 27600 metric train_loss = 0.011691320803947747\n",
      "classification ---- 27610 metric train_loss = 0.02943383171223104\n",
      "classification ---- 27620 metric train_loss = 0.03563028485514223\n",
      "classification ---- 27630 metric train_loss = 0.040867861919105054\n",
      "classification ---- 27640 metric train_loss = 0.03472895843442529\n",
      "classification ---- 27650 metric train_loss = 0.020175131876021622\n",
      "classification ---- 27660 metric train_loss = 0.060556739661842586\n",
      "classification ---- 27670 metric train_loss = 0.02877863636240363\n",
      "classification ---- 27680 metric train_loss = 0.021083911461755634\n",
      "classification ---- 27690 metric train_loss = 0.021895524417050182\n",
      "classification ---- 27700 metric train_loss = 0.020569541142322123\n",
      "classification ---- 27710 metric train_loss = 0.011106263170950115\n",
      "classification ---- 27720 metric train_loss = 0.008209279750008135\n",
      "classification ---- 27730 metric train_loss = 0.013129888597177342\n",
      "classification ---- 27740 metric train_loss = 0.041278458177112044\n",
      "classification ---- 27750 metric train_loss = 0.03583570993505418\n",
      "classification ---- 27760 metric train_loss = 0.026131992414593696\n",
      "classification ---- 27770 metric train_loss = 0.015141378005500883\n",
      "classification ---- 27780 metric train_loss = 0.03726384902838618\n",
      "classification ---- 27790 metric train_loss = 0.03829751845914871\n",
      "classification ---- 27800 metric train_loss = 0.01597890828270465\n",
      "classification ---- 27810 metric train_loss = 0.03839156476315111\n",
      "classification ---- 27820 metric train_loss = 0.02496189222438261\n",
      "classification ---- 27830 metric train_loss = 0.05032470719888806\n",
      "classification ---- 27840 metric train_loss = 0.03304644282907247\n",
      "classification ---- 27850 metric train_loss = 0.039324256032705306\n",
      "classification ---- 27860 metric train_loss = 0.04745238292962313\n",
      "classification ---- 27870 metric train_loss = 0.03567717089317739\n",
      "classification ---- 27880 metric train_loss = 0.028248760662972928\n",
      "classification ---- 27890 metric train_loss = 0.01717876901384443\n",
      "classification ---- 27900 metric train_loss = 0.05529835019260645\n",
      "classification ---- 27910 metric train_loss = 0.02762030242010951\n",
      "classification ---- 27920 metric train_loss = 0.02436585519462824\n",
      "classification ---- 27930 metric train_loss = 0.03498756866902113\n",
      "classification ---- 27940 metric train_loss = 0.03792458458337933\n",
      "classification ---- 27950 metric train_loss = 0.027672114013694227\n",
      "classification ---- 27960 metric train_loss = 0.04714674549177289\n",
      "classification ---- 27970 metric train_loss = 0.027838038560003043\n",
      "classification ---- 27980 metric train_loss = 0.030767205590382218\n",
      "classification ---- 27990 metric train_loss = 0.018008920224383475\n",
      "classification ---- 28000 metric train_loss = 0.02921873186714947\n",
      "classification ---- 57 metric test_f1 = 0.994121330680198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m classification_trainer \u001b[38;5;241m=\u001b[39m NNClassificationTrainer(feature_extractor\u001b[38;5;241m=\u001b[39m feature_extractor,classifier\u001b[38;5;241m=\u001b[39m classifier,device\u001b[38;5;241m=\u001b[39m device,logger\u001b[38;5;241m=\u001b[39m Logger(name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclassification_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\Desktop\\UNSW\\flowprintOptimal\\sekigo\\modeling\\trainers.py:132\u001b[0m, in \u001b[0;36mNNClassificationTrainer.train\u001b[1;34m(self, train_dataset, test_dataset, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maddMetric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_f1)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m     train_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcF1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maddMetric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_f1)\n\u001b[0;32m    134\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\Desktop\\UNSW\\flowprintOptimal\\sekigo\\modeling\\trainers.py:25\u001b[0m, in \u001b[0;36mBaseClassificationTrainer.calcF1\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalcF1\u001b[39m(\u001b[38;5;28mself\u001b[39m,dataset : BaseFlowDataset):\n\u001b[0;32m     24\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset))\n\u001b[1;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictOnDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     _,_,f1,_ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(labels, preds, average\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m,zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f1\n",
      "File \u001b[1;32mc:\\Users\\rushi\\Desktop\\UNSW\\flowprintOptimal\\sekigo\\modeling\\trainers.py:144\u001b[0m, in \u001b[0;36mNNClassificationTrainer.predictOnDataset\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    143\u001b[0m     batch_X,batch_y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 144\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m    146\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(batch_y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\rushi\\Desktop\\UNSW\\flowprintOptimal\\sekigo\\modeling\\trainers.py:113\u001b[0m, in \u001b[0;36mNNClassificationTrainer.predictStep\u001b[1;34m(self, batch_X)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 113\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\Desktop\\UNSW\\flowprintOptimal\\sekigo\\modeling\\neuralNetworks.py:46\u001b[0m, in \u001b[0;36mLSTMFeatureExtractor.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    X is the timeseries input of shape \u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    (BS,Seq len, lstm_input_size)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    The output is of shape (BS,num_classes)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X,torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     48\u001b[0m         lstm_out \u001b[38;5;241m=\u001b[39m lstm_out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classification_trainer = NNClassificationTrainer(feature_extractor= feature_extractor,classifier= classifier,device= device,logger= Logger(name= \"classification\"))\n",
    "classification_trainer.train(train_dataset= all_train_dataset,test_dataset= all_test_dataset,epochs= 30,batch_size= 64,lr= .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,ood_features = [],[]\n",
    "labels = []\n",
    "loader = DataLoader(all_train_dataset,batch_size=64)\n",
    "ood_loader = DataLoader(ood_dataset,batch_size= 32)\n",
    "\n",
    "feature_extractor.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch_out = feature_extractor(batch[\"data\"].float().to(device)).cpu().numpy().tolist()\n",
    "        batch_labels = batch[\"label\"].numpy().tolist()\n",
    "        labels.extend(batch_labels)\n",
    "        features.extend(batch_out)\n",
    "\n",
    "\n",
    "features = np.array(features)\n",
    "#ood_features = np.array(ood_features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99321"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = features#list(map(lambda x : x[\"data\"].reshape(-1),train_dataset))\n",
    "#ood_data = list(map(lambda x : x[\"data\"].reshape(-1),ood_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = np.array(real_data)\n",
    "#ood_data = np.array(ood_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25ef2cd6a90>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=30)  # Choose number of principal components\n",
    "pca_real_data = pca.fit_transform(real_data)\n",
    "\n",
    "# Fit t-SNE on PCA-transformed real data\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_real_data = tsne.fit_transform(pca_real_data)\n",
    "\n",
    "# Transform fake data using the learned PCA and t-SNE transformations\n",
    "#pca_ood_data = pca.transform(ood_data)\n",
    "#tsne_ood_data = tsne.fit_transform(pca_ood_data)\n",
    "\n",
    "# Plot t-SNE of PCA-transformed real and fake data\n",
    "plt.scatter(tsne_real_data[:, 0], tsne_real_data[:, 1], label='Real Data (PCA+t-SNE)', c='blue', alpha=0.5)\n",
    "#plt.scatter(tsne_ood_data[:, 0], tsne_ood_data[:, 1], label='Fake Data (PCA+t-SNE)', c='red', alpha=0.1)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Plot of PCA-Transformed Real and Fake Data')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
