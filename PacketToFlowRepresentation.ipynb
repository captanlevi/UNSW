{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from flowprintOptimal.sekigo.core.flowRepresentation import FlowRepresentation,PacketFlowRepressentation\n",
    "from flowprintOptimal.sekigo.dataAnalysis.vNATDataFrameProcessor import VNATDataFrameProcessor\n",
    "from flowprintOptimal.sekigo.core.flowConfig import FlowConfig\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import normalizePacketRep\n",
    "import os\n",
    "from flowprintOptimal.sekigo.flowUtils.commons import saveFlows,loadFlows\n",
    "from flowprintOptimal.sekigo.dataAnalysis.dataFrameProcessor import UTMobileNetProcessor\n",
    "from flowprintOptimal.sekigo.flowUtils.flowDatasets import DDQNActivityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BROWSERS    19779\n",
       "P2P         14315\n",
       "MAIL         4432\n",
       "OTHER        2558\n",
       "Skype         633\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows = loadFlows(path= \"data/unibs/unibs.json\", cls= PacketFlowRepressentation)\n",
    "flows = list(filter(lambda x : len(x) >= 20,flows))\n",
    "#print(pd.Series(map(lambda x : x.class_type,flows)).value_counts())\n",
    "#keep_class = set([\"firefox-bin \", \"Safari\", \"Mail\", \"Transmission\",\"amule\", \"Skype\",\"firefox\"])\n",
    "packet_flow_reps = flows\n",
    "#packet_flow_reps = list(filter(lambda x : x.class_type in keep_class, packet_flow_reps))\n",
    "def assignUNibsClass(class_type):\n",
    "    class_type = class_type.lower().strip()\n",
    "    if class_type in [\"amule\",\"transmission\", \"bittorrent.exe\"]:\n",
    "        return \"P2P\"\n",
    "    elif class_type in [\"mail\",\"thunderbird.exe\"]:\n",
    "        return \"MAIL\"\n",
    "    elif class_type in [\"skype\", \"skype\"]:\n",
    "        return \"Skype\"\n",
    "    elif class_type in [\"safari\", \"firefox-bin\", \"opera\",\"safari webpage p\", \"safari webpage\"]:\n",
    "        return \"BROWSERS\"\n",
    "    else:\n",
    "        return \"OTHER\"\n",
    "for packet_flow_rep in packet_flow_reps:\n",
    "    packet_flow_rep.class_type = assignUNibsClass(class_type= packet_flow_rep.class_type)\n",
    "\n",
    "pd.Series(map(lambda x : x.class_type,packet_flow_reps)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google-maps     3256\n",
       "netflix         2081\n",
       "reddit          1609\n",
       "facebook        1312\n",
       "youtube         1309\n",
       "pinterest       1233\n",
       "instagram       1222\n",
       "spotify          878\n",
       "google-drive     877\n",
       "twitter          847\n",
       "gmail            511\n",
       "hangout          426\n",
       "messenger        411\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows = loadFlows(path= \"data/UTMobileNet2021/mobilenetPacketRep.json\", cls= PacketFlowRepressentation)\n",
    "keep_class = set([\"facebook\",\"gmail\", \"google-drive\", \"google-maps\",\"hangout\",\"instagram\",\"messenger\",\"netflix\", \"pinterest\", \"reddit\", \"spotify\",\"twitter\", \"youtube\"])\n",
    "packet_flow_reps = flows\n",
    "packet_flow_reps = list(filter(lambda x : x.class_type in keep_class, packet_flow_reps))\n",
    "pd.Series(map(lambda x : x.class_type,packet_flow_reps)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_flow_reps = VNATDataFrameProcessor.getPacketFlows()\n",
    "packet_flow_reps = VNATDataFrameProcessor.convertLabelsToTopLevel(flows= packet_flow_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToFlowRep(packet_flow_rep,grain,band_thresholds):\n",
    "    inter_arrival_times = packet_flow_rep.inter_arrival_times\n",
    "    timestamps = [0]\n",
    "    C = np.log(900000)\n",
    "    base_time = datetime.datetime(year= 2023, month= 7, day= 31)\n",
    "    for i in range(1,len(inter_arrival_times)):\n",
    "        this_timestamp = timestamps[i-1] + np.exp(inter_arrival_times[i]*C) - 1\n",
    "        timestamps.append(this_timestamp)\n",
    "\n",
    "    for i in range(len(timestamps)):\n",
    "        timestamps[i] = base_time + datetime.timedelta(microseconds= float(np.round(timestamps[i])))\n",
    "\n",
    "\n",
    "    aggregated =  VNATDataFrameProcessor.aggregateByTimeBins(grain= datetime.timedelta(seconds= grain),normalized_timestamps= timestamps,direction= packet_flow_rep.directions,\n",
    "                                               packet_sizes= packet_flow_rep.lengths,band_thresholds= band_thresholds)\n",
    "\n",
    "    keys = list(aggregated.keys())\n",
    "    keys.sort()\n",
    "    sorted_values = [aggregated[i] for i in keys]\n",
    "    up_bytes,down_bytes,up_packets,down_packets = [],[],[],[]\n",
    "\n",
    "    for values in sorted_values:\n",
    "        up_packets.append(values[\"up_packets\"])\n",
    "        down_packets.append(values[\"down_packets\"])\n",
    "        up_bytes.append(values[\"up_bytes\"])\n",
    "        down_bytes.append(values[\"down_bytes\"])\n",
    "\n",
    "\n",
    "    up_bytes,down_bytes,up_packets,down_packets = np.array(up_bytes).T,np.array(down_bytes).T,np.array(up_packets).T,np.array(down_packets).T\n",
    "    return FlowRepresentation(up_bytes= up_bytes, down_bytes= down_bytes, up_packets= up_packets,down_packets= down_packets,\n",
    "                              class_type= packet_flow_rep.class_type,flow_config= FlowConfig(grain=grain, band_thresholds= band_thresholds)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41717/41717 [12:53<00:00, 53.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "timeslot_flow_reps = []\n",
    "for i in tqdm(range(len(packet_flow_reps))):\n",
    "    timeslot_rep  = convertToFlowRep(packet_flow_rep= packet_flow_reps[i],grain= .5, band_thresholds= [1250])\n",
    "    if len(timeslot_rep) >= 20:\n",
    "        timeslot_flow_reps.append(timeslot_rep.getSubFlow(start_index= 0, length= 20))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33711/33711 [05:23<00:00, 104.34it/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeslot_flow_reps = VNATDataFrameProcessor.convertToFlowRepresentations(flow_config= FlowConfig(grain= .5,band_thresholds= [1250]))\n",
    "timeslot_flow_reps = list(filter(lambda x : len(x) >= 20, timeslot_flow_reps))\n",
    "timeslot_flow_reps = list(map(lambda x : x.getSubFlow(0,20),timeslot_flow_reps))\n",
    "timeslot_flow_reps =  VNATDataFrameProcessor.convertLabelsToTopLevel(flows= timeslot_flow_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P2P         14112\n",
       "BROWSERS    13976\n",
       "OTHER        1845\n",
       "Skype         615\n",
       "MAIL          355\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(map(lambda x : x.class_type,timeslot_flow_reps)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x : x.class_type,timeslot_flow_reps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flows,test_flows,train_labels,test_labels = train_test_split(timeslot_flow_reps,labels,test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BROWSERS    2814\n",
       "P2P         2788\n",
       "OTHER        369\n",
       "Skype        130\n",
       "MAIL          80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DDQNActivityDataset(flows= train_flows,label_to_index= None)\n",
    "test_dataset = DDQNActivityDataset(flows= test_flows,label_to_index= train_dataset.label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowprintOptimal.sekigo.modeling.trainers import NNClassificationTrainer\n",
    "from flowprintOptimal.sekigo.modeling.neuralNetworks import LSTMNetwork,TransformerGenerator,CNNNetwork1D\n",
    "from flowprintOptimal.sekigo.modeling.loggers import Logger\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification ---- 1 metric test_f1 = 0.08267716535433071\n",
      "classification ---- 1 metric test_accuracy = 0.19811320754716982\n",
      "classification ---- 1 metric train_accuracy = 0.17621293800539084\n",
      "classification ---- 1 metric train_f1 = 0.07490690346605557\n",
      "classification ---- 10 metric train_loss = 1.383901059627533\n",
      "classification ---- 20 metric train_loss = 1.3735269069671632\n",
      "classification ---- 30 metric train_loss = 1.3618366599082947\n",
      "classification ---- 40 metric train_loss = 1.3293136477470398\n",
      "classification ---- 50 metric train_loss = 1.2505483984947205\n",
      "classification ---- 60 metric train_loss = 1.0707031071186066\n",
      "classification ---- 70 metric train_loss = 0.915131402015686\n",
      "classification ---- 80 metric train_loss = 0.8499157965183258\n",
      "classification ---- 90 metric train_loss = 0.7824710369110107\n",
      "classification ---- 100 metric train_loss = 0.8162100732326507\n",
      "classification ---- 110 metric train_loss = 0.7767880976200103\n",
      "classification ---- 120 metric train_loss = 0.7795724689960479\n",
      "classification ---- 130 metric train_loss = 0.7094115614891052\n",
      "classification ---- 140 metric train_loss = 0.7568125665187836\n",
      "classification ---- 150 metric train_loss = 0.659823191165924\n",
      "classification ---- 160 metric train_loss = 0.6996348321437835\n",
      "classification ---- 170 metric train_loss = 0.6365737676620483\n",
      "classification ---- 180 metric train_loss = 0.6854663729667664\n",
      "classification ---- 190 metric train_loss = 0.6317835837602616\n",
      "classification ---- 200 metric train_loss = 0.6730222940444947\n",
      "classification ---- 210 metric train_loss = 0.6420901715755463\n",
      "classification ---- 220 metric train_loss = 0.616298571228981\n",
      "classification ---- 230 metric train_loss = 0.5331831067800522\n",
      "classification ---- 240 metric train_loss = 0.6210889756679535\n",
      "classification ---- 250 metric train_loss = 0.5643826127052307\n",
      "classification ---- 260 metric train_loss = 0.5135719865560532\n",
      "classification ---- 270 metric train_loss = 0.5641878247261047\n",
      "classification ---- 280 metric train_loss = 0.5195584982633591\n",
      "classification ---- 290 metric train_loss = 0.5668028086423874\n",
      "classification ---- 300 metric train_loss = 0.5459739297628403\n",
      "classification ---- 310 metric train_loss = 0.47894173562526704\n",
      "classification ---- 320 metric train_loss = 0.522611427307129\n",
      "classification ---- 330 metric train_loss = 0.4822969913482666\n",
      "classification ---- 340 metric train_loss = 0.5101387351751328\n",
      "classification ---- 350 metric train_loss = 0.5599984705448151\n",
      "classification ---- 360 metric train_loss = 0.4798864334821701\n",
      "classification ---- 370 metric train_loss = 0.4925624251365662\n",
      "classification ---- 380 metric train_loss = 0.4668756574392319\n",
      "classification ---- 390 metric train_loss = 0.46765952110290526\n",
      "classification ---- 400 metric train_loss = 0.45333366096019745\n",
      "classification ---- 410 metric train_loss = 0.47156224846839906\n",
      "classification ---- 420 metric train_loss = 0.4835847049951553\n",
      "classification ---- 430 metric train_loss = 0.48971215486526487\n",
      "classification ---- 440 metric train_loss = 0.4460922658443451\n",
      "classification ---- 450 metric train_loss = 0.46406101882457734\n",
      "classification ---- 460 metric train_loss = 0.46819810271263124\n",
      "classification ---- 470 metric train_loss = 0.4712673634290695\n",
      "classification ---- 480 metric train_loss = 0.4526093930006027\n",
      "classification ---- 490 metric train_loss = 0.4539039969444275\n",
      "classification ---- 500 metric train_loss = 0.4722164571285248\n",
      "classification ---- 2 metric test_f1 = 0.6853363922579729\n",
      "classification ---- 2 metric test_accuracy = 0.683288409703504\n",
      "classification ---- 510 metric train_loss = 0.4871309369802475\n",
      "classification ---- 520 metric train_loss = 0.44801308810710905\n",
      "classification ---- 530 metric train_loss = 0.4753386825323105\n",
      "classification ---- 540 metric train_loss = 0.44687074422836304\n",
      "classification ---- 550 metric train_loss = 0.47830451726913453\n",
      "classification ---- 560 metric train_loss = 0.4478527158498764\n",
      "classification ---- 570 metric train_loss = 0.5112949579954147\n",
      "classification ---- 580 metric train_loss = 0.4381822645664215\n",
      "classification ---- 590 metric train_loss = 0.48227812349796295\n",
      "classification ---- 600 metric train_loss = 0.4854352712631226\n",
      "classification ---- 610 metric train_loss = 0.4892684668302536\n",
      "classification ---- 620 metric train_loss = 0.45813347697257994\n",
      "classification ---- 630 metric train_loss = 0.4182140678167343\n",
      "classification ---- 640 metric train_loss = 0.416653174161911\n",
      "classification ---- 650 metric train_loss = 0.4132326066493988\n",
      "classification ---- 660 metric train_loss = 0.40025308430194856\n",
      "classification ---- 670 metric train_loss = 0.4411507219076157\n",
      "classification ---- 680 metric train_loss = 0.4406383246183395\n",
      "classification ---- 690 metric train_loss = 0.4313871204853058\n",
      "classification ---- 700 metric train_loss = 0.4962062567472458\n",
      "classification ---- 710 metric train_loss = 0.47112767994403837\n",
      "classification ---- 720 metric train_loss = 0.4026478886604309\n",
      "classification ---- 730 metric train_loss = 0.38341168463230135\n",
      "classification ---- 740 metric train_loss = 0.4409357011318207\n",
      "classification ---- 750 metric train_loss = 0.40783335268497467\n",
      "classification ---- 760 metric train_loss = 0.3991371601819992\n",
      "classification ---- 770 metric train_loss = 0.4149434432387352\n",
      "classification ---- 780 metric train_loss = 0.40868252515792847\n",
      "classification ---- 790 metric train_loss = 0.41250931918621064\n",
      "classification ---- 800 metric train_loss = 0.3840980976819992\n",
      "classification ---- 810 metric train_loss = 0.39652627408504487\n",
      "classification ---- 820 metric train_loss = 0.555839654803276\n",
      "classification ---- 830 metric train_loss = 0.4573018729686737\n",
      "classification ---- 840 metric train_loss = 0.46055839359760287\n",
      "classification ---- 850 metric train_loss = 0.42036312222480776\n",
      "classification ---- 860 metric train_loss = 0.4100762516260147\n",
      "classification ---- 870 metric train_loss = 0.3555121660232544\n",
      "classification ---- 880 metric train_loss = 0.4090575873851776\n",
      "classification ---- 890 metric train_loss = 0.41417517960071565\n",
      "classification ---- 900 metric train_loss = 0.4512330800294876\n",
      "classification ---- 910 metric train_loss = 0.44208332896232605\n",
      "classification ---- 920 metric train_loss = 0.41805255115032197\n",
      "classification ---- 930 metric train_loss = 0.4145506531000137\n",
      "classification ---- 940 metric train_loss = 0.3895630642771721\n",
      "classification ---- 950 metric train_loss = 0.41436362862586973\n",
      "classification ---- 960 metric train_loss = 0.3968950942158699\n",
      "classification ---- 970 metric train_loss = 0.4129547864198685\n",
      "classification ---- 980 metric train_loss = 0.3921332836151123\n",
      "classification ---- 990 metric train_loss = 0.38445194363594054\n",
      "classification ---- 1000 metric train_loss = 0.39858749210834504\n",
      "classification ---- 3 metric test_f1 = 0.7922064004438254\n",
      "classification ---- 3 metric test_accuracy = 0.8315363881401617\n",
      "classification ---- 2 metric train_accuracy = 0.8736522911051213\n",
      "classification ---- 2 metric train_f1 = 0.8360253127186226\n",
      "classification ---- 1010 metric train_loss = 0.4214619129896164\n",
      "classification ---- 1020 metric train_loss = 0.36751642525196077\n",
      "classification ---- 1030 metric train_loss = 0.3909950375556946\n",
      "classification ---- 1040 metric train_loss = 0.41266010999679564\n",
      "classification ---- 1050 metric train_loss = 0.3871538817882538\n",
      "classification ---- 1060 metric train_loss = 0.39058530926704405\n",
      "classification ---- 1070 metric train_loss = 0.4024179995059967\n",
      "classification ---- 1080 metric train_loss = 0.43359856903553007\n",
      "classification ---- 1090 metric train_loss = 0.5086017578840256\n",
      "classification ---- 1100 metric train_loss = 0.48061963319778445\n",
      "classification ---- 1110 metric train_loss = 0.4425757139921188\n",
      "classification ---- 1120 metric train_loss = 0.40869538187980653\n",
      "classification ---- 1130 metric train_loss = 0.3892092615365982\n",
      "classification ---- 1140 metric train_loss = 0.3705836683511734\n",
      "classification ---- 1150 metric train_loss = 0.39546370804309844\n",
      "classification ---- 1160 metric train_loss = 0.39909714460372925\n",
      "classification ---- 1170 metric train_loss = 0.4206488996744156\n",
      "classification ---- 1180 metric train_loss = 0.42855022847652435\n",
      "classification ---- 1190 metric train_loss = 0.3732427418231964\n",
      "classification ---- 1200 metric train_loss = 0.3645128145813942\n",
      "classification ---- 1210 metric train_loss = 0.3813768446445465\n",
      "classification ---- 1220 metric train_loss = 0.386675825715065\n",
      "classification ---- 1230 metric train_loss = 0.35564095079898833\n",
      "classification ---- 1240 metric train_loss = 0.4337840676307678\n",
      "classification ---- 1250 metric train_loss = 0.4331098794937134\n",
      "classification ---- 1260 metric train_loss = 0.3990288734436035\n",
      "classification ---- 1270 metric train_loss = 0.3783131778240204\n",
      "classification ---- 1280 metric train_loss = 0.383743542432785\n",
      "classification ---- 1290 metric train_loss = 0.344142909348011\n",
      "classification ---- 1300 metric train_loss = 0.36124024987220765\n",
      "classification ---- 1310 metric train_loss = 0.309739950299263\n",
      "classification ---- 1320 metric train_loss = 0.3778460443019867\n",
      "classification ---- 1330 metric train_loss = 0.38692506551742556\n",
      "classification ---- 1340 metric train_loss = 0.3465984582901001\n",
      "classification ---- 1350 metric train_loss = 0.4253474548459053\n",
      "classification ---- 1360 metric train_loss = 0.4224116176366806\n",
      "classification ---- 1370 metric train_loss = 0.4276663690805435\n",
      "classification ---- 1380 metric train_loss = 0.3795508980751038\n",
      "classification ---- 1390 metric train_loss = 0.37517398595809937\n",
      "classification ---- 1400 metric train_loss = 0.4073782205581665\n",
      "classification ---- 1410 metric train_loss = 0.396472293138504\n",
      "classification ---- 1420 metric train_loss = 0.4301196813583374\n",
      "classification ---- 1430 metric train_loss = 0.4064996033906937\n",
      "classification ---- 1440 metric train_loss = 0.3745009571313858\n",
      "classification ---- 1450 metric train_loss = 0.3872536957263947\n",
      "classification ---- 1460 metric train_loss = 0.38389932811260224\n",
      "classification ---- 1470 metric train_loss = 0.37220093607902527\n",
      "classification ---- 1480 metric train_loss = 0.42453243434429166\n",
      "classification ---- 1490 metric train_loss = 0.35877723097801206\n",
      "classification ---- 1500 metric train_loss = 0.3918769657611847\n",
      "classification ---- 4 metric test_f1 = 0.7062028371424677\n",
      "classification ---- 4 metric test_accuracy = 0.6657681940700808\n",
      "classification ---- 1510 metric train_loss = 0.4146618217229843\n",
      "classification ---- 1520 metric train_loss = 0.41129123866558076\n",
      "classification ---- 1530 metric train_loss = 0.37262709736824035\n",
      "classification ---- 1540 metric train_loss = 0.3504091501235962\n",
      "classification ---- 1550 metric train_loss = 0.4188084006309509\n",
      "classification ---- 1560 metric train_loss = 0.38710553348064425\n",
      "classification ---- 1570 metric train_loss = 0.4163974612951279\n",
      "classification ---- 1580 metric train_loss = 0.42300370931625364\n",
      "classification ---- 1590 metric train_loss = 0.3909037530422211\n",
      "classification ---- 1600 metric train_loss = 0.38661273419857023\n",
      "classification ---- 1610 metric train_loss = 0.36737880706787107\n",
      "classification ---- 1620 metric train_loss = 0.35447146743535995\n",
      "classification ---- 1630 metric train_loss = 0.3531130850315094\n",
      "classification ---- 1640 metric train_loss = 0.3661191135644913\n",
      "classification ---- 1650 metric train_loss = 0.3850323408842087\n",
      "classification ---- 1660 metric train_loss = 0.352765417098999\n",
      "classification ---- 1670 metric train_loss = 0.4315862894058228\n",
      "classification ---- 1680 metric train_loss = 0.3871643662452698\n",
      "classification ---- 1690 metric train_loss = 0.3677878320217133\n",
      "classification ---- 1700 metric train_loss = 0.37472696602344513\n",
      "classification ---- 1710 metric train_loss = 0.42299793660640717\n",
      "classification ---- 1720 metric train_loss = 0.37702611088752747\n",
      "classification ---- 1730 metric train_loss = 0.3927426367998123\n",
      "classification ---- 1740 metric train_loss = 0.3953469067811966\n",
      "classification ---- 1750 metric train_loss = 0.37602419257164\n",
      "classification ---- 1760 metric train_loss = 0.3709973394870758\n",
      "classification ---- 1770 metric train_loss = 0.3702012121677399\n",
      "classification ---- 1780 metric train_loss = 0.37172519564628603\n",
      "classification ---- 1790 metric train_loss = 0.3730503708124161\n",
      "classification ---- 1800 metric train_loss = 0.38712337911128997\n",
      "classification ---- 1810 metric train_loss = 0.3641342282295227\n",
      "classification ---- 1820 metric train_loss = 0.3784861385822296\n",
      "classification ---- 1830 metric train_loss = 0.42013428211212156\n",
      "classification ---- 1840 metric train_loss = 0.38090469837188723\n",
      "classification ---- 1850 metric train_loss = 0.3939227670431137\n",
      "classification ---- 1860 metric train_loss = 0.33797697722911835\n",
      "classification ---- 1870 metric train_loss = 0.4183383047580719\n",
      "classification ---- 1880 metric train_loss = 0.45900396406650545\n",
      "classification ---- 1890 metric train_loss = 0.3819220870733261\n",
      "classification ---- 1900 metric train_loss = 0.35791917741298673\n",
      "classification ---- 1910 metric train_loss = 0.3566588282585144\n",
      "classification ---- 1920 metric train_loss = 0.3473497837781906\n",
      "classification ---- 1930 metric train_loss = 0.3541015923023224\n",
      "classification ---- 1940 metric train_loss = 0.3695258259773254\n",
      "classification ---- 1950 metric train_loss = 0.3522102743387222\n",
      "classification ---- 1960 metric train_loss = 0.3559395283460617\n",
      "classification ---- 1970 metric train_loss = 0.3174644231796265\n",
      "classification ---- 1980 metric train_loss = 0.3687994062900543\n",
      "classification ---- 1990 metric train_loss = 0.2854099690914154\n",
      "classification ---- 2000 metric train_loss = 0.3211281955242157\n",
      "classification ---- 5 metric test_f1 = 0.8107811306340718\n",
      "classification ---- 5 metric test_accuracy = 0.7964959568733153\n",
      "classification ---- 3 metric train_accuracy = 0.8200808625336927\n",
      "classification ---- 3 metric train_f1 = 0.833122349470671\n",
      "classification ---- 2010 metric train_loss = 0.35581308901309966\n",
      "classification ---- 2020 metric train_loss = 0.3176308423280716\n",
      "classification ---- 2030 metric train_loss = 0.40590283274650574\n",
      "classification ---- 2040 metric train_loss = 0.3700419425964355\n",
      "classification ---- 2050 metric train_loss = 0.37350688874721527\n",
      "classification ---- 2060 metric train_loss = 0.36713729202747347\n",
      "classification ---- 2070 metric train_loss = 0.34944878369569776\n",
      "classification ---- 2080 metric train_loss = 0.3553983956575394\n",
      "classification ---- 2090 metric train_loss = 0.3487627372145653\n",
      "classification ---- 2100 metric train_loss = 0.34074944704771043\n",
      "classification ---- 2110 metric train_loss = 0.3504524141550064\n",
      "classification ---- 2120 metric train_loss = 0.38251899778842924\n",
      "classification ---- 2130 metric train_loss = 0.3609290987253189\n",
      "classification ---- 2140 metric train_loss = 0.3238913148641586\n",
      "classification ---- 2150 metric train_loss = 0.3382424160838127\n",
      "classification ---- 2160 metric train_loss = 0.33625018447637556\n",
      "classification ---- 2170 metric train_loss = 0.3093767732381821\n",
      "classification ---- 2180 metric train_loss = 0.3703926980495453\n",
      "classification ---- 2190 metric train_loss = 0.3447980314493179\n",
      "classification ---- 2200 metric train_loss = 0.3691960424184799\n",
      "classification ---- 2210 metric train_loss = 0.38238186538219454\n",
      "classification ---- 2220 metric train_loss = 0.41187626123428345\n",
      "classification ---- 2230 metric train_loss = 0.4103831350803375\n",
      "classification ---- 2240 metric train_loss = 0.4287498205900192\n",
      "classification ---- 2250 metric train_loss = 0.37551704347133635\n",
      "classification ---- 2260 metric train_loss = 0.32593445032835006\n",
      "classification ---- 2270 metric train_loss = 0.36005475372076035\n",
      "classification ---- 2280 metric train_loss = 0.3189040690660477\n",
      "classification ---- 2290 metric train_loss = 0.35717365592718125\n",
      "classification ---- 2300 metric train_loss = 0.4304519951343536\n",
      "classification ---- 2310 metric train_loss = 0.35094662606716154\n",
      "classification ---- 2320 metric train_loss = 0.37590014934539795\n",
      "classification ---- 2330 metric train_loss = 0.3265319958329201\n",
      "classification ---- 2340 metric train_loss = 0.43553346991539\n",
      "classification ---- 2350 metric train_loss = 0.33249770998954775\n",
      "classification ---- 2360 metric train_loss = 0.3376355811953545\n",
      "classification ---- 2370 metric train_loss = 0.38868856728076934\n",
      "classification ---- 2380 metric train_loss = 0.3549185261130333\n",
      "classification ---- 2390 metric train_loss = 0.39286965429782866\n",
      "classification ---- 2400 metric train_loss = 0.36387188732624054\n",
      "classification ---- 2410 metric train_loss = 0.38308586478233336\n",
      "classification ---- 2420 metric train_loss = 0.3644883632659912\n",
      "classification ---- 2430 metric train_loss = 0.37358906865119934\n",
      "classification ---- 2440 metric train_loss = 0.3429305613040924\n",
      "classification ---- 2450 metric train_loss = 0.3096818834543228\n",
      "classification ---- 2460 metric train_loss = 0.3022269383072853\n",
      "classification ---- 2470 metric train_loss = 0.290682826936245\n",
      "classification ---- 2480 metric train_loss = 0.3114157736301422\n",
      "classification ---- 2490 metric train_loss = 0.39529591500759126\n",
      "classification ---- 2500 metric train_loss = 0.35399841219186784\n",
      "classification ---- 6 metric test_f1 = 0.801762107000058\n",
      "classification ---- 6 metric test_accuracy = 0.7789757412398922\n",
      "classification ---- 2510 metric train_loss = 0.3610492706298828\n",
      "classification ---- 2520 metric train_loss = 0.3254732504487038\n",
      "classification ---- 2530 metric train_loss = 0.3324451372027397\n",
      "classification ---- 2540 metric train_loss = 0.37642580568790435\n",
      "classification ---- 2550 metric train_loss = 0.3675405472517014\n",
      "classification ---- 2560 metric train_loss = 0.328747895359993\n",
      "classification ---- 2570 metric train_loss = 0.364670430123806\n",
      "classification ---- 2580 metric train_loss = 0.3721314072608948\n",
      "classification ---- 2590 metric train_loss = 0.351254104077816\n",
      "classification ---- 2600 metric train_loss = 0.3514420479536057\n",
      "classification ---- 2610 metric train_loss = 0.3135083496570587\n",
      "classification ---- 2620 metric train_loss = 0.29134356528520583\n",
      "classification ---- 2630 metric train_loss = 0.28201202899217603\n",
      "classification ---- 2640 metric train_loss = 0.24585967808961867\n",
      "classification ---- 2650 metric train_loss = 0.39595822989940643\n",
      "classification ---- 2660 metric train_loss = 0.37314516603946685\n",
      "classification ---- 2670 metric train_loss = 0.28704700618982315\n",
      "classification ---- 2680 metric train_loss = 0.34499482810497284\n",
      "classification ---- 2690 metric train_loss = 0.33667331486940383\n",
      "classification ---- 2700 metric train_loss = 0.36529378294944764\n",
      "classification ---- 2710 metric train_loss = 0.3726540744304657\n",
      "classification ---- 2720 metric train_loss = 0.34312525689601897\n",
      "classification ---- 2730 metric train_loss = 0.3541934907436371\n",
      "classification ---- 2740 metric train_loss = 0.34278923869132993\n",
      "classification ---- 2750 metric train_loss = 0.3216665729880333\n",
      "classification ---- 2760 metric train_loss = 0.3835953027009964\n",
      "classification ---- 2770 metric train_loss = 0.37079236209392546\n",
      "classification ---- 2780 metric train_loss = 0.378830149769783\n",
      "classification ---- 2790 metric train_loss = 0.3172809317708015\n",
      "classification ---- 2800 metric train_loss = 0.34254178404808044\n",
      "classification ---- 2810 metric train_loss = 0.3598867952823639\n",
      "classification ---- 2820 metric train_loss = 0.34576043486595154\n",
      "classification ---- 2830 metric train_loss = 0.3469888925552368\n",
      "classification ---- 2840 metric train_loss = 0.3619983494281769\n",
      "classification ---- 2850 metric train_loss = 0.3662860065698624\n",
      "classification ---- 2860 metric train_loss = 0.36383007764816283\n",
      "classification ---- 2870 metric train_loss = 0.37284332811832427\n",
      "classification ---- 2880 metric train_loss = 0.33318640291690826\n",
      "classification ---- 2890 metric train_loss = 0.3433545231819153\n",
      "classification ---- 2900 metric train_loss = 0.35869184136390686\n",
      "classification ---- 2910 metric train_loss = 0.3376958131790161\n",
      "classification ---- 2920 metric train_loss = 0.39675825238227846\n",
      "classification ---- 2930 metric train_loss = 0.34738429486751554\n",
      "classification ---- 2940 metric train_loss = 0.33515005111694335\n",
      "classification ---- 2950 metric train_loss = 0.3607964962720871\n",
      "classification ---- 2960 metric train_loss = 0.3501445114612579\n",
      "classification ---- 2970 metric train_loss = 0.3668907880783081\n",
      "classification ---- 2980 metric train_loss = 0.3440337359905243\n",
      "classification ---- 2990 metric train_loss = 0.35630808472633363\n",
      "classification ---- 3000 metric train_loss = 0.35626479983329773\n",
      "classification ---- 7 metric test_f1 = 0.7184503274658912\n",
      "classification ---- 7 metric test_accuracy = 0.6752021563342318\n",
      "classification ---- 4 metric train_accuracy = 0.6856469002695418\n",
      "classification ---- 4 metric train_f1 = 0.7388428111012787\n",
      "classification ---- 3010 metric train_loss = 0.3892363697290421\n",
      "classification ---- 3020 metric train_loss = 0.3781533569097519\n",
      "classification ---- 3030 metric train_loss = 0.3456858515739441\n",
      "classification ---- 3040 metric train_loss = 0.3247608199715614\n",
      "classification ---- 3050 metric train_loss = 0.32884239703416823\n",
      "classification ---- 3060 metric train_loss = 0.3347273454070091\n",
      "classification ---- 3070 metric train_loss = 0.38887990415096285\n",
      "classification ---- 3080 metric train_loss = 0.34755156338214876\n",
      "classification ---- 3090 metric train_loss = 0.3486963450908661\n",
      "classification ---- 3100 metric train_loss = 0.364779195189476\n",
      "classification ---- 3110 metric train_loss = 0.32675566971302034\n",
      "classification ---- 3120 metric train_loss = 0.359128525853157\n",
      "classification ---- 3130 metric train_loss = 0.3544718027114868\n",
      "classification ---- 3140 metric train_loss = 0.33639286160469056\n",
      "classification ---- 3150 metric train_loss = 0.3124007686972618\n",
      "classification ---- 3160 metric train_loss = 0.35876971781253814\n",
      "classification ---- 3170 metric train_loss = 0.28496616184711454\n",
      "classification ---- 3180 metric train_loss = 0.3471026480197906\n",
      "classification ---- 3190 metric train_loss = 0.38068501502275465\n",
      "classification ---- 3200 metric train_loss = 0.33366829454898833\n",
      "classification ---- 3210 metric train_loss = 0.3191059783101082\n",
      "classification ---- 3220 metric train_loss = 0.3328411906957626\n",
      "classification ---- 3230 metric train_loss = 0.33070219308137894\n",
      "classification ---- 3240 metric train_loss = 0.3298059433698654\n",
      "classification ---- 3250 metric train_loss = 0.35325699150562284\n",
      "classification ---- 3260 metric train_loss = 0.3446710675954819\n",
      "classification ---- 3270 metric train_loss = 0.3424324721097946\n",
      "classification ---- 3280 metric train_loss = 0.3250924915075302\n",
      "classification ---- 3290 metric train_loss = 0.3395255535840988\n",
      "classification ---- 3300 metric train_loss = 0.3444150000810623\n",
      "classification ---- 3310 metric train_loss = 0.3417427569627762\n",
      "classification ---- 3320 metric train_loss = 0.3668693721294403\n",
      "classification ---- 3330 metric train_loss = 0.3691841274499893\n",
      "classification ---- 3340 metric train_loss = 0.32640292048454284\n",
      "classification ---- 3350 metric train_loss = 0.32942417711019517\n",
      "classification ---- 3360 metric train_loss = 0.3585441917181015\n",
      "classification ---- 3370 metric train_loss = 0.33294476717710497\n",
      "classification ---- 3380 metric train_loss = 0.35501441955566404\n",
      "classification ---- 3390 metric train_loss = 0.3291549555957317\n",
      "classification ---- 3400 metric train_loss = 0.32798662781715393\n",
      "classification ---- 3410 metric train_loss = 0.3323351085186005\n",
      "classification ---- 3420 metric train_loss = 0.31584616005420685\n",
      "classification ---- 3430 metric train_loss = 0.35702609121799467\n",
      "classification ---- 3440 metric train_loss = 0.34708061814308167\n",
      "classification ---- 3450 metric train_loss = 0.3467108368873596\n",
      "classification ---- 3460 metric train_loss = 0.33350677192211153\n",
      "classification ---- 3470 metric train_loss = 0.3639554247260094\n",
      "classification ---- 3480 metric train_loss = 0.30012520402669907\n",
      "classification ---- 3490 metric train_loss = 0.3158893048763275\n",
      "classification ---- 3500 metric train_loss = 0.35912095606327055\n",
      "classification ---- 8 metric test_f1 = 0.7150883423173605\n",
      "classification ---- 8 metric test_accuracy = 0.6725067385444744\n",
      "classification ---- 3510 metric train_loss = 0.35292077362537383\n",
      "classification ---- 3520 metric train_loss = 0.3268901228904724\n",
      "classification ---- 3530 metric train_loss = 0.31133169680833817\n",
      "classification ---- 3540 metric train_loss = 0.33265799283981323\n",
      "classification ---- 3550 metric train_loss = 0.29858933985233305\n",
      "classification ---- 3560 metric train_loss = 0.3122894078493118\n",
      "classification ---- 3570 metric train_loss = 0.30046881586313245\n",
      "classification ---- 3580 metric train_loss = 0.30043428391218185\n",
      "classification ---- 3590 metric train_loss = 0.30355473309755326\n",
      "classification ---- 3600 metric train_loss = 0.44477460980415345\n",
      "classification ---- 3610 metric train_loss = 0.37638275027275087\n",
      "classification ---- 3620 metric train_loss = 0.3468276679515839\n",
      "classification ---- 3630 metric train_loss = 0.3757511764764786\n",
      "classification ---- 3640 metric train_loss = 0.3564453572034836\n",
      "classification ---- 3650 metric train_loss = 0.3337601274251938\n",
      "classification ---- 3660 metric train_loss = 0.3568042486906052\n",
      "classification ---- 3670 metric train_loss = 0.338032653927803\n",
      "classification ---- 3680 metric train_loss = 0.3745764195919037\n",
      "classification ---- 3690 metric train_loss = 0.3809682786464691\n",
      "classification ---- 3700 metric train_loss = 0.3277702540159225\n",
      "classification ---- 3710 metric train_loss = 0.32928246557712554\n",
      "classification ---- 3720 metric train_loss = 0.36974682211875914\n",
      "classification ---- 3730 metric train_loss = 0.35488532334566114\n",
      "classification ---- 3740 metric train_loss = 0.3554950475692749\n",
      "classification ---- 3750 metric train_loss = 0.3396007716655731\n",
      "classification ---- 3760 metric train_loss = 0.3592169150710106\n",
      "classification ---- 3770 metric train_loss = 0.36388255953788756\n",
      "classification ---- 3780 metric train_loss = 0.33233375251293185\n",
      "classification ---- 3790 metric train_loss = 0.3537900775671005\n",
      "classification ---- 3800 metric train_loss = 0.2951688677072525\n",
      "classification ---- 3810 metric train_loss = 0.35717451572418213\n",
      "classification ---- 3820 metric train_loss = 0.340760263800621\n",
      "classification ---- 3830 metric train_loss = 0.3428633689880371\n",
      "classification ---- 3840 metric train_loss = 0.3620248526334763\n",
      "classification ---- 3850 metric train_loss = 0.36993075013160703\n",
      "classification ---- 3860 metric train_loss = 0.37958766222000123\n",
      "classification ---- 3870 metric train_loss = 0.32141534686088563\n",
      "classification ---- 3880 metric train_loss = 0.3202142402529716\n",
      "classification ---- 3890 metric train_loss = 0.3225007265806198\n",
      "classification ---- 3900 metric train_loss = 0.31553526520729064\n",
      "classification ---- 3910 metric train_loss = 0.3275224953889847\n",
      "classification ---- 3920 metric train_loss = 0.35704340040683746\n",
      "classification ---- 3930 metric train_loss = 0.3369054585695267\n",
      "classification ---- 3940 metric train_loss = 0.3288524359464645\n",
      "classification ---- 3950 metric train_loss = 0.3486997738480568\n",
      "classification ---- 3960 metric train_loss = 0.35786978006362913\n",
      "classification ---- 3970 metric train_loss = 0.31364417970180514\n",
      "classification ---- 3980 metric train_loss = 0.3707412302494049\n",
      "classification ---- 3990 metric train_loss = 0.3626630425453186\n",
      "classification ---- 4000 metric train_loss = 0.3533016249537468\n",
      "classification ---- 9 metric test_f1 = 0.7123622752478314\n",
      "classification ---- 9 metric test_accuracy = 0.6684636118598383\n",
      "classification ---- 5 metric train_accuracy = 0.6866576819407008\n",
      "classification ---- 5 metric train_f1 = 0.740140885553079\n",
      "classification ---- 4010 metric train_loss = 0.3033199802041054\n",
      "classification ---- 4020 metric train_loss = 0.330629563331604\n",
      "classification ---- 4030 metric train_loss = 0.32188590466976164\n",
      "classification ---- 4040 metric train_loss = 0.3402981132268906\n",
      "classification ---- 4050 metric train_loss = 0.31003663390874864\n",
      "classification ---- 4060 metric train_loss = 0.3162191420793533\n",
      "classification ---- 4070 metric train_loss = 0.3557432502508163\n",
      "classification ---- 4080 metric train_loss = 0.2855588778853416\n",
      "classification ---- 4090 metric train_loss = 0.3421850115060806\n",
      "classification ---- 4100 metric train_loss = 0.36099016666412354\n",
      "classification ---- 4110 metric train_loss = 0.3409337818622589\n",
      "classification ---- 4120 metric train_loss = 0.3402135640382767\n",
      "classification ---- 4130 metric train_loss = 0.35231977701187134\n",
      "classification ---- 4140 metric train_loss = 0.370687635242939\n",
      "classification ---- 4150 metric train_loss = 0.36770073473453524\n",
      "classification ---- 4160 metric train_loss = 0.38052005469799044\n",
      "classification ---- 4170 metric train_loss = 0.31890460550785066\n",
      "classification ---- 4180 metric train_loss = 0.32131301760673525\n",
      "classification ---- 4190 metric train_loss = 0.30547780096530913\n",
      "classification ---- 4200 metric train_loss = 0.31540040522813795\n",
      "classification ---- 4210 metric train_loss = 0.2894288063049316\n",
      "classification ---- 4220 metric train_loss = 0.3090553432703018\n",
      "classification ---- 4230 metric train_loss = 0.29040096402168275\n",
      "classification ---- 4240 metric train_loss = 0.3298108488321304\n",
      "classification ---- 4250 metric train_loss = 0.32790465354919435\n",
      "classification ---- 4260 metric train_loss = 0.3424603208899498\n",
      "classification ---- 4270 metric train_loss = 0.32077077925205233\n",
      "classification ---- 4280 metric train_loss = 0.27961933314800264\n",
      "classification ---- 4290 metric train_loss = 0.3127078667283058\n",
      "classification ---- 4300 metric train_loss = 0.3306210353970528\n",
      "classification ---- 4310 metric train_loss = 0.27787796407938004\n",
      "classification ---- 4320 metric train_loss = 0.27884787172079084\n",
      "classification ---- 4330 metric train_loss = 0.36078396886587144\n",
      "classification ---- 4340 metric train_loss = 0.2964920476078987\n",
      "classification ---- 4350 metric train_loss = 0.2751208275556564\n",
      "classification ---- 4360 metric train_loss = 0.2533605396747589\n",
      "classification ---- 4370 metric train_loss = 0.3467998668551445\n",
      "classification ---- 4380 metric train_loss = 0.301879009604454\n",
      "classification ---- 4390 metric train_loss = 0.34639188945293425\n",
      "classification ---- 4400 metric train_loss = 0.26750677078962326\n",
      "classification ---- 4410 metric train_loss = 0.23250236511230468\n",
      "classification ---- 4420 metric train_loss = 0.1987149141728878\n",
      "classification ---- 4430 metric train_loss = 0.47384206727147105\n",
      "classification ---- 4440 metric train_loss = 0.4685078769922256\n",
      "classification ---- 4450 metric train_loss = 0.430082967877388\n",
      "classification ---- 4460 metric train_loss = 0.3919570595026016\n",
      "classification ---- 4470 metric train_loss = 0.34969489723443986\n",
      "classification ---- 4480 metric train_loss = 0.3331053271889687\n",
      "classification ---- 4490 metric train_loss = 0.3429999500513077\n",
      "classification ---- 4500 metric train_loss = 0.3046939328312874\n",
      "classification ---- 10 metric test_f1 = 0.8276286172293069\n",
      "classification ---- 10 metric test_accuracy = 0.8126684636118598\n",
      "classification ---- 4510 metric train_loss = 0.2098781406879425\n",
      "classification ---- 4520 metric train_loss = 0.20100258737802507\n",
      "classification ---- 4530 metric train_loss = 0.2064227744936943\n",
      "classification ---- 4540 metric train_loss = 0.2087385430932045\n",
      "classification ---- 4550 metric train_loss = 0.1852220483124256\n",
      "classification ---- 4560 metric train_loss = 0.30785977765917777\n",
      "classification ---- 4570 metric train_loss = 0.5011054962873459\n",
      "classification ---- 4580 metric train_loss = 0.37489272356033326\n",
      "classification ---- 4590 metric train_loss = 0.3209979236125946\n",
      "classification ---- 4600 metric train_loss = 0.3274646490812302\n",
      "classification ---- 4610 metric train_loss = 0.34372825771570203\n",
      "classification ---- 4620 metric train_loss = 0.36029033958911894\n",
      "classification ---- 4630 metric train_loss = 0.3193094089627266\n",
      "classification ---- 4640 metric train_loss = 0.31947646141052244\n",
      "classification ---- 4650 metric train_loss = 0.30418548583984373\n",
      "classification ---- 4660 metric train_loss = 0.31834485232830045\n",
      "classification ---- 4670 metric train_loss = 0.3062046021223068\n",
      "classification ---- 4680 metric train_loss = 0.2632207810878754\n",
      "classification ---- 4690 metric train_loss = 0.16730587631464006\n",
      "classification ---- 4700 metric train_loss = 0.1485970512032509\n",
      "classification ---- 4710 metric train_loss = 0.15499646961688995\n",
      "classification ---- 4720 metric train_loss = 0.16919103637337685\n",
      "classification ---- 4730 metric train_loss = 0.14168702512979509\n",
      "classification ---- 4740 metric train_loss = 0.1543099120259285\n",
      "classification ---- 4750 metric train_loss = 0.13788393288850784\n",
      "classification ---- 4760 metric train_loss = 0.2987971119582653\n",
      "classification ---- 4770 metric train_loss = 0.2044893682003021\n",
      "classification ---- 4780 metric train_loss = 0.3376028150320053\n",
      "classification ---- 4790 metric train_loss = 0.18405988439917564\n",
      "classification ---- 4800 metric train_loss = 0.1562705598771572\n",
      "classification ---- 4810 metric train_loss = 0.16936544850468635\n",
      "classification ---- 4820 metric train_loss = 0.13253970444202423\n",
      "classification ---- 4830 metric train_loss = 0.17356487214565278\n",
      "classification ---- 4840 metric train_loss = 0.14023862406611443\n",
      "classification ---- 4850 metric train_loss = 0.13976527489721774\n",
      "classification ---- 4860 metric train_loss = 0.1402144145220518\n",
      "classification ---- 4870 metric train_loss = 0.11604091748595238\n",
      "classification ---- 4880 metric train_loss = 0.12265353873372078\n",
      "classification ---- 4890 metric train_loss = 0.1291925072669983\n",
      "classification ---- 4900 metric train_loss = 0.11767258644104003\n",
      "classification ---- 4910 metric train_loss = 0.11079368554055691\n",
      "classification ---- 4920 metric train_loss = 0.0999569695442915\n",
      "classification ---- 4930 metric train_loss = 0.12932369261980056\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39mdefault_step_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      5\u001b[0m classification_trainer \u001b[38;5;241m=\u001b[39m NNClassificationTrainer(classifier \u001b[38;5;241m=\u001b[39m classifier_,device\u001b[38;5;241m=\u001b[39m device,logger\u001b[38;5;241m=\u001b[39m logger)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mclassification_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_balanced_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/modeling/trainers.py:137\u001b[0m, in \u001b[0;36mNNClassificationTrainer.train\u001b[0;34m(self, train_dataset, test_dataset, epochs, batch_size, lr, use_balanced_sampler)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mclassifier_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclassifier_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    141\u001b[0m             test_f1  \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalcF1(test_dataset)\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/modeling/trainers.py:111\u001b[0m, in \u001b[0;36mNNClassificationTrainer.trainStep\u001b[0;34m(self, batch, classifier_optimizer)\u001b[0m\n\u001b[1;32m    109\u001b[0m classifier_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    110\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_loss(model_out,y)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 111\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m classifier_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maddMetric(metric_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier_ = CNNNetwork1D(in_channels= 6,num_filters= 64,output_dims= [len(train_dataset.label_to_index)],num_layers= 4)#LSTMNetwork(lstm_hidden_size= 64,lstm_input_size=6,output_dim = len(all_train_dataset.label_to_index))\n",
    "#classifier_ = LSTMNetwork(lstm_input_size= 3,lstm_hidden_size= 128,output_dim= len(train_dataset.label_to_index))\n",
    "logger= Logger(name= \"classification\",verbose= True)\n",
    "logger.default_step_size = 500\n",
    "classification_trainer = NNClassificationTrainer(classifier = classifier_,device= device,logger= logger)\n",
    "classification_trainer.train(train_dataset= train_dataset,test_dataset= test_dataset,epochs=200,batch_size= 64,lr= .0001,use_balanced_sampler= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowprintOptimal.sekigo.earlyClassification.DQL.core import MemoryElement,Rewarder,State\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.datasets import MemoryDataset\n",
    "from flowprintOptimal.sekigo.earlyClassification.DQL.trainers import EarlyClassificationtrainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewarder:\n",
    "    def __init__(self,max_length,l,num_labels : int):\n",
    "        self.max_length = max_length\n",
    "        self.l = l # l is smaller than 1\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def reward(self,state : State,action : int):\n",
    "        if state.label == action:\n",
    "            # reward 1 on a correct prediction\n",
    "            return 1, True\n",
    "        else:\n",
    "            # either incorrect or wait\n",
    "            # wait \n",
    "            # treat the wait action with a negative reward\n",
    "            if action == self.num_labels:\n",
    "                if state.length == self.max_length:\n",
    "                    # it is the last timestamp\n",
    "                    if state.label == -1:\n",
    "                        assert False\n",
    "                        return 0, True\n",
    "                    else:\n",
    "                        return -1,True#self.l*(state.length/self.max_length),True\n",
    "                else:\n",
    "                    if state.label == -1:\n",
    "                        assert False\n",
    "                        return 0, False\n",
    "                    else:\n",
    "                        return -self.l*(state.length/self.max_length), False\n",
    "            else:\n",
    "                # incorrect\n",
    "                return -1,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryFiller:\n",
    "    def __init__(self,dataset,rewarder : Rewarder,min_length : int,max_length):\n",
    "        self.dataset = dataset\n",
    "        self.rewarder = rewarder\n",
    "\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.actions = list(self.dataset.label_to_index.values())  \n",
    "        self.actions.append(len(self.actions))\n",
    "\n",
    "    def processSingleSample(self,data):\n",
    "        flow, label = data[\"data\"], data[\"label\"]\n",
    "        memory_elements : List[MemoryElement] = []\n",
    "        for length in range(self.min_length, self.max_length+1):\n",
    "            for action in self.actions:\n",
    "                state = State(timeseries= flow,label= label,length= length)\n",
    "                reward, terminate = self.rewarder.reward(state= state,action= action)\n",
    "                \n",
    "                next_state = State(timeseries= flow,label= label,length= length + 1)\n",
    "                if terminate == True:\n",
    "                    # I am reducing the length as I will ahve to pass the state to LSTM \n",
    "                    # So I instead of filtering I will just zero all terminal states later.\n",
    "                    next_state.length -= 1\n",
    "                    next_state.setTerminal()\n",
    "\n",
    "                \n",
    "                memory_element = MemoryElement(state= state,action= action,reward= reward,next_state= next_state)\n",
    "                memory_elements.append(memory_element)\n",
    "        return memory_elements\n",
    "    def processDataset(self):\n",
    "\n",
    "        memory_elements = []\n",
    "        \n",
    "        for i in range(1,len(self.dataset)+1):\n",
    "            data = self.dataset[i-1]\n",
    "            memory_elements.extend(self.processSingleSample(data))\n",
    " \n",
    "        return memory_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "rewarder = Rewarder(max_length= max_length,l= .1,num_labels= len(train_dataset.label_to_index))\n",
    "memory_filler = MemoryFiller(dataset= train_dataset,rewarder= rewarder, min_length= 5, max_length= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373312\n"
     ]
    }
   ],
   "source": [
    "memory = memory_filler.processDataset()\n",
    "print(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_dataset = MemoryDataset(memories= memory,num_classes= len(train_dataset.label_to_index), corrupt_prob= 0,\n",
    "                               min_length= memory_filler.min_length,max_length= memory_filler.max_length)\n",
    "predictor = LSTMNetwork(lstm_input_size= 6,lstm_hidden_size= 256,output_dim= len(train_dataset.label_to_index) + 1)\n",
    "logger = Logger(verbose= True)\n",
    "logger.default_step_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddq_model = EarlyClassificationtrainer(predictor= predictor,train_dataset = train_dataset,test_dataset= test_dataset,memory_dataset= memory_dataset,\n",
    "                                       ood_dataset= test_dataset,\n",
    "                                       logger= logger,device=device,model_replacement_steps= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- 1000 metric loss = 0.277944629073143\n",
      " ---- 1 metric test_eval_f1 = 0.8040770101925255\n",
      " ---- 1 metric test_eval_time = 7.1755379388448475\n",
      " ---- 1 metric ood_eval = 0.0\n",
      " ---- 1 metric ood_eval_time = 7.1755379388448475\n",
      " ---- 2000 metric loss = 0.21906021643429996\n",
      " ---- 2 metric test_eval_f1 = 0.8099013104675619\n",
      " ---- 2 metric test_eval_time = 13.022811842743893\n",
      " ---- 2 metric ood_eval = 0.0\n",
      " ---- 2 metric ood_eval_time = 13.022811842743893\n",
      " ---- 1 metric train_eval_f1 = 0.8132432651080009\n",
      " ---- 1 metric train_eval_time = 12.991869589839009\n",
      " ---- 3000 metric loss = 0.20469975847750901\n",
      " ---- 3 metric test_eval_f1 = 0.812651674486329\n",
      " ---- 3 metric test_eval_time = 6.179906164051125\n",
      " ---- 3 metric ood_eval = 0.0\n",
      " ---- 3 metric ood_eval_time = 6.179906164051125\n",
      " ---- 4000 metric loss = 0.19693738949671388\n",
      " ---- 4 metric test_eval_f1 = 0.8207409804238797\n",
      " ---- 4 metric test_eval_time = 6.45931079113412\n",
      " ---- 4 metric ood_eval = 0.0\n",
      " ---- 4 metric ood_eval_time = 6.45931079113412\n",
      " ---- 2 metric train_eval_f1 = 0.8220613218995227\n",
      " ---- 2 metric train_eval_time = 6.475446970309846\n",
      " ---- 5000 metric loss = 0.1818670976422727\n",
      " ---- 5 metric test_eval_f1 = 0.8354635172302216\n",
      " ---- 5 metric test_eval_time = 6.023458987218897\n",
      " ---- 5 metric ood_eval = 0.0\n",
      " ---- 5 metric ood_eval_time = 6.023458987218897\n",
      " ---- 6000 metric loss = 0.1767167294546962\n",
      " ---- 6 metric test_eval_f1 = 0.8430674648115192\n",
      " ---- 6 metric test_eval_time = 8.085261284581783\n",
      " ---- 6 metric ood_eval = 0.0\n",
      " ---- 6 metric ood_eval_time = 8.085261284581783\n",
      " ---- 3 metric train_eval_f1 = 0.8484750424722919\n",
      " ---- 3 metric train_eval_time = 8.076328775988998\n",
      " ---- 7000 metric loss = 0.1712617102228105\n",
      " ---- 7 metric test_eval_f1 = 0.8349781588739686\n",
      " ---- 7 metric test_eval_time = 8.991587121824947\n",
      " ---- 7 metric ood_eval = 0.0\n",
      " ---- 7 metric ood_eval_time = 8.991587121824947\n",
      " ---- 8000 metric loss = 0.16837939105182886\n",
      " ---- 8 metric test_eval_f1 = 0.8432292509302702\n",
      " ---- 8 metric test_eval_time = 6.50072803753438\n",
      " ---- 8 metric ood_eval = 0.0\n",
      " ---- 8 metric ood_eval_time = 6.50072803753438\n",
      " ---- 4 metric train_eval_f1 = 0.8487986408866597\n",
      " ---- 4 metric train_eval_time = 6.520669848717741\n",
      " ---- 9000 metric loss = 0.1601170729920268\n",
      " ---- 9 metric test_eval_f1 = 0.8461414010677883\n",
      " ---- 9 metric test_eval_time = 7.465782235884161\n",
      " ---- 9 metric ood_eval = 0.00016178611875101117\n",
      " ---- 9 metric ood_eval_time = 7.465782235884161\n",
      " ---- 10000 metric loss = 0.1540042235162109\n",
      " ---- 10 metric test_eval_f1 = 0.8607021517553793\n",
      " ---- 10 metric test_eval_time = 7.051933344119075\n",
      " ---- 10 metric ood_eval = 0.0\n",
      " ---- 10 metric ood_eval_time = 7.051933344119075\n",
      " ---- 5 metric train_eval_f1 = 0.8679313971361541\n",
      " ---- 5 metric train_eval_time = 7.0485397621551655\n",
      " ---- 11000 metric loss = 0.15095288385637104\n",
      " ---- 11 metric test_eval_f1 = 0.8619964407053875\n",
      " ---- 11 metric test_eval_time = 6.230868791457693\n",
      " ---- 11 metric ood_eval = 0.0\n",
      " ---- 11 metric ood_eval_time = 6.230868791457693\n",
      " ---- 12000 metric loss = 0.1476026679147035\n",
      " ---- 12 metric test_eval_f1 = 0.8645850186054037\n",
      " ---- 12 metric test_eval_time = 7.854554279242841\n",
      " ---- 12 metric ood_eval = 0.0\n",
      " ---- 12 metric ood_eval_time = 7.854554279242841\n",
      " ---- 6 metric train_eval_f1 = 0.8717336785049753\n",
      " ---- 6 metric train_eval_time = 7.844146913680123\n",
      " ---- 13000 metric loss = 0.14660604446567596\n",
      " ---- 13 metric test_eval_f1 = 0.8644232324866526\n",
      " ---- 13 metric test_eval_time = 6.983497815887397\n",
      " ---- 13 metric ood_eval = 0.0\n",
      " ---- 13 metric ood_eval_time = 6.983497815887397\n",
      " ---- 14000 metric loss = 0.14736750354245304\n",
      " ---- 14 metric test_eval_f1 = 0.8618346545866364\n",
      " ---- 14 metric test_eval_time = 6.352208380520952\n",
      " ---- 14 metric ood_eval = 0.0\n",
      " ---- 14 metric ood_eval_time = 6.352208380520952\n",
      " ---- 7 metric train_eval_f1 = 0.8694684896044009\n",
      " ---- 7 metric train_eval_time = 6.337553595987379\n",
      " ---- 15000 metric loss = 0.14906643818505108\n",
      " ---- 15 metric test_eval_f1 = 0.8644232324866526\n",
      " ---- 15 metric test_eval_time = 10.208865879307556\n",
      " ---- 15 metric ood_eval = 0.0\n",
      " ---- 15 metric ood_eval_time = 10.208865879307556\n",
      " ---- 16000 metric loss = 0.1442815144751221\n",
      " ---- 16 metric test_eval_f1 = 0.8666882381491668\n",
      " ---- 16 metric test_eval_time = 6.972981718168581\n",
      " ---- 16 metric ood_eval = 0.00016178611875101117\n",
      " ---- 16 metric ood_eval_time = 6.972981718168581\n",
      " ---- 8 metric train_eval_f1 = 0.8738370681983658\n",
      " ---- 8 metric train_eval_time = 6.923549874605614\n",
      " ---- 17000 metric loss = 0.1450339695364237\n",
      " ---- 17 metric test_eval_f1 = 0.8632907296553956\n",
      " ---- 17 metric test_eval_time = 6.400744216146254\n",
      " ---- 17 metric ood_eval = 0.0\n",
      " ---- 17 metric ood_eval_time = 6.400744216146254\n",
      " ---- 18000 metric loss = 0.14464859930612148\n",
      " ---- 18 metric test_eval_f1 = 0.8670118103866689\n",
      " ---- 18 metric test_eval_time = 7.087688076363048\n",
      " ---- 18 metric ood_eval = 0.0\n",
      " ---- 18 metric ood_eval_time = 7.087688076363048\n",
      " ---- 9 metric train_eval_f1 = 0.8745247148288974\n",
      " ---- 9 metric train_eval_time = 7.07018040611601\n",
      " ---- 19000 metric loss = 0.1418442063704133\n",
      " ---- 19 metric test_eval_f1 = 0.8603785795178773\n",
      " ---- 19 metric test_eval_time = 6.6990778191231195\n",
      " ---- 19 metric ood_eval = 0.0\n",
      " ---- 19 metric ood_eval_time = 6.6990778191231195\n",
      " ---- 20000 metric loss = 0.14308929763920605\n",
      " ---- 20 metric test_eval_f1 = 0.8665264520304158\n",
      " ---- 20 metric test_eval_time = 7.089953082025562\n",
      " ---- 20 metric ood_eval = 0.0\n",
      " ---- 20 metric ood_eval_time = 7.089953082025562\n",
      " ---- 10 metric train_eval_f1 = 0.872299975730119\n",
      " ---- 10 metric train_eval_time = 7.0896367607798725\n",
      " ---- 21000 metric loss = 0.14307309247087688\n",
      " ---- 21 metric test_eval_f1 = 0.8594078628053713\n",
      " ---- 21 metric test_eval_time = 6.428247856333926\n",
      " ---- 21 metric ood_eval = 0.0\n",
      " ---- 21 metric ood_eval_time = 6.428247856333926\n",
      " ---- 22000 metric loss = 0.14124247584305705\n",
      " ---- 22 metric test_eval_f1 = 0.8670118103866689\n",
      " ---- 22 metric test_eval_time = 8.138327131532115\n",
      " ---- 22 metric ood_eval = 0.0\n",
      " ---- 22 metric ood_eval_time = 8.138327131532115\n",
      " ---- 11 metric train_eval_f1 = 0.8779629479815549\n",
      " ---- 11 metric train_eval_time = 8.181862308874686\n",
      " ---- 23000 metric loss = 0.13970101603679358\n",
      " ---- 23 metric test_eval_f1 = 0.87040931888044\n",
      " ---- 23 metric test_eval_time = 7.130561397832066\n",
      " ---- 23 metric ood_eval = 0.00016178611875101117\n",
      " ---- 23 metric ood_eval_time = 7.130561397832066\n",
      " ---- 24000 metric loss = 0.13640451733395456\n",
      " ---- 24 metric test_eval_f1 = 0.87040931888044\n",
      " ---- 24 metric test_eval_time = 7.4546189936903415\n",
      " ---- 24 metric ood_eval = 0.0\n",
      " ---- 24 metric ood_eval_time = 7.4546189936903415\n",
      " ---- 12 metric train_eval_f1 = 0.877841598576167\n",
      " ---- 12 metric train_eval_time = 7.391432731979613\n",
      " ---- 25000 metric loss = 0.13308392015844583\n",
      " ---- 25 metric test_eval_f1 = 0.8728361106617052\n",
      " ---- 25 metric test_eval_time = 8.617214043035107\n",
      " ---- 25 metric ood_eval = 0.0\n",
      " ---- 25 metric ood_eval_time = 8.617214043035107\n",
      " ---- 26000 metric loss = 0.13565684208832682\n",
      " ---- 26 metric test_eval_f1 = 0.8747775440867174\n",
      " ---- 26 metric test_eval_time = 7.055654424850347\n",
      " ---- 26 metric ood_eval = 0.0\n",
      " ---- 26 metric ood_eval_time = 7.055654424850347\n",
      " ---- 13 metric train_eval_f1 = 0.8845562656742982\n",
      " ---- 13 metric train_eval_time = 7.048701561362349\n",
      " ---- 27000 metric loss = 0.1288717229347676\n",
      " ---- 27 metric test_eval_f1 = 0.8762336191554765\n",
      " ---- 27 metric test_eval_time = 6.71525643099822\n",
      " ---- 27 metric ood_eval = 0.0\n",
      " ---- 27 metric ood_eval_time = 6.71525643099822\n",
      " ---- 28000 metric loss = 0.13236578508466482\n",
      " ---- 28 metric test_eval_f1 = 0.8742921857304643\n",
      " ---- 28 metric test_eval_time = 7.6733538262417085\n",
      " ---- 28 metric ood_eval = 0.0\n",
      " ---- 28 metric ood_eval_time = 7.6733538262417085\n",
      " ---- 14 metric train_eval_f1 = 0.8830596230078472\n",
      " ---- 14 metric train_eval_time = 7.645902435078068\n",
      " ---- 29000 metric loss = 0.12959485589154066\n",
      " ---- 29 metric test_eval_f1 = 0.8765571913929785\n",
      " ---- 29 metric test_eval_time = 7.263387801326646\n",
      " ---- 29 metric ood_eval = 0.0\n",
      " ---- 29 metric ood_eval_time = 7.263387801326646\n",
      " ---- 30000 metric loss = 0.12916709174308927\n",
      " ---- 30 metric test_eval_f1 = 0.8817343471930108\n",
      " ---- 30 metric test_eval_time = 6.755702960685973\n",
      " ---- 30 metric ood_eval = 0.0\n",
      " ---- 30 metric ood_eval_time = 6.755702960685973\n",
      " ---- 15 metric train_eval_f1 = 0.8882371976377316\n",
      " ---- 15 metric train_eval_time = 6.747714586198527\n",
      " ---- 31000 metric loss = 0.13014708526805044\n",
      " ---- 31 metric test_eval_f1 = 0.8831904222617699\n",
      " ---- 31 metric test_eval_time = 8.955832389580975\n",
      " ---- 31 metric ood_eval = 0.0\n",
      " ---- 31 metric ood_eval_time = 8.955832389580975\n",
      " ---- 32000 metric loss = 0.12726235245727002\n",
      " ---- 32 metric test_eval_f1 = 0.8810872027180068\n",
      " ---- 32 metric test_eval_time = 7.799061640511244\n",
      " ---- 32 metric ood_eval = 0.0006471444750040447\n",
      " ---- 32 metric ood_eval_time = 7.799061640511244\n",
      " ---- 16 metric train_eval_f1 = 0.8886821454574872\n",
      " ---- 16 metric train_eval_time = 7.714181700509667\n",
      " ---- 33000 metric loss = 0.12734623762220143\n",
      " ---- 33 metric test_eval_f1 = 0.8841611389742761\n",
      " ---- 33 metric test_eval_time = 7.141401067788384\n",
      " ---- 33 metric ood_eval = 0.0\n",
      " ---- 33 metric ood_eval_time = 7.141401067788384\n",
      " ---- 34000 metric loss = 0.126611180819571\n",
      " ---- 34 metric test_eval_f1 = 0.8801164860055007\n",
      " ---- 34 metric test_eval_time = 6.537777058728361\n",
      " ---- 34 metric ood_eval = 0.00032357223750202233\n",
      " ---- 34 metric ood_eval_time = 6.537777058728361\n",
      " ---- 17 metric train_eval_f1 = 0.8867405549712807\n",
      " ---- 17 metric train_eval_time = 6.506067470269396\n",
      " ---- 35000 metric loss = 0.12420452010352165\n",
      " ---- 35 metric test_eval_f1 = 0.8869115029930432\n",
      " ---- 35 metric test_eval_time = 6.580326807959877\n",
      " ---- 35 metric ood_eval = 0.0\n",
      " ---- 35 metric ood_eval_time = 6.580326807959877\n",
      " ---- 36000 metric loss = 0.12501032120734454\n",
      " ---- 36 metric test_eval_f1 = 0.8886911502993043\n",
      " ---- 36 metric test_eval_time = 8.010677883837566\n",
      " ---- 36 metric ood_eval = 0.0\n",
      " ---- 36 metric ood_eval_time = 8.010677883837566\n",
      " ---- 18 metric train_eval_f1 = 0.8970957042310492\n",
      " ---- 18 metric train_eval_time = 8.006148369872987\n",
      " ---- 37000 metric loss = 0.12200923688523471\n",
      " ---- 37 metric test_eval_f1 = 0.8873968613492962\n",
      " ---- 37 metric test_eval_time = 9.585989322116163\n",
      " ---- 37 metric ood_eval = 0.0\n",
      " ---- 37 metric ood_eval_time = 9.585989322116163\n",
      " ---- 38000 metric loss = 0.12091268872376532\n",
      " ---- 38 metric test_eval_f1 = 0.884646497330529\n",
      " ---- 38 metric test_eval_time = 6.603947581297525\n",
      " ---- 38 metric ood_eval = 0.0\n",
      " ---- 38 metric ood_eval_time = 6.603947581297525\n",
      " ---- 19 metric train_eval_f1 = 0.8939001698891675\n",
      " ---- 19 metric train_eval_time = 6.606140279912628\n",
      " ---- 39000 metric loss = 0.11691295890603215\n",
      " ---- 39 metric test_eval_f1 = 0.8928975893868306\n",
      " ---- 39 metric test_eval_time = 7.281507846626759\n",
      " ---- 39 metric ood_eval = 0.0022650056625141564\n",
      " ---- 39 metric ood_eval_time = 7.281507846626759\n",
      " ---- 40000 metric loss = 0.11954429875686765\n",
      " ---- 40 metric test_eval_f1 = 0.8901472253680635\n",
      " ---- 40 metric test_eval_time = 6.6031386507037695\n",
      " ---- 40 metric ood_eval = 0.00016178611875101117\n",
      " ---- 40 metric ood_eval_time = 6.6031386507037695\n",
      " ---- 20 metric train_eval_f1 = 0.8998867405549713\n",
      " ---- 20 metric train_eval_time = 6.561605048135264\n",
      " ---- 41000 metric loss = 0.11796839669905602\n",
      " ---- 41 metric test_eval_f1 = 0.8917650865555735\n",
      " ---- 41 metric test_eval_time = 6.536320983659602\n",
      " ---- 41 metric ood_eval = 0.0\n",
      " ---- 41 metric ood_eval_time = 6.536320983659602\n",
      " ---- 42000 metric loss = 0.11804548165388405\n",
      " ---- 42 metric test_eval_f1 = 0.8888529364180553\n",
      " ---- 42 metric test_eval_time = 6.422261769940139\n",
      " ---- 42 metric ood_eval = 0.0\n",
      " ---- 42 metric ood_eval_time = 6.422261769940139\n",
      " ---- 21 metric train_eval_f1 = 0.898956395113664\n",
      " ---- 21 metric train_eval_time = 6.392120378610144\n",
      " ---- 43000 metric loss = 0.1173223929181695\n",
      " ---- 43 metric test_eval_f1 = 0.8870732891117942\n",
      " ---- 43 metric test_eval_time = 7.770425497492315\n",
      " ---- 43 metric ood_eval = 0.0\n",
      " ---- 43 metric ood_eval_time = 7.770425497492315\n",
      " ---- 44000 metric loss = 0.1105965453227982\n",
      " ---- 44 metric test_eval_f1 = 0.8966186701181039\n",
      " ---- 44 metric test_eval_time = 7.425982850671413\n",
      " ---- 44 metric ood_eval = 0.0004853583562530335\n",
      " ---- 44 metric ood_eval_time = 7.425982850671413\n",
      " ---- 22 metric train_eval_f1 = 0.9038508211309765\n",
      " ---- 22 metric train_eval_time = 7.377598899765391\n",
      " ---- 45000 metric loss = 0.11694309385959059\n",
      " ---- 45 metric test_eval_f1 = 0.885293641805533\n",
      " ---- 45 metric test_eval_time = 6.837243164536483\n",
      " ---- 45 metric ood_eval = 0.0008089305937550558\n",
      " ---- 45 metric ood_eval_time = 6.837243164536483\n",
      " ---- 46000 metric loss = 0.11263889935892075\n",
      " ---- 46 metric test_eval_f1 = 0.8967804562368549\n",
      " ---- 46 metric test_eval_time = 7.853259990292833\n",
      " ---- 46 metric ood_eval = 0.0\n",
      " ---- 46 metric ood_eval_time = 7.853259990292833\n",
      " ---- 23 metric train_eval_f1 = 0.9049834155812636\n",
      " ---- 23 metric train_eval_time = 7.793422862227975\n",
      " ---- 47000 metric loss = 0.11333021469414234\n",
      " ---- 47 metric test_eval_f1 = 0.8964568839993529\n",
      " ---- 47 metric test_eval_time = 7.045138327131532\n",
      " ---- 47 metric ood_eval = 0.0\n",
      " ---- 47 metric ood_eval_time = 7.045138327131532\n",
      " ---- 48000 metric loss = 0.11120054735802114\n",
      " ---- 48 metric test_eval_f1 = 0.8961333117618508\n",
      " ---- 48 metric test_eval_time = 6.756673677398479\n",
      " ---- 48 metric ood_eval = 0.00016178611875101117\n",
      " ---- 48 metric ood_eval_time = 6.756673677398479\n",
      " ---- 24 metric train_eval_f1 = 0.9047002669686919\n",
      " ---- 24 metric train_eval_time = 6.703947900655287\n",
      " ---- 49000 metric loss = 0.10880699990689754\n",
      " ---- 49 metric test_eval_f1 = 0.8857790001617861\n",
      " ---- 49 metric test_eval_time = 7.301083966995631\n",
      " ---- 49 metric ood_eval = 0.0\n",
      " ---- 49 metric ood_eval_time = 7.301083966995631\n",
      " ---- 50000 metric loss = 0.11041623485740275\n",
      " ---- 50 metric test_eval_f1 = 0.9001779647306261\n",
      " ---- 50 metric test_eval_time = 6.95842096748099\n",
      " ---- 50 metric ood_eval = 0.0\n",
      " ---- 50 metric ood_eval_time = 6.95842096748099\n",
      " ---- 25 metric train_eval_f1 = 0.9069654558692662\n",
      " ---- 25 metric train_eval_time = 6.886538305962301\n",
      " ---- 51000 metric loss = 0.10739301354251803\n",
      " ---- 51 metric test_eval_f1 = 0.8969422423556059\n",
      " ---- 51 metric test_eval_time = 6.497815887396861\n",
      " ---- 51 metric ood_eval = 0.0\n",
      " ---- 51 metric ood_eval_time = 6.497815887396861\n",
      " ---- 52000 metric loss = 0.10731109890993684\n",
      " ---- 52 metric test_eval_f1 = 0.9022811842743893\n",
      " ---- 52 metric test_eval_time = 8.023297201100146\n",
      " ---- 52 metric ood_eval = 0.0\n",
      " ---- 52 metric ood_eval_time = 8.023297201100146\n",
      " ---- 26 metric train_eval_f1 = 0.9104036890219238\n",
      " ---- 26 metric train_eval_time = 7.901666531833994\n",
      " ---- 53000 metric loss = 0.10632284268829971\n",
      " ---- 53 metric test_eval_f1 = 0.9005015369681282\n",
      " ---- 53 metric test_eval_time = 7.990131046756188\n",
      " ---- 53 metric ood_eval = 0.0\n",
      " ---- 53 metric ood_eval_time = 7.990131046756188\n",
      " ---- 54000 metric loss = 0.10609351838659495\n",
      " ---- 54 metric test_eval_f1 = 0.8996926063743731\n",
      " ---- 54 metric test_eval_time = 6.98721889661867\n",
      " ---- 54 metric ood_eval = 0.0\n",
      " ---- 54 metric ood_eval_time = 6.98721889661867\n",
      " ---- 27 metric train_eval_f1 = 0.9106463878326996\n",
      " ---- 27 metric train_eval_time = 6.924965617668473\n",
      " ---- 55000 metric loss = 0.10374433675594628\n",
      " ---- 55 metric test_eval_f1 = 0.9061640511244136\n",
      " ---- 55 metric test_eval_time = 6.690503154829315\n",
      " ---- 55 metric ood_eval = 0.0\n",
      " ---- 55 metric ood_eval_time = 6.690503154829315\n",
      " ---- 56000 metric loss = 0.10430105541273951\n",
      " ---- 56 metric test_eval_f1 = 0.898398317424365\n",
      " ---- 56 metric test_eval_time = 9.54085099498463\n",
      " ---- 56 metric ood_eval = 0.0\n",
      " ---- 56 metric ood_eval_time = 9.54085099498463\n",
      " ---- 28 metric train_eval_f1 = 0.9104441388237198\n",
      " ---- 28 metric train_eval_time = 9.446161313809561\n",
      " ---- 57000 metric loss = 0.1004136733720079\n",
      " ---- 57 metric test_eval_f1 = 0.9026047565118913\n",
      " ---- 57 metric test_eval_time = 7.785795178773661\n",
      " ---- 57 metric ood_eval = 0.0\n",
      " ---- 57 metric ood_eval_time = 7.785795178773661\n",
      " ---- 58000 metric loss = 0.10288990875985474\n",
      " ---- 58 metric test_eval_f1 = 0.9050315482931565\n",
      " ---- 58 metric test_eval_time = 7.199644070538747\n",
      " ---- 58 metric ood_eval = 0.0\n",
      " ---- 58 metric ood_eval_time = 7.199644070538747\n",
      " ---- 29 metric train_eval_f1 = 0.9146509182105008\n",
      " ---- 29 metric train_eval_time = 7.140482161637408\n",
      " ---- 59000 metric loss = 0.10107707803696395\n",
      " ---- 59 metric test_eval_f1 = 0.9055169066494094\n",
      " ---- 59 metric test_eval_time = 7.052256916356576\n",
      " ---- 59 metric ood_eval = 0.0\n",
      " ---- 59 metric ood_eval_time = 7.052256916356576\n",
      " ---- 60000 metric loss = 0.10023453918192536\n",
      " ---- 60 metric test_eval_f1 = 0.9061640511244136\n",
      " ---- 60 metric test_eval_time = 6.730626112279566\n",
      " ---- 60 metric ood_eval = 0.0\n",
      " ---- 60 metric ood_eval_time = 6.730626112279566\n",
      " ---- 30 metric train_eval_f1 = 0.9137610225709895\n",
      " ---- 30 metric train_eval_time = 6.678626324731009\n",
      " ---- 61000 metric loss = 0.102292271762155\n",
      " ---- 61 metric test_eval_f1 = 0.8872350752305452\n",
      " ---- 61 metric test_eval_time = 6.644555897104029\n",
      " ---- 61 metric ood_eval = 0.0\n",
      " ---- 61 metric ood_eval_time = 6.644555897104029\n",
      " ---- 62000 metric loss = 0.09715473995357751\n",
      " ---- 62 metric test_eval_f1 = 0.9040608315806504\n",
      " ---- 62 metric test_eval_time = 8.122472091894515\n",
      " ---- 62 metric ood_eval = 0.0004853583562530335\n",
      " ---- 62 metric ood_eval_time = 8.122472091894515\n",
      " ---- 31 metric train_eval_f1 = 0.9152576652374403\n",
      " ---- 31 metric train_eval_time = 8.054202734406601\n",
      " ---- 63000 metric loss = 0.0974449385534972\n",
      " ---- 63 metric test_eval_f1 = 0.9069729817181685\n",
      " ---- 63 metric test_eval_time = 7.263711373564148\n",
      " ---- 63 metric ood_eval = 0.0\n",
      " ---- 63 metric ood_eval_time = 7.263711373564148\n",
      " ---- 64000 metric loss = 0.09696225209906698\n",
      " ---- 64 metric test_eval_f1 = 0.9017958259181362\n",
      " ---- 64 metric test_eval_time = 6.647144475004045\n",
      " ---- 64 metric ood_eval = 0.0\n",
      " ---- 64 metric ood_eval_time = 6.647144475004045\n",
      " ---- 32 metric train_eval_f1 = 0.9137610225709895\n",
      " ---- 32 metric train_eval_time = 6.613259445028719\n",
      " ---- 65000 metric loss = 0.10012760979030282\n",
      " ---- 65 metric test_eval_f1 = 0.8990454618993691\n",
      " ---- 65 metric test_eval_time = 8.578708946772366\n",
      " ---- 65 metric ood_eval = 0.0\n",
      " ---- 65 metric ood_eval_time = 8.578708946772366\n",
      " ---- 66000 metric loss = 0.09513996285293251\n",
      " ---- 66 metric test_eval_f1 = 0.9069729817181685\n",
      " ---- 66 metric test_eval_time = 6.67367739847921\n",
      " ---- 66 metric ood_eval = 0.00016178611875101117\n",
      " ---- 66 metric ood_eval_time = 6.67367739847921\n",
      " ---- 33 metric train_eval_f1 = 0.9195453442278133\n",
      " ---- 33 metric train_eval_time = 6.6058975811018525\n",
      " ---- 67000 metric loss = 0.09762308422476053\n",
      " ---- 67 metric test_eval_f1 = 0.9011486814431322\n",
      " ---- 67 metric test_eval_time = 6.65766057272286\n",
      " ---- 67 metric ood_eval = 0.0\n",
      " ---- 67 metric ood_eval_time = 6.65766057272286\n",
      " ---- 68000 metric loss = 0.09503274904657155\n",
      " ---- 68 metric test_eval_f1 = 0.9072965539556705\n",
      " ---- 68 metric test_eval_time = 7.273094968451707\n",
      " ---- 68 metric ood_eval = 0.0\n",
      " ---- 68 metric ood_eval_time = 7.273094968451707\n",
      " ---- 34 metric train_eval_f1 = 0.9203947900655287\n",
      " ---- 34 metric train_eval_time = 7.199741121268506\n",
      " ---- 69000 metric loss = 0.09673686481034383\n",
      " ---- 69 metric test_eval_f1 = 0.9082672706681767\n",
      " ---- 69 metric test_eval_time = 7.511405921371947\n",
      " ---- 69 metric ood_eval = 0.0\n",
      " ---- 69 metric ood_eval_time = 7.511405921371947\n",
      " ---- 70000 metric loss = 0.09571818482829259\n",
      " ---- 70 metric test_eval_f1 = 0.9093997734994338\n",
      " ---- 70 metric test_eval_time = 6.893382947743084\n",
      " ---- 70 metric ood_eval = 0.00016178611875101117\n",
      " ---- 70 metric ood_eval_time = 6.893382947743084\n",
      " ---- 35 metric train_eval_f1 = 0.9221745813445514\n",
      " ---- 35 metric train_eval_time = 6.855634657390179\n",
      " ---- 71000 metric loss = 0.09340559865906835\n",
      " ---- 71 metric test_eval_f1 = 0.9152240737744701\n",
      " ---- 71 metric test_eval_time = 6.959877042549749\n",
      " ---- 71 metric ood_eval = 0.0\n",
      " ---- 71 metric ood_eval_time = 6.959877042549749\n",
      " ---- 72000 metric loss = 0.09303815206699073\n",
      " ---- 72 metric test_eval_f1 = 0.9123119236369519\n",
      " ---- 72 metric test_eval_time = 6.5817828830286365\n",
      " ---- 72 metric ood_eval = 0.0\n",
      " ---- 72 metric ood_eval_time = 6.5817828830286365\n",
      " ---- 36 metric train_eval_f1 = 0.9232262761912466\n",
      " ---- 36 metric train_eval_time = 6.537213817652294\n",
      " ---- 73000 metric loss = 0.0919473268352449\n",
      " ---- 73 metric test_eval_f1 = 0.9097233457369358\n",
      " ---- 73 metric test_eval_time = 6.991425335706197\n",
      " ---- 73 metric ood_eval = 0.00016178611875101117\n",
      " ---- 73 metric ood_eval_time = 6.991425335706197\n",
      " ---- 74000 metric loss = 0.09217700174683705\n",
      " ---- 74 metric test_eval_f1 = 0.9149005015369681\n",
      " ---- 74 metric test_eval_time = 7.188319042226177\n",
      " ---- 74 metric ood_eval = 0.0\n",
      " ---- 74 metric ood_eval_time = 7.188319042226177\n",
      " ---- 37 metric train_eval_f1 = 0.9269072081546801\n",
      " ---- 37 metric train_eval_time = 7.089919909392444\n",
      " ---- 75000 metric loss = 0.08872452689008788\n",
      " ---- 75 metric test_eval_f1 = 0.912797281993205\n",
      " ---- 75 metric test_eval_time = 7.3311761850833195\n",
      " ---- 75 metric ood_eval = 0.0\n",
      " ---- 75 metric ood_eval_time = 7.3311761850833195\n",
      " ---- 76000 metric loss = 0.09147805669065565\n",
      " ---- 76 metric test_eval_f1 = 0.898883675780618\n",
      " ---- 76 metric test_eval_time = 7.525966672059537\n",
      " ---- 76 metric ood_eval = 0.0\n",
      " ---- 76 metric ood_eval_time = 7.525966672059537\n",
      " ---- 38 metric train_eval_f1 = 0.9111317854542512\n",
      " ---- 38 metric train_eval_time = 7.423833023218187\n",
      " ---- 77000 metric loss = 0.09240787512715906\n",
      " ---- 77 metric test_eval_f1 = 0.9182980100307394\n",
      " ---- 77 metric test_eval_time = 6.901634039799386\n",
      " ---- 77 metric ood_eval = 0.0\n",
      " ---- 77 metric ood_eval_time = 6.901634039799386\n",
      " ---- 78000 metric loss = 0.0895626712567173\n",
      " ---- 78 metric test_eval_f1 = 0.9149005015369681\n",
      " ---- 78 metric test_eval_time = 8.038181524025239\n",
      " ---- 78 metric ood_eval = 0.0\n",
      " ---- 78 metric ood_eval_time = 8.038181524025239\n",
      " ---- 39 metric train_eval_f1 = 0.9282825014157431\n",
      " ---- 39 metric train_eval_time = 7.941873634819189\n",
      " ---- 79000 metric loss = 0.08712660970073194\n",
      " ---- 79 metric test_eval_f1 = 0.9030901148681443\n",
      " ---- 79 metric test_eval_time = 8.46238472739039\n",
      " ---- 79 metric ood_eval = 0.0\n",
      " ---- 79 metric ood_eval_time = 8.46238472739039\n",
      " ---- 80000 metric loss = 0.08900634767999872\n",
      " ---- 80 metric test_eval_f1 = 0.9150622876557192\n",
      " ---- 80 metric test_eval_time = 6.867982527099175\n",
      " ---- 80 metric ood_eval = 0.0\n",
      " ---- 80 metric ood_eval_time = 6.867982527099175\n",
      " ---- 40 metric train_eval_f1 = 0.9273926057762317\n",
      " ---- 40 metric train_eval_time = 6.767696788285737\n",
      " ---- 81000 metric loss = 0.08662041497509927\n",
      " ---- 81 metric test_eval_f1 = 0.898236531305614\n",
      " ---- 81 metric test_eval_time = 6.9150622876557195\n",
      " ---- 81 metric ood_eval = 0.0\n",
      " ---- 81 metric ood_eval_time = 6.9150622876557195\n",
      " ---- 82000 metric loss = 0.08997981547564268\n",
      " ---- 82 metric test_eval_f1 = 0.9205630156932535\n",
      " ---- 82 metric test_eval_time = 7.322277948552014\n",
      " ---- 82 metric ood_eval = 0.0\n",
      " ---- 82 metric ood_eval_time = 7.322277948552014\n",
      " ---- 41 metric train_eval_f1 = 0.9308308389288893\n",
      " ---- 41 metric train_eval_time = 7.279144082193997\n",
      " ---- 83000 metric loss = 0.08619686446618288\n",
      " ---- 83 metric test_eval_f1 = 0.912959068111956\n",
      " ---- 83 metric test_eval_time = 7.471768322277948\n",
      " ---- 83 metric ood_eval = 0.00032357223750202233\n",
      " ---- 83 metric ood_eval_time = 7.471768322277948\n",
      " ---- 84000 metric loss = 0.08859516563452781\n",
      " ---- 84 metric test_eval_f1 = 0.9171655071994823\n",
      " ---- 84 metric test_eval_time = 7.824300275036402\n",
      " ---- 84 metric ood_eval = 0.0\n",
      " ---- 84 metric ood_eval_time = 7.824300275036402\n",
      " ---- 42 metric train_eval_f1 = 0.9305881401181134\n",
      " ---- 42 metric train_eval_time = 7.751152819351185\n",
      " ---- 85000 metric loss = 0.08368938524043187\n",
      " ---- 85 metric test_eval_f1 = 0.9191069406244944\n",
      " ---- 85 metric test_eval_time = 7.307717197864423\n",
      " ---- 85 metric ood_eval = 0.0\n",
      " ---- 85 metric ood_eval_time = 7.307717197864423\n",
      " ---- 86000 metric loss = 0.08770061120763421\n",
      " ---- 86 metric test_eval_f1 = 0.9136062125869601\n",
      " ---- 86 metric test_eval_time = 7.052095130237825\n",
      " ---- 86 metric ood_eval = 0.0\n",
      " ---- 86 metric ood_eval_time = 7.052095130237825\n",
      " ---- 43 metric train_eval_f1 = 0.9281207022085591\n",
      " ---- 43 metric train_eval_time = 6.950813041016099\n",
      " ---- 87000 metric loss = 0.08321872859541327\n",
      " ---- 87 metric test_eval_f1 = 0.9171655071994823\n",
      " ---- 87 metric test_eval_time = 7.371460928652322\n",
      " ---- 87 metric ood_eval = 0.0\n",
      " ---- 87 metric ood_eval_time = 7.371460928652322\n",
      " ---- 88000 metric loss = 0.08151314220903441\n",
      " ---- 88 metric test_eval_f1 = 0.9179744377932373\n",
      " ---- 88 metric test_eval_time = 7.7265814593107915\n",
      " ---- 88 metric ood_eval = 0.0\n",
      " ---- 88 metric ood_eval_time = 7.7265814593107915\n",
      " ---- 44 metric train_eval_f1 = 0.9319634333791764\n",
      " ---- 44 metric train_eval_time = 7.576126526980018\n",
      " ---- 89000 metric loss = 0.0827837416678667\n",
      " ---- 89 metric test_eval_f1 = 0.9220190907620126\n",
      " ---- 89 metric test_eval_time = 7.7933991263549585\n",
      " ---- 89 metric ood_eval = 0.0\n",
      " ---- 89 metric ood_eval_time = 7.7933991263549585\n",
      " ---- 90000 metric loss = 0.08332381521444768\n",
      " ---- 90 metric test_eval_f1 = 0.9058404788869115\n",
      " ---- 90 metric test_eval_time = 7.692444588254328\n",
      " ---- 90 metric ood_eval = 0.0\n",
      " ---- 90 metric ood_eval_time = 7.692444588254328\n",
      " ---- 45 metric train_eval_f1 = 0.9226599789661031\n",
      " ---- 45 metric train_eval_time = 7.60100315508454\n",
      " ---- 91000 metric loss = 0.08250994996260852\n",
      " ---- 91 metric test_eval_f1 = 0.914091570943213\n",
      " ---- 91 metric test_eval_time = 6.976702798899854\n",
      " ---- 91 metric ood_eval = 0.0\n",
      " ---- 91 metric ood_eval_time = 6.976702798899854\n",
      " ---- 92000 metric loss = 0.08463293101731688\n",
      " ---- 92 metric test_eval_f1 = 0.9184597961494904\n",
      " ---- 92 metric test_eval_time = 6.91360621258696\n",
      " ---- 92 metric ood_eval = 0.0\n",
      " ---- 92 metric ood_eval_time = 6.91360621258696\n",
      " ---- 46 metric train_eval_f1 = 0.9340668230725669\n",
      " ---- 46 metric train_eval_time = 6.789135183237602\n",
      " ---- 93000 metric loss = 0.08130975623242558\n",
      " ---- 93 metric test_eval_f1 = 0.9153858598932212\n",
      " ---- 93 metric test_eval_time = 7.177155800032358\n",
      " ---- 93 metric ood_eval = 0.0\n",
      " ---- 93 metric ood_eval_time = 7.177155800032358\n",
      " ---- 94000 metric loss = 0.08022081099729984\n",
      " ---- 94 metric test_eval_f1 = 0.9165183627244783\n",
      " ---- 94 metric test_eval_time = 6.854877851480343\n",
      " ---- 94 metric ood_eval = 0.00016178611875101117\n",
      " ---- 94 metric ood_eval_time = 6.854877851480343\n",
      " ---- 47 metric train_eval_f1 = 0.9306285899199094\n",
      " ---- 47 metric train_eval_time = 6.762397864250465\n",
      " ---- 95000 metric loss = 0.07875587175553665\n",
      " ---- 95 metric test_eval_f1 = 0.9189451545057434\n",
      " ---- 95 metric test_eval_time = 8.342339427277139\n",
      " ---- 95 metric ood_eval = 0.0\n",
      " ---- 95 metric ood_eval_time = 8.342339427277139\n",
      " ---- 96000 metric loss = 0.0837638383116573\n",
      " ---- 96 metric test_eval_f1 = 0.9084290567869276\n",
      " ---- 96 metric test_eval_time = 7.874615757967966\n",
      " ---- 96 metric ood_eval = 0.0\n",
      " ---- 96 metric ood_eval_time = 7.874615757967966\n",
      " ---- 48 metric train_eval_f1 = 0.9257341639025969\n",
      " ---- 48 metric train_eval_time = 7.777242941509587\n",
      " ---- 97000 metric loss = 0.07823293468542397\n",
      " ---- 97 metric test_eval_f1 = 0.9170037210807312\n",
      " ---- 97 metric test_eval_time = 7.044976541012781\n",
      " ---- 97 metric ood_eval = 0.0\n",
      " ---- 97 metric ood_eval_time = 7.044976541012781\n",
      " ---- 98000 metric loss = 0.07813409427599981\n",
      " ---- 98 metric test_eval_f1 = 0.9176508655557353\n",
      " ---- 98 metric test_eval_time = 7.353179097233458\n",
      " ---- 98 metric ood_eval = 0.0\n",
      " ---- 98 metric ood_eval_time = 7.353179097233458\n",
      " ---- 49 metric train_eval_f1 = 0.9347140199013024\n",
      " ---- 49 metric train_eval_time = 7.2617102176199335\n",
      " ---- 99000 metric loss = 0.08023331963503734\n",
      " ---- 99 metric test_eval_f1 = 0.9192687267432454\n",
      " ---- 99 metric test_eval_time = 6.646011972172787\n",
      " ---- 99 metric ood_eval = 0.0\n",
      " ---- 99 metric ood_eval_time = 6.646011972172787\n",
      " ---- 100000 metric loss = 0.07771161952987313\n",
      " ---- 100 metric test_eval_f1 = 0.9176508655557353\n",
      " ---- 100 metric test_eval_time = 7.09270344604433\n",
      " ---- 100 metric ood_eval = 0.0\n",
      " ---- 100 metric ood_eval_time = 7.09270344604433\n",
      " ---- 50 metric train_eval_f1 = 0.9350376183156702\n",
      " ---- 50 metric train_eval_time = 6.946727611034706\n",
      " ---- 101000 metric loss = 0.08375776564516127\n",
      " ---- 101 metric test_eval_f1 = 0.9204012295745025\n",
      " ---- 101 metric test_eval_time = 7.404141724640026\n",
      " ---- 101 metric ood_eval = 0.0\n",
      " ---- 101 metric ood_eval_time = 7.404141724640026\n",
      " ---- 102000 metric loss = 0.08016024752240627\n",
      " ---- 102 metric test_eval_f1 = 0.9205630156932535\n",
      " ---- 102 metric test_eval_time = 6.8013266461737585\n",
      " ---- 102 metric ood_eval = 0.0\n",
      " ---- 102 metric ood_eval_time = 6.8013266461737585\n",
      " ---- 51 metric train_eval_f1 = 0.939487096513227\n",
      " ---- 51 metric train_eval_time = 6.711835612005501\n",
      " ---- 103000 metric loss = 0.07701574651617557\n",
      " ---- 103 metric test_eval_f1 = 0.9234751658307717\n",
      " ---- 103 metric test_eval_time = 7.020385050962627\n",
      " ---- 103 metric ood_eval = 0.0\n",
      " ---- 103 metric ood_eval_time = 7.020385050962627\n",
      " ---- 104000 metric loss = 0.07730124953249469\n",
      " ---- 104 metric test_eval_f1 = 0.9220190907620126\n",
      " ---- 104 metric test_eval_time = 8.176508655557353\n",
      " ---- 104 metric ood_eval = 0.00016178611875101117\n",
      " ---- 104 metric ood_eval_time = 8.176508655557353\n",
      " ---- 52 metric train_eval_f1 = 0.9389207992880835\n",
      " ---- 52 metric train_eval_time = 8.051290348677291\n",
      " ---- 105000 metric loss = 0.07968521654000506\n",
      " ---- 105 metric test_eval_f1 = 0.9195922989807475\n",
      " ---- 105 metric test_eval_time = 7.896942242355606\n",
      " ---- 105 metric ood_eval = 0.0\n",
      " ---- 105 metric ood_eval_time = 7.896942242355606\n",
      " ---- 106000 metric loss = 0.07397569634672255\n",
      " ---- 106 metric test_eval_f1 = 0.9195922989807475\n",
      " ---- 106 metric test_eval_time = 6.9409480666558805\n",
      " ---- 106 metric ood_eval = 0.0\n",
      " ---- 106 metric ood_eval_time = 6.9409480666558805\n",
      " ---- 53 metric train_eval_f1 = 0.9399724941347788\n",
      " ---- 53 metric train_eval_time = 6.846978399805841\n",
      " ---- 107000 metric loss = 0.07767038034601137\n",
      " ---- 107 metric test_eval_f1 = 0.9239605241870248\n",
      " ---- 107 metric test_eval_time = 7.282963921695519\n",
      " ---- 107 metric ood_eval = 0.0\n",
      " ---- 107 metric ood_eval_time = 7.282963921695519\n",
      " ---- 108000 metric loss = 0.07574619282316417\n",
      " ---- 108 metric test_eval_f1 = 0.9079436984306747\n",
      " ---- 108 metric test_eval_time = 7.033004368225206\n",
      " ---- 108 metric ood_eval = 0.0\n",
      " ---- 108 metric ood_eval_time = 7.033004368225206\n",
      " ---- 54 metric train_eval_f1 = 0.9307094895235013\n",
      " ---- 54 metric train_eval_time = 6.92399482242537\n",
      " ---- 109000 metric loss = 0.07378893891070037\n",
      " ---- 109 metric test_eval_f1 = 0.9246076686620288\n",
      " ---- 109 metric test_eval_time = 7.229574502507685\n",
      " ---- 109 metric ood_eval = 0.0\n",
      " ---- 109 metric ood_eval_time = 7.229574502507685\n",
      " ---- 110000 metric loss = 0.07325495928246528\n",
      " ---- 110 metric test_eval_f1 = 0.9223426629995146\n",
      " ---- 110 metric test_eval_time = 7.199805856657499\n",
      " ---- 110 metric ood_eval = 0.0\n",
      " ---- 110 metric ood_eval_time = 7.199805856657499\n",
      " ---- 55 metric train_eval_f1 = 0.9377882048377963\n",
      " ---- 55 metric train_eval_time = 7.1029851953725425\n",
      " ---- 111000 metric loss = 0.07340898811677471\n",
      " ---- 111 metric test_eval_f1 = 0.9186215822682414\n",
      " ---- 111 metric test_eval_time = 7.078789839831742\n",
      " ---- 111 metric ood_eval = 0.00016178611875101117\n",
      " ---- 111 metric ood_eval_time = 7.078789839831742\n",
      " ---- 112000 metric loss = 0.07414738049823791\n",
      " ---- 112 metric test_eval_f1 = 0.914415143180715\n",
      " ---- 112 metric test_eval_time = 6.905840478886912\n",
      " ---- 112 metric ood_eval = 0.0\n",
      " ---- 112 metric ood_eval_time = 6.905840478886912\n",
      " ---- 56 metric train_eval_f1 = 0.9378286546395923\n",
      " ---- 56 metric train_eval_time = 6.7617102176199335\n",
      " ---- 113000 metric loss = 0.07593124353000894\n",
      " ---- 113 metric test_eval_f1 = 0.9228280213557677\n",
      " ---- 113 metric test_eval_time = 8.174081863776088\n",
      " ---- 113 metric ood_eval = 0.0\n",
      " ---- 113 metric ood_eval_time = 8.174081863776088\n",
      " ---- 114000 metric loss = 0.07343447641376406\n",
      " ---- 114 metric test_eval_f1 = 0.9212101601682575\n",
      " ---- 114 metric test_eval_time = 7.550234589872189\n",
      " ---- 114 metric ood_eval = 0.0\n",
      " ---- 114 metric ood_eval_time = 7.550234589872189\n",
      " ---- 57 metric train_eval_f1 = 0.9384354016665318\n",
      " ---- 57 metric train_eval_time = 7.421608284119408\n",
      " ---- 115000 metric loss = 0.07466706649400294\n",
      " ---- 115 metric test_eval_f1 = 0.9176508655557353\n",
      " ---- 115 metric test_eval_time = 6.608154020385051\n",
      " ---- 115 metric ood_eval = 0.0\n",
      " ---- 115 metric ood_eval_time = 6.608154020385051\n",
      " ---- 116000 metric loss = 0.0746643155994825\n",
      " ---- 116 metric test_eval_f1 = 0.912473709755703\n",
      " ---- 116 metric test_eval_time = 7.5169066494094805\n",
      " ---- 116 metric ood_eval = 0.0\n",
      " ---- 116 metric ood_eval_time = 7.5169066494094805\n",
      " ---- 58 metric train_eval_f1 = 0.9356039155408139\n",
      " ---- 58 metric train_eval_time = 7.4008170859962785\n",
      " ---- 117000 metric loss = 0.06984504268085584\n",
      " ---- 117 metric test_eval_f1 = 0.9213719462870086\n",
      " ---- 117 metric test_eval_time = 7.110176346869439\n",
      " ---- 117 metric ood_eval = 0.0\n",
      " ---- 117 metric ood_eval_time = 7.110176346869439\n",
      " ---- 118000 metric loss = 0.07162443663971499\n",
      " ---- 118 metric test_eval_f1 = 0.9204012295745025\n",
      " ---- 118 metric test_eval_time = 6.71833036725449\n",
      " ---- 118 metric ood_eval = 0.0\n",
      " ---- 118 metric ood_eval_time = 6.71833036725449\n",
      " ---- 59 metric train_eval_f1 = 0.942359032440741\n",
      " ---- 59 metric train_eval_time = 6.610630207911981\n",
      " ---- 119000 metric loss = 0.0743145974832587\n",
      " ---- 119 metric test_eval_f1 = 0.9174890794369843\n",
      " ---- 119 metric test_eval_time = 7.206115515288788\n",
      " ---- 119 metric ood_eval = 0.0\n",
      " ---- 119 metric ood_eval_time = 7.206115515288788\n",
      " ---- 120000 metric loss = 0.07194659400358797\n",
      " ---- 120 metric test_eval_f1 = 0.9207248018120046\n",
      " ---- 120 metric test_eval_time = 7.304157903251901\n",
      " ---- 120 metric ood_eval = 0.0\n",
      " ---- 120 metric ood_eval_time = 7.304157903251901\n",
      " ---- 60 metric train_eval_f1 = 0.9436938759000081\n",
      " ---- 60 metric train_eval_time = 7.119205565892727\n",
      " ---- 121000 metric loss = 0.06800471745664254\n",
      " ---- 121 metric test_eval_f1 = 0.9194305128619964\n",
      " ---- 121 metric test_eval_time = 8.547646011972173\n",
      " ---- 121 metric ood_eval = 0.0\n",
      " ---- 121 metric ood_eval_time = 8.547646011972173\n",
      " ---- 122000 metric loss = 0.07410361225530505\n",
      " ---- 122 metric test_eval_f1 = 0.9220190907620126\n",
      " ---- 122 metric test_eval_time = 6.549749231515936\n",
      " ---- 122 metric ood_eval = 0.0\n",
      " ---- 122 metric ood_eval_time = 6.549749231515936\n",
      " ---- 61 metric train_eval_f1 = 0.9445433217377235\n",
      " ---- 61 metric train_eval_time = 6.444745570746703\n",
      " ---- 123000 metric loss = 0.07402938106050715\n",
      " ---- 123 metric test_eval_f1 = 0.8903090114868144\n",
      " ---- 123 metric test_eval_time = 7.690826727066818\n",
      " ---- 123 metric ood_eval = 0.0\n",
      " ---- 123 metric ood_eval_time = 7.690826727066818\n",
      " ---- 124000 metric loss = 0.0688206818937324\n",
      " ---- 124 metric test_eval_f1 = 0.9236369519495228\n",
      " ---- 124 metric test_eval_time = 7.42565927843391\n",
      " ---- 124 metric ood_eval = 0.0\n",
      " ---- 124 metric ood_eval_time = 7.42565927843391\n",
      " ---- 62 metric train_eval_f1 = 0.9489927999352803\n",
      " ---- 62 metric train_eval_time = 7.254833751314618\n",
      " ---- 125000 metric loss = 0.07234563288651406\n",
      " ---- 125 metric test_eval_f1 = 0.9233133797120207\n",
      " ---- 125 metric test_eval_time = 7.366930917327293\n",
      " ---- 125 metric ood_eval = 0.0\n",
      " ---- 125 metric ood_eval_time = 7.366930917327293\n",
      " ---- 126000 metric loss = 0.06997807986475527\n",
      " ---- 126 metric test_eval_f1 = 0.9181362239119883\n",
      " ---- 126 metric test_eval_time = 8.016663970231354\n",
      " ---- 126 metric ood_eval = 0.0\n",
      " ---- 126 metric ood_eval_time = 8.016663970231354\n",
      " ---- 63 metric train_eval_f1 = 0.9375455060270205\n",
      " ---- 63 metric train_eval_time = 7.818299490332497\n",
      " ---- 127000 metric loss = 0.06887169547006487\n",
      " ---- 127 metric test_eval_f1 = 0.9246076686620288\n",
      " ---- 127 metric test_eval_time = 7.485034784015531\n",
      " ---- 127 metric ood_eval = 0.0\n",
      " ---- 127 metric ood_eval_time = 7.485034784015531\n",
      " ---- 128000 metric loss = 0.06690837654750795\n",
      " ---- 128 metric test_eval_f1 = 0.9208865879307555\n",
      " ---- 128 metric test_eval_time = 6.927843391037049\n",
      " ---- 128 metric ood_eval = 0.0\n",
      " ---- 128 metric ood_eval_time = 6.927843391037049\n",
      " ---- 64 metric train_eval_f1 = 0.9464040126203381\n",
      " ---- 64 metric train_eval_time = 6.785252002265189\n",
      " ---- 129000 metric loss = 0.06801218906440772\n",
      " ---- 129 metric test_eval_f1 = 0.9126354958744539\n",
      " ---- 129 metric test_eval_time = 7.0236207733376474\n",
      " ---- 129 metric ood_eval = 0.0\n",
      " ---- 129 metric ood_eval_time = 7.0236207733376474\n",
      " ---- 130000 metric loss = 0.06965348552260547\n",
      " ---- 130 metric test_eval_f1 = 0.884970069568031\n",
      " ---- 130 metric test_eval_time = 8.843714609286524\n",
      " ---- 130 metric ood_eval = 0.0\n",
      " ---- 130 metric ood_eval_time = 8.843714609286524\n",
      " ---- 65 metric train_eval_f1 = 0.9050238653830597\n",
      " ---- 65 metric train_eval_time = 8.852075074832133\n",
      " ---- 131000 metric loss = 0.06958758202474565\n",
      " ---- 131 metric test_eval_f1 = 0.9200776573370005\n",
      " ---- 131 metric test_eval_time = 7.206115515288788\n",
      " ---- 131 metric ood_eval = 0.0\n",
      " ---- 131 metric ood_eval_time = 7.206115515288788\n",
      " ---- 132000 metric loss = 0.07173222170560621\n",
      " ---- 132 metric test_eval_f1 = 0.9184597961494904\n",
      " ---- 132 metric test_eval_time = 7.039475812975247\n",
      " ---- 132 metric ood_eval = 0.0\n",
      " ---- 132 metric ood_eval_time = 7.039475812975247\n",
      " ---- 66 metric train_eval_f1 = 0.9420758838281692\n",
      " ---- 66 metric train_eval_time = 6.945514116980827\n",
      " ---- 133000 metric loss = 0.06858630447415635\n",
      " ---- 133 metric test_eval_f1 = 0.9262255298495389\n",
      " ---- 133 metric test_eval_time = 6.700210321954376\n",
      " ---- 133 metric ood_eval = 0.0\n",
      " ---- 133 metric ood_eval_time = 6.700210321954376\n",
      " ---- 134000 metric loss = 0.06678482581861317\n",
      " ---- 134 metric test_eval_f1 = 0.9181362239119883\n",
      " ---- 134 metric test_eval_time = 7.5613978320660085\n",
      " ---- 134 metric ood_eval = 0.0\n",
      " ---- 134 metric ood_eval_time = 7.5613978320660085\n",
      " ---- 67 metric train_eval_f1 = 0.9411859881886578\n",
      " ---- 67 metric train_eval_time = 7.404578917563304\n",
      " ---- 135000 metric loss = 0.06791492419224232\n",
      " ---- 135 metric test_eval_f1 = 0.9085908429056787\n",
      " ---- 135 metric test_eval_time = 6.993043196893707\n",
      " ---- 135 metric ood_eval = 0.0\n",
      " ---- 135 metric ood_eval_time = 6.993043196893707\n",
      " ---- 136000 metric loss = 0.06561643733130768\n",
      " ---- 136 metric test_eval_f1 = 0.9207248018120046\n",
      " ---- 136 metric test_eval_time = 6.722051447985763\n",
      " ---- 136 metric ood_eval = 0.0\n",
      " ---- 136 metric ood_eval_time = 6.722051447985763\n",
      " ---- 68 metric train_eval_f1 = 0.9439770245125799\n",
      " ---- 68 metric train_eval_time = 6.61807297144244\n",
      " ---- 137000 metric loss = 0.06669906870275735\n",
      " ---- 137 metric test_eval_f1 = 0.928328749393302\n",
      " ---- 137 metric test_eval_time = 7.261122795664132\n",
      " ---- 137 metric ood_eval = 0.0\n",
      " ---- 137 metric ood_eval_time = 7.261122795664132\n",
      " ---- 138000 metric loss = 0.06853886563098058\n",
      " ---- 138 metric test_eval_f1 = 0.9186215822682414\n",
      " ---- 138 metric test_eval_time = 7.071671250606698\n",
      " ---- 138 metric ood_eval = 0.0\n",
      " ---- 138 metric ood_eval_time = 7.071671250606698\n",
      " ---- 69 metric train_eval_f1 = 0.9450287193592751\n",
      " ---- 69 metric train_eval_time = 6.931033087937869\n",
      " ---- 139000 metric loss = 0.06644333924050443\n",
      " ---- 139 metric test_eval_f1 = 0.9250930270182818\n",
      " ---- 139 metric test_eval_time = 6.823814916680149\n",
      " ---- 139 metric ood_eval = 0.0\n",
      " ---- 139 metric ood_eval_time = 6.823814916680149\n",
      " ---- 140000 metric loss = 0.06637791357794777\n",
      " ---- 140 metric test_eval_f1 = 0.9212101601682575\n",
      " ---- 140 metric test_eval_time = 6.784339103704903\n",
      " ---- 140 metric ood_eval = 0.0\n",
      " ---- 140 metric ood_eval_time = 6.784339103704903\n",
      " ---- 70 metric train_eval_f1 = 0.9481838038993609\n",
      " ---- 70 metric train_eval_time = 6.619529164307095\n",
      " ---- 141000 metric loss = 0.06586219231877476\n",
      " ---- 141 metric test_eval_f1 = 0.9250930270182818\n",
      " ---- 141 metric test_eval_time = 6.988027827212425\n",
      " ---- 141 metric ood_eval = 0.0\n",
      " ---- 141 metric ood_eval_time = 6.988027827212425\n",
      " ---- 142000 metric loss = 0.06727294299122877\n",
      " ---- 142 metric test_eval_f1 = 0.9182980100307394\n",
      " ---- 142 metric test_eval_time = 6.638731596828992\n",
      " ---- 142 metric ood_eval = 0.0\n",
      " ---- 142 metric ood_eval_time = 6.638731596828992\n",
      " ---- 71 metric train_eval_f1 = 0.9459186149987865\n",
      " ---- 71 metric train_eval_time = 6.551209449073699\n",
      " ---- 143000 metric loss = 0.06852324485499411\n",
      " ---- 143 metric test_eval_f1 = 0.9215337324057595\n",
      " ---- 143 metric test_eval_time = 7.010677883837567\n",
      " ---- 143 metric ood_eval = 0.0\n",
      " ---- 143 metric ood_eval_time = 7.010677883837567\n",
      " ---- 144000 metric loss = 0.06565800214279444\n",
      " ---- 144 metric test_eval_f1 = 0.9234751658307717\n",
      " ---- 144 metric test_eval_time = 6.837566736773985\n",
      " ---- 144 metric ood_eval = 0.0\n",
      " ---- 144 metric ood_eval_time = 6.837566736773985\n",
      " ---- 72 metric train_eval_f1 = 0.9487905509263005\n",
      " ---- 72 metric train_eval_time = 6.646549631906804\n",
      " ---- 145000 metric loss = 0.06177831398160197\n",
      " ---- 145 metric test_eval_f1 = 0.8954861672868468\n",
      " ---- 145 metric test_eval_time = 7.724316453648277\n",
      " ---- 145 metric ood_eval = 0.0\n",
      " ---- 145 metric ood_eval_time = 7.724316453648277\n",
      " ---- 146000 metric loss = 0.0670323483897373\n",
      " ---- 146 metric test_eval_f1 = 0.9158712182494742\n",
      " ---- 146 metric test_eval_time = 6.9391684193496195\n",
      " ---- 146 metric ood_eval = 0.0\n",
      " ---- 146 metric ood_eval_time = 6.9391684193496195\n",
      " ---- 73 metric train_eval_f1 = 0.948022004692177\n",
      " ---- 73 metric train_eval_time = 6.778699134374242\n",
      " ---- 147000 metric loss = 0.06706866992544383\n",
      " ---- 147 metric test_eval_f1 = 0.9213719462870086\n",
      " ---- 147 metric test_eval_time = 7.334250121339589\n",
      " ---- 147 metric ood_eval = 0.0\n",
      " ---- 147 metric ood_eval_time = 7.334250121339589\n",
      " ---- 148000 metric loss = 0.06309656763356179\n",
      " ---- 148 metric test_eval_f1 = 0.913282640349458\n",
      " ---- 148 metric test_eval_time = 7.427762497977674\n",
      " ---- 148 metric ood_eval = 0.0\n",
      " ---- 148 metric ood_eval_time = 7.427762497977674\n",
      " ---- 74 metric train_eval_f1 = 0.9390421486934714\n",
      " ---- 74 metric train_eval_time = 7.247269638378772\n",
      " ---- 149000 metric loss = 0.06518760916986503\n",
      " ---- 149 metric test_eval_f1 = 0.9181362239119883\n",
      " ---- 149 metric test_eval_time = 7.443941109852775\n",
      " ---- 149 metric ood_eval = 0.0\n",
      " ---- 149 metric ood_eval_time = 7.443941109852775\n",
      " ---- 150000 metric loss = 0.06455643056798727\n",
      " ---- 150 metric test_eval_f1 = 0.9165183627244783\n",
      " ---- 150 metric test_eval_time = 6.941109852774632\n",
      " ---- 150 metric ood_eval = 0.00016178611875101117\n",
      " ---- 150 metric ood_eval_time = 6.941109852774632\n",
      " ---- 75 metric train_eval_f1 = 0.9459995146023784\n",
      " ---- 75 metric train_eval_time = 6.82570180406116\n",
      " ---- 151000 metric loss = 0.0656836639791727\n",
      " ---- 151 metric test_eval_f1 = 0.9239605241870248\n",
      " ---- 151 metric test_eval_time = 7.361106617052257\n",
      " ---- 151 metric ood_eval = 0.0\n",
      " ---- 151 metric ood_eval_time = 7.361106617052257\n",
      " ---- 152000 metric loss = 0.06209153404645622\n",
      " ---- 152 metric test_eval_f1 = 0.9184597961494904\n",
      " ---- 152 metric test_eval_time = 7.060346222294127\n",
      " ---- 152 metric ood_eval = 0.0\n",
      " ---- 152 metric ood_eval_time = 7.060346222294127\n",
      " ---- 76 metric train_eval_f1 = 0.9487096513227086\n",
      " ---- 76 metric train_eval_time = 6.891634980988593\n",
      " ---- 153000 metric loss = 0.06536718344176189\n",
      " ---- 153 metric test_eval_f1 = 0.9244458825432778\n",
      " ---- 153 metric test_eval_time = 7.952758453324705\n",
      " ---- 153 metric ood_eval = 0.0\n",
      " ---- 153 metric ood_eval_time = 7.952758453324705\n",
      " ---- 154000 metric loss = 0.06073548419680446\n",
      " ---- 154 metric test_eval_f1 = 0.9155476460119721\n",
      " ---- 154 metric test_eval_time = 7.067626597637923\n",
      " ---- 154 metric ood_eval = 0.0\n",
      " ---- 154 metric ood_eval_time = 7.067626597637923\n",
      " ---- 77 metric train_eval_f1 = 0.9428848798640886\n",
      " ---- 77 metric train_eval_time = 6.950165844187364\n",
      " ---- 155000 metric loss = 0.061800649635260926\n",
      " ---- 155 metric test_eval_f1 = 0.9229898074745186\n",
      " ---- 155 metric test_eval_time = 7.158550396375991\n",
      " ---- 155 metric ood_eval = 0.0\n",
      " ---- 155 metric ood_eval_time = 7.158550396375991\n",
      " ---- 156000 metric loss = 0.06791112383268774\n",
      " ---- 156 metric test_eval_f1 = 0.9106940624494418\n",
      " ---- 156 metric test_eval_time = 7.15677074906973\n",
      " ---- 156 metric ood_eval = 0.0\n",
      " ---- 156 metric ood_eval_time = 7.15677074906973\n",
      " ---- 78 metric train_eval_f1 = 0.9391230482970634\n",
      " ---- 78 metric train_eval_time = 7.010921446484912\n",
      " ---- 157000 metric loss = 0.06474152863025666\n",
      " ---- 157 metric test_eval_f1 = 0.9234751658307717\n",
      " ---- 157 metric test_eval_time = 7.045623685487785\n",
      " ---- 157 metric ood_eval = 0.0\n",
      " ---- 157 metric ood_eval_time = 7.045623685487785\n",
      " ---- 158000 metric loss = 0.06815879805898294\n",
      " ---- 158 metric test_eval_f1 = 0.9221808768807637\n",
      " ---- 158 metric test_eval_time = 6.717845008898236\n",
      " ---- 158 metric ood_eval = 0.0\n",
      " ---- 158 metric ood_eval_time = 6.717845008898236\n",
      " ---- 79 metric train_eval_f1 = 0.9489523501334843\n",
      " ---- 79 metric train_eval_time = 6.573537739665076\n",
      " ---- 159000 metric loss = 0.06285783389769495\n",
      " ---- 159 metric test_eval_f1 = 0.9215337324057595\n",
      " ---- 159 metric test_eval_time = 7.110823491344442\n",
      " ---- 159 metric ood_eval = 0.0\n",
      " ---- 159 metric ood_eval_time = 7.110823491344442\n",
      " ---- 160000 metric loss = 0.06165987514448352\n",
      " ---- 160 metric test_eval_f1 = 0.9187833683869924\n",
      " ---- 160 metric test_eval_time = 7.141401067788384\n",
      " ---- 160 metric ood_eval = 0.0\n",
      " ---- 160 metric ood_eval_time = 7.141401067788384\n",
      " ---- 80 metric train_eval_f1 = 0.9444219723323356\n",
      " ---- 80 metric train_eval_time = 7.067753418008252\n",
      " ---- 161000 metric loss = 0.06430593444220721\n",
      " ---- 161 metric test_eval_f1 = 0.9237987380682737\n",
      " ---- 161 metric test_eval_time = 7.87833683869924\n",
      " ---- 161 metric ood_eval = 0.0\n",
      " ---- 161 metric ood_eval_time = 7.87833683869924\n",
      " ---- 162000 metric loss = 0.062069873951142655\n",
      " ---- 162 metric test_eval_f1 = 0.9202394434557515\n",
      " ---- 162 metric test_eval_time = 6.945478077980909\n",
      " ---- 162 metric ood_eval = 0.0\n",
      " ---- 162 metric ood_eval_time = 6.945478077980909\n",
      " ---- 81 metric train_eval_f1 = 0.9449073699538872\n",
      " ---- 81 metric train_eval_time = 6.8202815306205\n",
      " ---- 163000 metric loss = 0.06578308384865522\n",
      " ---- 163 metric test_eval_f1 = 0.9110176346869439\n",
      " ---- 163 metric test_eval_time = 7.48163727552176\n",
      " ---- 163 metric ood_eval = 0.0\n",
      " ---- 163 metric ood_eval_time = 7.48163727552176\n",
      " ---- 164000 metric loss = 0.06254002887150273\n",
      " ---- 164 metric test_eval_f1 = 0.9234751658307717\n",
      " ---- 164 metric test_eval_time = 7.0618022973628864\n",
      " ---- 164 metric ood_eval = 0.0\n",
      " ---- 164 metric ood_eval_time = 7.0618022973628864\n",
      " ---- 82 metric train_eval_f1 = 0.9516624868538144\n",
      " ---- 82 metric train_eval_time = 6.920637488876305\n",
      " ---- 165000 metric loss = 0.059643119584070516\n",
      " ---- 165 metric test_eval_f1 = 0.9212101601682575\n",
      " ---- 165 metric test_eval_time = 7.179744377932374\n",
      " ---- 165 metric ood_eval = 0.0\n",
      " ---- 165 metric ood_eval_time = 7.179744377932374\n",
      " ---- 166000 metric loss = 0.06321852993266656\n",
      " ---- 166 metric test_eval_f1 = 0.927034460443294\n",
      " ---- 166 metric test_eval_time = 6.674809901310468\n",
      " ---- 166 metric ood_eval = 0.0\n",
      " ---- 166 metric ood_eval_time = 6.674809901310468\n",
      " ---- 83 metric train_eval_f1 = 0.9527141817005097\n",
      " ---- 83 metric train_eval_time = 6.555861176280236\n",
      " ---- 167000 metric loss = 0.0604657501343172\n",
      " ---- 167 metric test_eval_f1 = 0.9257401714932859\n",
      " ---- 167 metric test_eval_time = 7.505096262740657\n",
      " ---- 167 metric ood_eval = 0.0\n",
      " ---- 167 metric ood_eval_time = 7.505096262740657\n",
      " ---- 168000 metric loss = 0.06352146038017235\n",
      " ---- 168 metric test_eval_f1 = 0.9216955185245106\n",
      " ---- 168 metric test_eval_time = 7.129914253357062\n",
      " ---- 168 metric ood_eval = 0.0\n",
      " ---- 168 metric ood_eval_time = 7.129914253357062\n",
      " ---- 84 metric train_eval_f1 = 0.9505703422053232\n",
      " ---- 84 metric train_eval_time = 6.98353693066904\n",
      " ---- 169000 metric loss = 0.058864049459807576\n",
      " ---- 169 metric test_eval_f1 = 0.9074583400744216\n",
      " ---- 169 metric test_eval_time = 7.751334735479696\n",
      " ---- 169 metric ood_eval = 0.0\n",
      " ---- 169 metric ood_eval_time = 7.751334735479696\n",
      " ---- 170000 metric loss = 0.06928577118669636\n",
      " ---- 170 metric test_eval_f1 = 0.9257401714932859\n",
      " ---- 170 metric test_eval_time = 7.165830771719786\n",
      " ---- 170 metric ood_eval = 0.0\n",
      " ---- 170 metric ood_eval_time = 7.165830771719786\n",
      " ---- 85 metric train_eval_f1 = 0.9540490251597767\n",
      " ---- 85 metric train_eval_time = 6.9839818784887955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mddq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel_lag_in_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.0005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/earlyClassification/DQL/trainers.py:201\u001b[0m, in \u001b[0;36mEarlyClassificationtrainer.train\u001b[0;34m(self, epochs, batch_size, lr, lam, model_lag_in_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictor_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m         steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m            \n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m steps\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/earlyClassification/DQL/trainers.py:89\u001b[0m, in \u001b[0;36mEarlyClassificationtrainer.trainStep\u001b[0;34m(self, steps, batch, lam, predictor_optimizer)\u001b[0m\n\u001b[1;32m     86\u001b[0m     next_state_values_for_max_action \u001b[38;5;241m=\u001b[39m next_state_values_for_max_action\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m~\u001b[39m(is_terminal\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     87\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m lam\u001b[38;5;241m*\u001b[39m(next_state_values_for_max_action\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m---> 89\u001b[0m predicted_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     90\u001b[0m predicted_values_for_taken_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m predicted_values, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,index\u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse_loss_function(target, predicted_values_for_taken_action\u001b[38;5;241m.\u001b[39msqueeze())\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/modeling/neuralNetworks.py:50\u001b[0m, in \u001b[0;36mLSTMNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m lstm_out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unpackAndGetFeatureFromLSTMOutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m lstm_out\u001b[38;5;66;03m#/torch.linalg.norm(lstm_out,dim = -1,keepdims = True)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(lstm_out)\n",
      "File \u001b[0;32m~/Desktop/UNSW/flowprintOptimal/sekigo/modeling/neuralNetworks.py:26\u001b[0m, in \u001b[0;36mBaseLSTMNetwork._unpackAndGetFeatureFromLSTMOutput\u001b[0;34m(self, lstm_out)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unpackAndGetFeatureFromLSTMOutput\u001b[39m(\u001b[38;5;28mself\u001b[39m,lstm_out : nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence):\n\u001b[0;32m---> 26\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m \u001b[43munpack_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x : x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],lstm_out))\n\u001b[1;32m     28\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(lstm_out,dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/utils/rnn.py:516\u001b[0m, in \u001b[0;36munpack_sequence\u001b[0;34m(packed_sequences)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Unpack PackedSequence into a list of variable length Tensors.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m``packed_sequences`` should be a PackedSequence object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    a list of :class:`Tensor` objects\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m padded_sequences, lengths \u001b[38;5;241m=\u001b[39m pad_packed_sequence(packed_sequences, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 516\u001b[0m unpacked_sequences \u001b[38;5;241m=\u001b[39m \u001b[43munpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpacked_sequences\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/utils/rnn.py:446\u001b[0m, in \u001b[0;36munpad_sequence\u001b[0;34m(padded_sequences, lengths, batch_first)\u001b[0m\n\u001b[1;32m    444\u001b[0m     mask \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m length\n\u001b[1;32m    445\u001b[0m     unpacked_seq \u001b[38;5;241m=\u001b[39m seq[mask]\n\u001b[0;32m--> 446\u001b[0m     \u001b[43munpadded_sequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpacked_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpadded_sequences\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddq_model.train(epochs= 25,batch_size= 128,model_lag_in_steps= 100,lr= .0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.261122795664132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.928328749393302"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(logger.getMetric(metric_name= \"test_eval_f1\")))\n",
    "print(logger.getMetric(metric_name= \"test_eval_time\")[136])\n",
    "logger.getMetric(metric_name= \"test_eval_f1\")[136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7bae9b7dba00>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logger.getMetric(metric_name= \"loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'test_eval_f1',\n",
       " 'test_eval_time',\n",
       " 'ood_eval',\n",
       " 'ood_eval_time',\n",
       " 'train_eval_f1',\n",
       " 'train_eval_time']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.getAllMetricNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
